{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing required libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading data from files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = np.loadtxt('lab_1_test.csv', delimiter=',', skiprows=1)\n",
        "train = np.loadtxt('lab_1_train.csv', delimiter=',', skiprows=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splitting data into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_data = train[:, 1:]\n",
        "test_data = test[:, 1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "alpha = 0.01\n",
        "delta = 0.000001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the learning rate (alpha) and delta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initializing the weights (in our case beta0 and beta1) with zeros (quite common case) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "beta = np.zeros((2, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the loss (cost) function (mean squared error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def compute_cost(x, y, beta):\n",
        "    m = len(y)\n",
        "    p = x.dot(beta)\n",
        "    return (1/(2*m)) * np.sum(np.square(p - y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining gradient descent to minimize the loss function, where the loop stop factor is difference between prev and cur loss values being < delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent(x, y, beta, alpha, delta):\n",
        "    m = len(y)\n",
        "    prev_loss, cur_loss = 0, 0\n",
        "    i = 0\n",
        "    while True:\n",
        "        i += 1\n",
        "        h = x.dot(beta)\n",
        "        beta = beta - (alpha/m) * x.T.dot(h - y)\n",
        "        cur_loss = compute_cost(x, y, beta)\n",
        "        print('Iteration', i, 'Loss =', cur_loss)\n",
        "        if np.abs(prev_loss - cur_loss) < delta:\n",
        "            break\n",
        "        prev_loss = cur_loss\n",
        "    return beta, cur_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding a column of ones to the data matrix for the intercept term (beta0) and reshaping train target data into single column array "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = np.hstack((np.ones((len(train_data), 1)), train_data[:, :-1]))\n",
        "y_train = train_data[:, -1].reshape((-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running gradient descent on the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1 Loss = 206.65857757979356\n",
            "Iteration 2 Loss = 202.17787676259215\n",
            "Iteration 3 Loss = 197.79444023128175\n",
            "Iteration 4 Loss = 193.50615657776237\n",
            "Iteration 5 Loss = 189.31096022830326\n",
            "Iteration 6 Loss = 185.20683044857154\n",
            "Iteration 7 Loss = 181.19179037026078\n",
            "Iteration 8 Loss = 177.26390603884835\n",
            "Iteration 9 Loss = 173.42128548202513\n",
            "Iteration 10 Loss = 169.6620777983464\n",
            "Iteration 11 Loss = 165.9844722656679\n",
            "Iteration 12 Loss = 162.3866974689344\n",
            "Iteration 13 Loss = 158.8670204469031\n",
            "Iteration 14 Loss = 155.4237458573897\n",
            "Iteration 15 Loss = 152.05521516063516\n",
            "Iteration 16 Loss = 148.7598058203999\n",
            "Iteration 17 Loss = 145.53593052240115\n",
            "Iteration 18 Loss = 142.38203640971545\n",
            "Iteration 19 Loss = 139.29660433477994\n",
            "Iteration 20 Loss = 136.27814812763057\n",
            "Iteration 21 Loss = 133.32521388002576\n",
            "Iteration 22 Loss = 130.4363792451101\n",
            "Iteration 23 Loss = 127.61025275228064\n",
            "Iteration 24 Loss = 124.84547313692681\n",
            "Iteration 25 Loss = 122.14070868471947\n",
            "Iteration 26 Loss = 119.4946565901346\n",
            "Iteration 27 Loss = 116.90604232890209\n",
            "Iteration 28 Loss = 114.37361904407715\n",
            "Iteration 29 Loss = 111.89616694543923\n",
            "Iteration 30 Loss = 109.47249272192838\n",
            "Iteration 31 Loss = 107.10142896683675\n",
            "Iteration 32 Loss = 104.78183361547768\n",
            "Iteration 33 Loss = 102.51258939506222\n",
            "Iteration 34 Loss = 100.29260328651739\n",
            "Iteration 35 Loss = 98.12080599798756\n",
            "Iteration 36 Loss = 95.99615144976458\n",
            "Iteration 37 Loss = 93.91761627039975\n",
            "Iteration 38 Loss = 91.88419930375338\n",
            "Iteration 39 Loss = 89.89492112674588\n",
            "Iteration 40 Loss = 87.94882357757729\n",
            "Iteration 41 Loss = 86.044969294188\n",
            "Iteration 42 Loss = 84.18244126273908\n",
            "Iteration 43 Loss = 82.36034237589348\n",
            "Iteration 44 Loss = 80.5777950006868\n",
            "Iteration 45 Loss = 78.83394055577793\n",
            "Iteration 46 Loss = 77.12793909787744\n",
            "Iteration 47 Loss = 75.45896891715319\n",
            "Iteration 48 Loss = 73.82622614141933\n",
            "Iteration 49 Loss = 72.2289243489171\n",
            "Iteration 50 Loss = 70.6662941895018\n",
            "Iteration 51 Loss = 69.13758301405265\n",
            "Iteration 52 Loss = 67.64205451192773\n",
            "Iteration 53 Loss = 66.17898835628871\n",
            "Iteration 54 Loss = 64.74767985712519\n",
            "Iteration 55 Loss = 63.347439621810906\n",
            "Iteration 56 Loss = 61.9775932230287\n",
            "Iteration 57 Loss = 60.637480873904195\n",
            "Iteration 58 Loss = 59.32645711019148\n",
            "Iteration 59 Loss = 58.04389047935812\n",
            "Iteration 60 Loss = 56.789163236419355\n",
            "Iteration 61 Loss = 55.56167104637518\n",
            "Iteration 62 Loss = 54.36082269310685\n",
            "Iteration 63 Loss = 53.1860397945927\n",
            "Iteration 64 Loss = 52.03675652430603\n",
            "Iteration 65 Loss = 50.91241933866092\n",
            "Iteration 66 Loss = 49.81248671037454\n",
            "Iteration 67 Loss = 48.736428867617754\n",
            "Iteration 68 Loss = 47.683727538828045\n",
            "Iteration 69 Loss = 46.653875703062305\n",
            "Iteration 70 Loss = 45.64637734576866\n",
            "Iteration 71 Loss = 44.66074721986024\n",
            "Iteration 72 Loss = 43.69651061197537\n",
            "Iteration 73 Loss = 42.75320311381188\n",
            "Iteration 74 Loss = 41.83037039842524\n",
            "Iteration 75 Loss = 40.9275680013828\n",
            "Iteration 76 Loss = 40.04436110666886\n",
            "Iteration 77 Loss = 39.18032433723698\n",
            "Iteration 78 Loss = 38.33504155010935\n",
            "Iteration 79 Loss = 37.508105635923926\n",
            "Iteration 80 Loss = 36.69911832283298\n",
            "Iteration 81 Loss = 35.90768998465872\n",
            "Iteration 82 Loss = 35.133439453213356\n",
            "Iteration 83 Loss = 34.37599383469345\n",
            "Iteration 84 Loss = 33.63498833005976\n",
            "Iteration 85 Loss = 32.9100660593165\n",
            "Iteration 86 Loss = 32.200877889604975\n",
            "Iteration 87 Loss = 31.507082267029023\n",
            "Iteration 88 Loss = 30.8283450521312\n",
            "Iteration 89 Loss = 30.164339358940417\n",
            "Iteration 90 Loss = 29.514745397513448\n",
            "Iteration 91 Loss = 28.879250319894727\n",
            "Iteration 92 Loss = 28.257548069419858\n",
            "Iteration 93 Loss = 27.649339233290554\n",
            "Iteration 94 Loss = 27.054330898349807\n",
            "Iteration 95 Loss = 26.472236509987923\n",
            "Iteration 96 Loss = 25.90277573411143\n",
            "Iteration 97 Loss = 25.345674322108323\n",
            "Iteration 98 Loss = 24.800663978744637\n",
            "Iteration 99 Loss = 24.267482232928774\n",
            "Iteration 100 Loss = 23.745872311281197\n",
            "Iteration 101 Loss = 23.235583014448746\n",
            "Iteration 102 Loss = 22.736368596103823\n",
            "Iteration 103 Loss = 22.247988644570356\n",
            "Iteration 104 Loss = 21.77020796701938\n",
            "Iteration 105 Loss = 21.30279647617848\n",
            "Iteration 106 Loss = 20.84552907950054\n",
            "Iteration 107 Loss = 20.398185570738455\n",
            "Iteration 108 Loss = 19.960550523873525\n",
            "Iteration 109 Loss = 19.5324131893464\n",
            "Iteration 110 Loss = 19.113567392540652\n",
            "Iteration 111 Loss = 18.703811434470147\n",
            "Iteration 112 Loss = 18.302947994622194\n",
            "Iteration 113 Loss = 17.91078403590979\n",
            "Iteration 114 Loss = 17.52713071168717\n",
            "Iteration 115 Loss = 17.151803274783894\n",
            "Iteration 116 Loss = 16.78462098851357\n",
            "Iteration 117 Loss = 16.425407039614417\n",
            "Iteration 118 Loss = 16.073988453079686\n",
            "Iteration 119 Loss = 15.730196008836986\n",
            "Iteration 120 Loss = 15.393864160236317\n",
            "Iteration 121 Loss = 15.064830954307501\n",
            "Iteration 122 Loss = 14.742937953748777\n",
            "Iteration 123 Loss = 14.428030160608747\n",
            "Iteration 124 Loss = 14.119955941625113\n",
            "Iteration 125 Loss = 13.818566955184114\n",
            "Iteration 126 Loss = 13.523718079865565\n",
            "Iteration 127 Loss = 13.235267344538984\n",
            "Iteration 128 Loss = 12.953075859977234\n",
            "Iteration 129 Loss = 12.677007751954687\n",
            "Iteration 130 Loss = 12.406930095797648\n",
            "Iteration 131 Loss = 12.14271285235563\n",
            "Iteration 132 Loss = 11.884228805362502\n",
            "Iteration 133 Loss = 11.631353500157413\n",
            "Iteration 134 Loss = 11.383965183735993\n",
            "Iteration 135 Loss = 11.141944746102851\n",
            "Iteration 136 Loss = 10.905175662897284\n",
            "Iteration 137 Loss = 10.673543939264375\n",
            "Iteration 138 Loss = 10.44693805494458\n",
            "Iteration 139 Loss = 10.225248910555319\n",
            "Iteration 140 Loss = 10.008369775038668\n",
            "Iteration 141 Loss = 9.79619623424982\n",
            "Iteration 142 Loss = 9.588626140661667\n",
            "Iteration 143 Loss = 9.385559564161118\n",
            "Iteration 144 Loss = 9.18689874391356\n",
            "Iteration 145 Loss = 8.992548041272247\n",
            "Iteration 146 Loss = 8.802413893709938\n",
            "Iteration 147 Loss = 8.616404769750522\n",
            "Iteration 148 Loss = 8.43443112487909\n",
            "Iteration 149 Loss = 8.256405358409026\n",
            "Iteration 150 Loss = 8.08224177128546\n",
            "Iteration 151 Loss = 7.91185652480472\n",
            "Iteration 152 Loss = 7.7451676002298795\n",
            "Iteration 153 Loss = 7.582094759283002\n",
            "Iteration 154 Loss = 7.42255950549502\n",
            "Iteration 155 Loss = 7.2664850463945685\n",
            "Iteration 156 Loss = 7.1137962565176975\n",
            "Iteration 157 Loss = 6.964419641220527\n",
            "Iteration 158 Loss = 6.8182833012774715\n",
            "Iteration 159 Loss = 6.675316898247948\n",
            "Iteration 160 Loss = 6.53545162059492\n",
            "Iteration 161 Loss = 6.398620150538927\n",
            "Iteration 162 Loss = 6.264756631631662\n",
            "Iteration 163 Loss = 6.133796637033427\n",
            "Iteration 164 Loss = 6.005677138479255\n",
            "Iteration 165 Loss = 5.8803364759187176\n",
            "Iteration 166 Loss = 5.757714327814734\n",
            "Iteration 167 Loss = 5.637751682087211\n",
            "Iteration 168 Loss = 5.520390807687384\n",
            "Iteration 169 Loss = 5.405575226789231\n",
            "Iteration 170 Loss = 5.293249687584561\n",
            "Iteration 171 Loss = 5.183360137668676\n",
            "Iteration 172 Loss = 5.075853698003751\n",
            "Iteration 173 Loss = 4.970678637447442\n",
            "Iteration 174 Loss = 4.867784347834402\n",
            "Iteration 175 Loss = 4.767121319598744\n",
            "Iteration 176 Loss = 4.66864111792567\n",
            "Iteration 177 Loss = 4.572296359420844\n",
            "Iteration 178 Loss = 4.47804068928616\n",
            "Iteration 179 Loss = 4.385828758991043\n",
            "Iteration 180 Loss = 4.295616204428426\n",
            "Iteration 181 Loss = 4.207359624544891\n",
            "Iteration 182 Loss = 4.121016560434776\n",
            "Iteration 183 Loss = 4.036545474888042\n",
            "Iteration 184 Loss = 3.953905732382148\n",
            "Iteration 185 Loss = 3.873057579508251\n",
            "Iteration 186 Loss = 3.793962125822315\n",
            "Iteration 187 Loss = 3.71658132511189\n",
            "Iteration 188 Loss = 3.640877957069591\n",
            "Iteration 189 Loss = 3.5668156093643493\n",
            "Iteration 190 Loss = 3.4943586601019216\n",
            "Iteration 191 Loss = 3.423472260666104\n",
            "Iteration 192 Loss = 3.3541223189324354\n",
            "Iteration 193 Loss = 3.2862754828463094\n",
            "Iteration 194 Loss = 3.2198991243575623\n",
            "Iteration 195 Loss = 3.1549613237038225\n",
            "Iteration 196 Loss = 3.0914308540350093\n",
            "Iteration 197 Loss = 3.0292771663716254\n",
            "Iteration 198 Loss = 2.968470374889578\n",
            "Iteration 199 Loss = 2.9089812425244097\n",
            "Iteration 200 Loss = 2.850781166888049\n",
            "Iteration 201 Loss = 2.7938421664912867\n",
            "Iteration 202 Loss = 2.7381368672653412\n",
            "Iteration 203 Loss = 2.6836384893759546\n",
            "Iteration 204 Loss = 2.6303208343238227\n",
            "Iteration 205 Loss = 2.57815827232497\n",
            "Iteration 206 Loss = 2.5271257299650847\n",
            "Iteration 207 Loss = 2.47719867812191\n",
            "Iteration 208 Loss = 2.4283531201497315\n",
            "Iteration 209 Loss = 2.380565580320416\n",
            "Iteration 210 Loss = 2.333813092515344\n",
            "Iteration 211 Loss = 2.288073189162836\n",
            "Iteration 212 Loss = 2.2433238904157093\n",
            "Iteration 213 Loss = 2.1995436935637636\n",
            "Iteration 214 Loss = 2.1567115626761195\n",
            "Iteration 215 Loss = 2.1148069184683917\n",
            "Iteration 216 Loss = 2.073809628389811\n",
            "Iteration 217 Loss = 2.0336999969255714\n",
            "Iteration 218 Loss = 1.9944587561096443\n",
            "Iteration 219 Loss = 1.956067056243582\n",
            "Iteration 220 Loss = 1.9185064568167711\n",
            "Iteration 221 Loss = 1.8817589176238019\n",
            "Iteration 222 Loss = 1.8458067900746633\n",
            "Iteration 223 Loss = 1.8106328086935692\n",
            "Iteration 224 Loss = 1.7762200828023218\n",
            "Iteration 225 Loss = 1.7425520883842256\n",
            "Iteration 226 Loss = 1.7096126601246064\n",
            "Iteration 227 Loss = 1.6773859836241336\n",
            "Iteration 228 Loss = 1.6458565877811553\n",
            "Iteration 229 Loss = 1.6150093373394045\n",
            "Iteration 230 Loss = 1.5848294255974837\n",
            "Iteration 231 Loss = 1.5553023672766033\n",
            "Iteration 232 Loss = 1.5264139915431583\n",
            "Iteration 233 Loss = 1.498150435182761\n",
            "Iteration 234 Loss = 1.4704981359224518\n",
            "Iteration 235 Loss = 1.4434438258978775\n",
            "Iteration 236 Loss = 1.4169745252622865\n",
            "Iteration 237 Loss = 1.3910775359342376\n",
            "Iteration 238 Loss = 1.3657404354810594\n",
            "Iteration 239 Loss = 1.3409510711350816\n",
            "Iteration 240 Loss = 1.3166975539397516\n",
            "Iteration 241 Loss = 1.2929682530228361\n",
            "Iteration 242 Loss = 1.2697517899939232\n",
            "Iteration 243 Loss = 1.2470370334635799\n",
            "Iteration 244 Loss = 1.2248130936814432\n",
            "Iteration 245 Loss = 1.2030693172907456\n",
            "Iteration 246 Loss = 1.1817952821966757\n",
            "Iteration 247 Loss = 1.1609807925461642\n",
            "Iteration 248 Loss = 1.1406158738166312\n",
            "Iteration 249 Loss = 1.120690768011355\n",
            "Iteration 250 Loss = 1.1011959289591369\n",
            "Iteration 251 Loss = 1.082122017715999\n",
            "Iteration 252 Loss = 1.063459898066705\n",
            "Iteration 253 Loss = 1.0452006321239353\n",
            "Iteration 254 Loss = 1.027335476022989\n",
            "Iteration 255 Loss = 1.0098558757099443\n",
            "Iteration 256 Loss = 0.9927534628212595\n",
            "Iteration 257 Loss = 0.9760200506528162\n",
            "Iteration 258 Loss = 0.9596476302164543\n",
            "Iteration 259 Loss = 0.9436283663821321\n",
            "Iteration 260 Loss = 0.9279545941038038\n",
            "Iteration 261 Loss = 0.912618814727264\n",
            "Iteration 262 Loss = 0.8976136923781063\n",
            "Iteration 263 Loss = 0.8829320504281065\n",
            "Iteration 264 Loss = 0.868566868038323\n",
            "Iteration 265 Loss = 0.8545112767772118\n",
            "Iteration 266 Loss = 0.8407585573121906\n",
            "Iteration 267 Loss = 0.8273021361729908\n",
            "Iteration 268 Loss = 0.8141355825852914\n",
            "Iteration 269 Loss = 0.8012526053730822\n",
            "Iteration 270 Loss = 0.7886470499282611\n",
            "Iteration 271 Loss = 0.7763128952460359\n",
            "Iteration 272 Loss = 0.764244251024652\n",
            "Iteration 273 Loss = 0.752435354828111\n",
            "Iteration 274 Loss = 0.740880569310462\n",
            "Iteration 275 Loss = 0.7295743795003431\n",
            "Iteration 276 Loss = 0.7185113901444956\n",
            "Iteration 277 Loss = 0.70768632310891\n",
            "Iteration 278 Loss = 0.6970940148364201\n",
            "Iteration 279 Loss = 0.6867294138594636\n",
            "Iteration 280 Loss = 0.6765875783668533\n",
            "Iteration 281 Loss = 0.666663673823355\n",
            "Iteration 282 Loss = 0.6569529706409499\n",
            "Iteration 283 Loss = 0.6474508419006397\n",
            "Iteration 284 Loss = 0.6381527611237164\n",
            "Iteration 285 Loss = 0.6290543000914064\n",
            "Iteration 286 Loss = 0.6201511267118482\n",
            "Iteration 287 Loss = 0.6114390029333706\n",
            "Iteration 288 Loss = 0.6029137827030807\n",
            "Iteration 289 Loss = 0.5945714099697449\n",
            "Iteration 290 Loss = 0.586407916730048\n",
            "Iteration 291 Loss = 0.5784194211172465\n",
            "Iteration 292 Loss = 0.5706021255313234\n",
            "Iteration 293 Loss = 0.5629523148097257\n",
            "Iteration 294 Loss = 0.5554663544378194\n",
            "Iteration 295 Loss = 0.5481406887981816\n",
            "Iteration 296 Loss = 0.5409718394579124\n",
            "Iteration 297 Loss = 0.533956403493107\n",
            "Iteration 298 Loss = 0.5270910518497153\n",
            "Iteration 299 Loss = 0.5203725277399752\n",
            "Iteration 300 Loss = 0.5137976450736602\n",
            "Iteration 301 Loss = 0.5073632869233858\n",
            "Iteration 302 Loss = 0.5010664040232125\n",
            "Iteration 303 Loss = 0.4949040132998682\n",
            "Iteration 304 Loss = 0.48887319643583205\n",
            "Iteration 305 Loss = 0.4829710984636188\n",
            "Iteration 306 Loss = 0.4771949263905842\n",
            "Iteration 307 Loss = 0.471541947853576\n",
            "Iteration 308 Loss = 0.46600948980278817\n",
            "Iteration 309 Loss = 0.46059493721419065\n",
            "Iteration 310 Loss = 0.4552957318299165\n",
            "Iteration 311 Loss = 0.4501093709259705\n",
            "Iteration 312 Loss = 0.44503340610670905\n",
            "Iteration 313 Loss = 0.4400654421254901\n",
            "Iteration 314 Loss = 0.4352031357309038\n",
            "Iteration 315 Loss = 0.4304441945380675\n",
            "Iteration 316 Loss = 0.42578637592442337\n",
            "Iteration 317 Loss = 0.4212274859494895\n",
            "Iteration 318 Loss = 0.41676537829808097\n",
            "Iteration 319 Loss = 0.4123979532464554\n",
            "Iteration 320 Loss = 0.4081231566509131\n",
            "Iteration 321 Loss = 0.4039389789583433\n",
            "Iteration 322 Loss = 0.3998434542382571\n",
            "Iteration 323 Loss = 0.3958346592358241\n",
            "Iteration 324 Loss = 0.3919107124454781\n",
            "Iteration 325 Loss = 0.38806977320462277\n",
            "Iteration 326 Loss = 0.38431004080701503\n",
            "Iteration 327 Loss = 0.38062975363539064\n",
            "Iteration 328 Loss = 0.377027188312929\n",
            "Iteration 329 Loss = 0.37350065887312617\n",
            "Iteration 330 Loss = 0.37004851594769583\n",
            "Iteration 331 Loss = 0.36666914597209654\n",
            "Iteration 332 Loss = 0.36336097040830373\n",
            "Iteration 333 Loss = 0.36012244498446\n",
            "Iteration 334 Loss = 0.3569520589510245\n",
            "Iteration 335 Loss = 0.3538483343530682\n",
            "Iteration 336 Loss = 0.3508098253183786\n",
            "Iteration 337 Loss = 0.3478351173610005\n",
            "Iteration 338 Loss = 0.34492282669991275\n",
            "Iteration 339 Loss = 0.3420715995924819\n",
            "Iteration 340 Loss = 0.33928011168238975\n",
            "Iteration 341 Loss = 0.3365470673617202\n",
            "Iteration 342 Loss = 0.3338711991468907\n",
            "Iteration 343 Loss = 0.33125126706812624\n",
            "Iteration 344 Loss = 0.3286860580721944\n",
            "Iteration 345 Loss = 0.3261743854381016\n",
            "Iteration 346 Loss = 0.3237150882054737\n",
            "Iteration 347 Loss = 0.3213070306153466\n",
            "Iteration 348 Loss = 0.3189491015630929\n",
            "Iteration 349 Loss = 0.3166402140632344\n",
            "Iteration 350 Loss = 0.31437930472585424\n",
            "Iteration 351 Loss = 0.3121653332443929\n",
            "Iteration 352 Loss = 0.3099972818945533\n",
            "Iteration 353 Loss = 0.3078741550440869\n",
            "Iteration 354 Loss = 0.3057949786732186\n",
            "Iteration 355 Loss = 0.3037587999054916\n",
            "Iteration 356 Loss = 0.3017646865487792\n",
            "Iteration 357 Loss = 0.29981172664627637\n",
            "Iteration 358 Loss = 0.29789902803723345\n",
            "Iteration 359 Loss = 0.29602571792722\n",
            "Iteration 360 Loss = 0.29419094246772576\n",
            "Iteration 361 Loss = 0.2923938663448841\n",
            "Iteration 362 Loss = 0.29063367237711724\n",
            "Iteration 363 Loss = 0.2889095611215295\n",
            "Iteration 364 Loss = 0.2872207504888301\n",
            "Iteration 365 Loss = 0.28556647536662616\n",
            "Iteration 366 Loss = 0.2839459872508899\n",
            "Iteration 367 Loss = 0.28235855388542047\n",
            "Iteration 368 Loss = 0.2808034589091417\n",
            "Iteration 369 Loss = 0.27928000151105037\n",
            "Iteration 370 Loss = 0.2777874960926549\n",
            "Iteration 371 Loss = 0.27632527193774437\n",
            "Iteration 372 Loss = 0.27489267288932523\n",
            "Iteration 373 Loss = 0.2734890570335696\n",
            "Iteration 374 Loss = 0.27211379639062566\n",
            "Iteration 375 Loss = 0.2707662766121413\n",
            "Iteration 376 Loss = 0.2694458966853491\n",
            "Iteration 377 Loss = 0.268152068643582\n",
            "Iteration 378 Loss = 0.2668842172830676\n",
            "Iteration 379 Loss = 0.2656417798858738\n",
            "Iteration 380 Loss = 0.2644242059488616\n",
            "Iteration 381 Loss = 0.2632309569185348\n",
            "Iteration 382 Loss = 0.26206150593163297\n",
            "Iteration 383 Loss = 0.2609153375613615\n",
            "Iteration 384 Loss = 0.25979194756913027\n",
            "Iteration 385 Loss = 0.25869084266168063\n",
            "Iteration 386 Loss = 0.25761154025348554\n",
            "Iteration 387 Loss = 0.25655356823430536\n",
            "Iteration 388 Loss = 0.2555164647417919\n",
            "Iteration 389 Loss = 0.2544997779390286\n",
            "Iteration 390 Loss = 0.25350306579689985\n",
            "Iteration 391 Loss = 0.2525258958811797\n",
            "Iteration 392 Loss = 0.251567845144252\n",
            "Iteration 393 Loss = 0.25062849972134355\n",
            "Iteration 394 Loss = 0.24970745473118425\n",
            "Iteration 395 Loss = 0.24880431408099457\n",
            "Iteration 396 Loss = 0.24791869027570476\n",
            "Iteration 397 Loss = 0.24705020423131788\n",
            "Iteration 398 Loss = 0.24619848509232067\n",
            "Iteration 399 Loss = 0.2453631700530626\n",
            "Iteration 400 Loss = 0.24454390418300778\n",
            "Iteration 401 Loss = 0.24374034025578473\n",
            "Iteration 402 Loss = 0.24295213858194392\n",
            "Iteration 403 Loss = 0.242178966845349\n",
            "Iteration 404 Loss = 0.24142049994311296\n",
            "Iteration 405 Loss = 0.2406764198290172\n",
            "Iteration 406 Loss = 0.23994641536032602\n",
            "Iteration 407 Loss = 0.23923018214792502\n",
            "Iteration 408 Loss = 0.23852742240971495\n",
            "Iteration 409 Loss = 0.2378378448271864\n",
            "Iteration 410 Loss = 0.23716116440510998\n",
            "Iteration 411 Loss = 0.23649710233426918\n",
            "Iteration 412 Loss = 0.2358453858571774\n",
            "Iteration 413 Loss = 0.23520574813670592\n",
            "Iteration 414 Loss = 0.23457792812756711\n",
            "Iteration 415 Loss = 0.23396167045058236\n",
            "Iteration 416 Loss = 0.23335672526968682\n",
            "Iteration 417 Loss = 0.2327628481715977\n",
            "Iteration 418 Loss = 0.23217980004809688\n",
            "Iteration 419 Loss = 0.23160734698087124\n",
            "Iteration 420 Loss = 0.2310452601288469\n",
            "Iteration 421 Loss = 0.2304933156179782\n",
            "Iteration 422 Loss = 0.22995129443341888\n",
            "Iteration 423 Loss = 0.2294189823140427\n",
            "Iteration 424 Loss = 0.2288961696492491\n",
            "Iteration 425 Loss = 0.2283826513780093\n",
            "Iteration 426 Loss = 0.22787822689010626\n",
            "Iteration 427 Loss = 0.22738269992951501\n",
            "Iteration 428 Loss = 0.2268958784998852\n",
            "Iteration 429 Loss = 0.22641757477207014\n",
            "Iteration 430 Loss = 0.22594760499366598\n",
            "Iteration 431 Loss = 0.22548578940051564\n",
            "Iteration 432 Loss = 0.22503195213013127\n",
            "Iteration 433 Loss = 0.22458592113699716\n",
            "Iteration 434 Loss = 0.2241475281097125\n",
            "Iteration 435 Loss = 0.2237166083899308\n",
            "Iteration 436 Loss = 0.2232930008930606\n",
            "Iteration 437 Loss = 0.22287654803068466\n",
            "Iteration 438 Loss = 0.2224670956346696\n",
            "Iteration 439 Loss = 0.22206449288291594\n",
            "Iteration 440 Loss = 0.22166859222672358\n",
            "Iteration 441 Loss = 0.22127924931973222\n",
            "Iteration 442 Loss = 0.22089632294840747\n",
            "Iteration 443 Loss = 0.22051967496403183\n",
            "Iteration 444 Loss = 0.2201491702161773\n",
            "Iteration 445 Loss = 0.2197846764876152\n",
            "Iteration 446 Loss = 0.21942606443064464\n",
            "Iteration 447 Loss = 0.2190732075048014\n",
            "Iteration 448 Loss = 0.21872598191591858\n",
            "Iteration 449 Loss = 0.21838426655651044\n",
            "Iteration 450 Loss = 0.21804794294745\n",
            "Iteration 451 Loss = 0.21771689518091455\n",
            "Iteration 452 Loss = 0.21739100986456528\n",
            "Iteration 453 Loss = 0.21707017606694534\n",
            "Iteration 454 Loss = 0.21675428526405682\n",
            "Iteration 455 Loss = 0.21644323128710075\n",
            "Iteration 456 Loss = 0.21613691027135104\n",
            "Iteration 457 Loss = 0.21583522060613788\n",
            "Iteration 458 Loss = 0.2155380628859179\n",
            "Iteration 459 Loss = 0.21524533986240382\n",
            "Iteration 460 Loss = 0.21495695639773812\n",
            "Iteration 461 Loss = 0.21467281941867727\n",
            "Iteration 462 Loss = 0.21439283787177585\n",
            "Iteration 463 Loss = 0.21411692267953966\n",
            "Iteration 464 Loss = 0.21384498669753285\n",
            "Iteration 465 Loss = 0.2135769446724151\n",
            "Iteration 466 Loss = 0.21331271320089096\n",
            "Iteration 467 Loss = 0.2130522106895488\n",
            "Iteration 468 Loss = 0.21279535731557242\n",
            "Iteration 469 Loss = 0.21254207498830582\n",
            "Iteration 470 Loss = 0.2122922873116502\n",
            "Iteration 471 Loss = 0.21204591954728189\n",
            "Iteration 472 Loss = 0.21180289857866325\n",
            "Iteration 473 Loss = 0.21156315287583805\n",
            "Iteration 474 Loss = 0.21132661246099108\n",
            "Iteration 475 Loss = 0.21109320887475316\n",
            "Iteration 476 Loss = 0.21086287514324145\n",
            "Iteration 477 Loss = 0.2106355457458108\n",
            "Iteration 478 Loss = 0.21041115658350792\n",
            "Iteration 479 Loss = 0.21018964494821007\n",
            "Iteration 480 Loss = 0.20997094949243295\n",
            "Iteration 481 Loss = 0.2097550101997948\n",
            "Iteration 482 Loss = 0.2095417683561199\n",
            "Iteration 483 Loss = 0.20933116652117348\n",
            "Iteration 484 Loss = 0.20912314850100366\n",
            "Iteration 485 Loss = 0.20891765932089046\n",
            "Iteration 486 Loss = 0.20871464519887833\n",
            "Iteration 487 Loss = 0.20851405351988325\n",
            "Iteration 488 Loss = 0.20831583281036312\n",
            "Iteration 489 Loss = 0.2081199327135387\n",
            "Iteration 490 Loss = 0.20792630396514902\n",
            "Iteration 491 Loss = 0.2077348983697376\n",
            "Iteration 492 Loss = 0.20754566877744984\n",
            "Iteration 493 Loss = 0.20735856906133596\n",
            "Iteration 494 Loss = 0.20717355409514623\n",
            "Iteration 495 Loss = 0.20699057973160837\n",
            "Iteration 496 Loss = 0.20680960278117508\n",
            "Iteration 497 Loss = 0.206630580991236\n",
            "Iteration 498 Loss = 0.2064534730257775\n",
            "Iteration 499 Loss = 0.20627823844548637\n",
            "Iteration 500 Loss = 0.20610483768828455\n",
            "Iteration 501 Loss = 0.20593323205028638\n",
            "Iteration 502 Loss = 0.2057633836671687\n",
            "Iteration 503 Loss = 0.20559525549594762\n",
            "Iteration 504 Loss = 0.20542881129714782\n",
            "Iteration 505 Loss = 0.20526401561736063\n",
            "Iteration 506 Loss = 0.20510083377218144\n",
            "Iteration 507 Loss = 0.20493923182951462\n",
            "Iteration 508 Loss = 0.20477917659324438\n",
            "Iteration 509 Loss = 0.20462063558725874\n",
            "Iteration 510 Loss = 0.2044635770398183\n",
            "Iteration 511 Loss = 0.2043079698682679\n",
            "Iteration 512 Loss = 0.20415378366407705\n",
            "Iteration 513 Loss = 0.20400098867820712\n",
            "Iteration 514 Loss = 0.20384955580679545\n",
            "Iteration 515 Loss = 0.20369945657714983\n",
            "Iteration 516 Loss = 0.20355066313404807\n",
            "Iteration 517 Loss = 0.20340314822633299\n",
            "Iteration 518 Loss = 0.20325688519380192\n",
            "Iteration 519 Loss = 0.2031118479543765\n",
            "Iteration 520 Loss = 0.2029680109915542\n",
            "Iteration 521 Loss = 0.2028253493421309\n",
            "Iteration 522 Loss = 0.20268383858419067\n",
            "Iteration 523 Loss = 0.202543454825355\n",
            "Iteration 524 Loss = 0.202404174691289\n",
            "Iteration 525 Loss = 0.20226597531445467\n",
            "Iteration 526 Loss = 0.2021288343231117\n",
            "Iteration 527 Loss = 0.20199272983055302\n",
            "Iteration 528 Loss = 0.20185764042457727\n",
            "Iteration 529 Loss = 0.20172354515718877\n",
            "Iteration 530 Loss = 0.20159042353451945\n",
            "Iteration 531 Loss = 0.20145825550697272\n",
            "Iteration 532 Loss = 0.20132702145957876\n",
            "Iteration 533 Loss = 0.20119670220255934\n",
            "Iteration 534 Loss = 0.20106727896209972\n",
            "Iteration 535 Loss = 0.20093873337131807\n",
            "Iteration 536 Loss = 0.20081104746143258\n",
            "Iteration 537 Loss = 0.2006842036531196\n",
            "Iteration 538 Loss = 0.20055818474805973\n",
            "Iteration 539 Loss = 0.20043297392066758\n",
            "Iteration 540 Loss = 0.20030855471000064\n",
            "Iteration 541 Loss = 0.20018491101184355\n",
            "Iteration 542 Loss = 0.20006202707096543\n",
            "Iteration 543 Loss = 0.19993988747354421\n",
            "Iteration 544 Loss = 0.19981847713975454\n",
            "Iteration 545 Loss = 0.1996977813165201\n",
            "Iteration 546 Loss = 0.19957778557041858\n",
            "Iteration 547 Loss = 0.19945847578074374\n",
            "Iteration 548 Loss = 0.19933983813271866\n",
            "Iteration 549 Loss = 0.1992218591108526\n",
            "Iteration 550 Loss = 0.19910452549244603\n",
            "Iteration 551 Loss = 0.1989878243412351\n",
            "Iteration 552 Loss = 0.19887174300117302\n",
            "Iteration 553 Loss = 0.1987562690903483\n",
            "Iteration 554 Loss = 0.1986413904950343\n",
            "Iteration 555 Loss = 0.198527095363867\n",
            "Iteration 556 Loss = 0.19841337210215065\n",
            "Iteration 557 Loss = 0.19830020936628584\n",
            "Iteration 558 Loss = 0.19818759605832006\n",
            "Iteration 559 Loss = 0.1980755213206139\n",
            "Iteration 560 Loss = 0.19796397453062614\n",
            "Iteration 561 Loss = 0.19785294529581038\n",
            "Iteration 562 Loss = 0.19774242344862147\n",
            "Iteration 563 Loss = 0.1976323990416334\n",
            "Iteration 564 Loss = 0.19752286234275956\n",
            "Iteration 565 Loss = 0.19741380383057888\n",
            "Iteration 566 Loss = 0.19730521418976482\n",
            "Iteration 567 Loss = 0.19719708430660945\n",
            "Iteration 568 Loss = 0.19708940526464752\n",
            "Iteration 569 Loss = 0.19698216834037643\n",
            "Iteration 570 Loss = 0.19687536499906616\n",
            "Iteration 571 Loss = 0.1967689868906629\n",
            "Iteration 572 Loss = 0.19666302584577985\n",
            "Iteration 573 Loss = 0.19655747387177588\n",
            "Iteration 574 Loss = 0.19645232314891817\n",
            "Iteration 575 Loss = 0.19634756602663078\n",
            "Iteration 576 Loss = 0.19624319501982154\n",
            "Iteration 577 Loss = 0.19613920280529037\n",
            "Iteration 578 Loss = 0.19603558221821546\n",
            "Iteration 579 Loss = 0.1959323262487158\n",
            "Iteration 580 Loss = 0.19582942803848757\n",
            "Iteration 581 Loss = 0.19572688087751403\n",
            "Iteration 582 Loss = 0.19562467820084703\n",
            "Iteration 583 Loss = 0.1955228135854586\n",
            "Iteration 584 Loss = 0.19542128074715953\n",
            "Iteration 585 Loss = 0.19532007353758646\n",
            "Iteration 586 Loss = 0.19521918594125381\n",
            "Iteration 587 Loss = 0.19511861207266923\n",
            "Iteration 588 Loss = 0.1950183461735123\n",
            "Iteration 589 Loss = 0.19491838260987412\n",
            "Iteration 590 Loss = 0.19481871586955665\n",
            "Iteration 591 Loss = 0.19471934055943094\n",
            "Iteration 592 Loss = 0.194620251402854\n",
            "Iteration 593 Loss = 0.1945214432371389\n",
            "Iteration 594 Loss = 0.19442291101108142\n",
            "Iteration 595 Loss = 0.19432464978254138\n",
            "Iteration 596 Loss = 0.19422665471607425\n",
            "Iteration 597 Loss = 0.1941289210806173\n",
            "Iteration 598 Loss = 0.19403144424722055\n",
            "Iteration 599 Loss = 0.19393421968683386\n",
            "Iteration 600 Loss = 0.19383724296813676\n",
            "Iteration 601 Loss = 0.1937405097554181\n",
            "Iteration 602 Loss = 0.19364401580650034\n",
            "Iteration 603 Loss = 0.19354775697070964\n",
            "Iteration 604 Loss = 0.19345172918689008\n",
            "Iteration 605 Loss = 0.19335592848146063\n",
            "Iteration 606 Loss = 0.19326035096651412\n",
            "Iteration 607 Loss = 0.19316499283795727\n",
            "Iteration 608 Loss = 0.19306985037369384\n",
            "Iteration 609 Loss = 0.19297491993184235\n",
            "Iteration 610 Loss = 0.19288019794899605\n",
            "Iteration 611 Loss = 0.19278568093852036\n",
            "Iteration 612 Loss = 0.19269136548888566\n",
            "Iteration 613 Loss = 0.19259724826203778\n",
            "Iteration 614 Loss = 0.19250332599180348\n",
            "Iteration 615 Loss = 0.19240959548233008\n",
            "Iteration 616 Loss = 0.1923160536065588\n",
            "Iteration 617 Loss = 0.19222269730473282\n",
            "Iteration 618 Loss = 0.1921295235829357\n",
            "Iteration 619 Loss = 0.1920365295116631\n",
            "Iteration 620 Loss = 0.19194371222442438\n",
            "Iteration 621 Loss = 0.19185106891637488\n",
            "Iteration 622 Loss = 0.19175859684297866\n",
            "Iteration 623 Loss = 0.19166629331869917\n",
            "Iteration 624 Loss = 0.191574155715719\n",
            "Iteration 625 Loss = 0.19148218146268675\n",
            "Iteration 626 Loss = 0.19139036804349321\n",
            "Iteration 627 Loss = 0.1912987129960701\n",
            "Iteration 628 Loss = 0.19120721391121895\n",
            "Iteration 629 Loss = 0.19111586843146364\n",
            "Iteration 630 Loss = 0.1910246742499268\n",
            "Iteration 631 Loss = 0.1909336291092329\n",
            "Iteration 632 Loss = 0.19084273080043385\n",
            "Iteration 633 Loss = 0.19075197716195744\n",
            "Iteration 634 Loss = 0.19066136607857953\n",
            "Iteration 635 Loss = 0.19057089548041875\n",
            "Iteration 636 Loss = 0.19048056334195182\n",
            "Iteration 637 Loss = 0.19039036768105116\n",
            "Iteration 638 Loss = 0.19030030655804292\n",
            "Iteration 639 Loss = 0.19021037807478677\n",
            "Iteration 640 Loss = 0.19012058037377355\n",
            "Iteration 641 Loss = 0.19003091163724384\n",
            "Iteration 642 Loss = 0.18994137008632514\n",
            "Iteration 643 Loss = 0.18985195398018898\n",
            "Iteration 644 Loss = 0.18976266161522454\n",
            "Iteration 645 Loss = 0.18967349132423067\n",
            "Iteration 646 Loss = 0.18958444147562759\n",
            "Iteration 647 Loss = 0.18949551047268157\n",
            "Iteration 648 Loss = 0.1894066967527506\n",
            "Iteration 649 Loss = 0.1893179987865442\n",
            "Iteration 650 Loss = 0.1892294150773994\n",
            "Iteration 651 Loss = 0.18914094416057287\n",
            "Iteration 652 Loss = 0.18905258460254878\n",
            "Iteration 653 Loss = 0.18896433500036044\n",
            "Iteration 654 Loss = 0.1888761939809286\n",
            "Iteration 655 Loss = 0.1887881602004117\n",
            "Iteration 656 Loss = 0.18870023234357233\n",
            "Iteration 657 Loss = 0.1886124091231564\n",
            "Iteration 658 Loss = 0.18852468927928592\n",
            "Iteration 659 Loss = 0.18843707157886525\n",
            "Iteration 660 Loss = 0.18834955481499965\n",
            "Iteration 661 Loss = 0.18826213780642703\n",
            "Iteration 662 Loss = 0.18817481939696207\n",
            "Iteration 663 Loss = 0.18808759845495188\n",
            "Iteration 664 Loss = 0.18800047387274404\n",
            "Iteration 665 Loss = 0.18791344456616532\n",
            "Iteration 666 Loss = 0.18782650947401278\n",
            "Iteration 667 Loss = 0.18773966755755583\n",
            "Iteration 668 Loss = 0.18765291780004748\n",
            "Iteration 669 Loss = 0.18756625920624925\n",
            "Iteration 670 Loss = 0.18747969080196275\n",
            "Iteration 671 Loss = 0.18739321163357422\n",
            "Iteration 672 Loss = 0.18730682076760824\n",
            "Iteration 673 Loss = 0.18722051729029018\n",
            "Iteration 674 Loss = 0.18713430030711908\n",
            "Iteration 675 Loss = 0.18704816894244974\n",
            "Iteration 676 Loss = 0.18696212233908358\n",
            "Iteration 677 Loss = 0.18687615965786905\n",
            "Iteration 678 Loss = 0.18679028007730875\n",
            "Iteration 679 Loss = 0.18670448279317806\n",
            "Iteration 680 Loss = 0.18661876701815028\n",
            "Iteration 681 Loss = 0.18653313198142885\n",
            "Iteration 682 Loss = 0.1864475769283903\n",
            "Iteration 683 Loss = 0.1863621011202326\n",
            "Iteration 684 Loss = 0.18627670383363257\n",
            "Iteration 685 Loss = 0.18619138436040966\n",
            "Iteration 686 Loss = 0.18610614200719794\n",
            "Iteration 687 Loss = 0.1860209760951249\n",
            "Iteration 688 Loss = 0.18593588595949662\n",
            "Iteration 689 Loss = 0.18585087094949054\n",
            "Iteration 690 Loss = 0.18576593042785494\n",
            "Iteration 691 Loss = 0.18568106377061439\n",
            "Iteration 692 Loss = 0.18559627036678236\n",
            "Iteration 693 Loss = 0.185511549618078\n",
            "Iteration 694 Loss = 0.18542690093865305\n",
            "Iteration 695 Loss = 0.18534232375481977\n",
            "Iteration 696 Loss = 0.1852578175047895\n",
            "Iteration 697 Loss = 0.18517338163841282\n",
            "Iteration 698 Loss = 0.18508901561692842\n",
            "Iteration 699 Loss = 0.18500471891271614\n",
            "Iteration 700 Loss = 0.18492049100905428\n",
            "Iteration 701 Loss = 0.18483633139988478\n",
            "Iteration 702 Loss = 0.184752239589581\n",
            "Iteration 703 Loss = 0.18466821509272227\n",
            "Iteration 704 Loss = 0.18458425743387202\n",
            "Iteration 705 Loss = 0.1845003661473621\n",
            "Iteration 706 Loss = 0.18441654077708025\n",
            "Iteration 707 Loss = 0.1843327808762641\n",
            "Iteration 708 Loss = 0.18424908600729667\n",
            "Iteration 709 Loss = 0.18416545574150991\n",
            "Iteration 710 Loss = 0.18408188965899003\n",
            "Iteration 711 Loss = 0.1839983873483878\n",
            "Iteration 712 Loss = 0.183914948406733\n",
            "Iteration 713 Loss = 0.18383157243925227\n",
            "Iteration 714 Loss = 0.18374825905919337\n",
            "Iteration 715 Loss = 0.183665007887648\n",
            "Iteration 716 Loss = 0.1835818185533859\n",
            "Iteration 717 Loss = 0.18349869069268512\n",
            "Iteration 718 Loss = 0.18341562394917116\n",
            "Iteration 719 Loss = 0.18333261797365705\n",
            "Iteration 720 Loss = 0.18324967242398774\n",
            "Iteration 721 Loss = 0.18316678696488842\n",
            "Iteration 722 Loss = 0.18308396126781432\n",
            "Iteration 723 Loss = 0.1830011950108057\n",
            "Iteration 724 Loss = 0.18291848787834505\n",
            "Iteration 725 Loss = 0.18283583956121777\n",
            "Iteration 726 Loss = 0.18275324975637502\n",
            "Iteration 727 Loss = 0.18267071816680092\n",
            "Iteration 728 Loss = 0.18258824450138086\n",
            "Iteration 729 Loss = 0.1825058284747754\n",
            "Iteration 730 Loss = 0.18242346980729338\n",
            "Iteration 731 Loss = 0.1823411682247708\n",
            "Iteration 732 Loss = 0.18225892345845068\n",
            "Iteration 733 Loss = 0.1821767352448661\n",
            "Iteration 734 Loss = 0.1820946033257257\n",
            "Iteration 735 Loss = 0.18201252744780105\n",
            "Iteration 736 Loss = 0.18193050736281827\n",
            "Iteration 737 Loss = 0.1818485428273502\n",
            "Iteration 738 Loss = 0.18176663360271045\n",
            "Iteration 739 Loss = 0.18168477945485284\n",
            "Iteration 740 Loss = 0.18160298015426876\n",
            "Iteration 741 Loss = 0.18152123547589089\n",
            "Iteration 742 Loss = 0.18143954519899616\n",
            "Iteration 743 Loss = 0.1813579091071116\n",
            "Iteration 744 Loss = 0.18127632698792256\n",
            "Iteration 745 Loss = 0.18119479863318325\n",
            "Iteration 746 Loss = 0.18111332383862774\n",
            "Iteration 747 Loss = 0.18103190240388528\n",
            "Iteration 748 Loss = 0.18095053413239404\n",
            "Iteration 749 Loss = 0.18086921883132118\n",
            "Iteration 750 Loss = 0.18078795631147934\n",
            "Iteration 751 Loss = 0.180706746387251\n",
            "Iteration 752 Loss = 0.18062558887650862\n",
            "Iteration 753 Loss = 0.18054448360054032\n",
            "Iteration 754 Loss = 0.18046343038397647\n",
            "Iteration 755 Loss = 0.1803824290547163\n",
            "Iteration 756 Loss = 0.18030147944385788\n",
            "Iteration 757 Loss = 0.1802205813856294\n",
            "Iteration 758 Loss = 0.18013973471732084\n",
            "Iteration 759 Loss = 0.18005893927921796\n",
            "Iteration 760 Loss = 0.1799781949145372\n",
            "Iteration 761 Loss = 0.179897501469364\n",
            "Iteration 762 Loss = 0.17981685879258852\n",
            "Iteration 763 Loss = 0.17973626673584656\n",
            "Iteration 764 Loss = 0.17965572515345976\n",
            "Iteration 765 Loss = 0.17957523390237792\n",
            "Iteration 766 Loss = 0.1794947928421217\n",
            "Iteration 767 Loss = 0.17941440183472734\n",
            "Iteration 768 Loss = 0.17933406074469313\n",
            "Iteration 769 Loss = 0.17925376943892463\n",
            "Iteration 770 Loss = 0.17917352778668458\n",
            "Iteration 771 Loss = 0.17909333565954022\n",
            "Iteration 772 Loss = 0.1790131929313152\n",
            "Iteration 773 Loss = 0.17893309947803987\n",
            "Iteration 774 Loss = 0.17885305517790334\n",
            "Iteration 775 Loss = 0.17877305991120887\n",
            "Iteration 776 Loss = 0.17869311356032566\n",
            "Iteration 777 Loss = 0.1786132160096463\n",
            "Iteration 778 Loss = 0.17853336714554172\n",
            "Iteration 779 Loss = 0.17845356685632033\n",
            "Iteration 780 Loss = 0.17837381503218386\n",
            "Iteration 781 Loss = 0.17829411156518904\n",
            "Iteration 782 Loss = 0.1782144563492056\n",
            "Iteration 783 Loss = 0.17813484927987835\n",
            "Iteration 784 Loss = 0.17805529025458874\n",
            "Iteration 785 Loss = 0.17797577917241755\n",
            "Iteration 786 Loss = 0.1778963159341077\n",
            "Iteration 787 Loss = 0.17781690044202889\n",
            "Iteration 788 Loss = 0.17773753260014286\n",
            "Iteration 789 Loss = 0.1776582123139688\n",
            "Iteration 790 Loss = 0.17757893949054968\n",
            "Iteration 791 Loss = 0.17749971403841985\n",
            "Iteration 792 Loss = 0.1774205358675725\n",
            "Iteration 793 Loss = 0.1773414048894289\n",
            "Iteration 794 Loss = 0.17726232101680736\n",
            "Iteration 795 Loss = 0.177183284163893\n",
            "Iteration 796 Loss = 0.17710429424620838\n",
            "Iteration 797 Loss = 0.1770253511805854\n",
            "Iteration 798 Loss = 0.1769464548851364\n",
            "Iteration 799 Loss = 0.17686760527922685\n",
            "Iteration 800 Loss = 0.17678880228344937\n",
            "Iteration 801 Loss = 0.17671004581959565\n",
            "Iteration 802 Loss = 0.17663133581063195\n",
            "Iteration 803 Loss = 0.17655267218067372\n",
            "Iteration 804 Loss = 0.1764740548549606\n",
            "Iteration 805 Loss = 0.17639548375983305\n",
            "Iteration 806 Loss = 0.17631695882270765\n",
            "Iteration 807 Loss = 0.1762384799720547\n",
            "Iteration 808 Loss = 0.1761600471373761\n",
            "Iteration 809 Loss = 0.17608166024918168\n",
            "Iteration 810 Loss = 0.1760033192389695\n",
            "Iteration 811 Loss = 0.1759250240392036\n",
            "Iteration 812 Loss = 0.1758467745832932\n",
            "Iteration 813 Loss = 0.17576857080557348\n",
            "Iteration 814 Loss = 0.17569041264128435\n",
            "Iteration 815 Loss = 0.1756123000265522\n",
            "Iteration 816 Loss = 0.1755342328983708\n",
            "Iteration 817 Loss = 0.1754562111945824\n",
            "Iteration 818 Loss = 0.1753782348538597\n",
            "Iteration 819 Loss = 0.1753003038156882\n",
            "Iteration 820 Loss = 0.175222418020349\n",
            "Iteration 821 Loss = 0.1751445774089013\n",
            "Iteration 822 Loss = 0.17506678192316658\n",
            "Iteration 823 Loss = 0.17498903150571155\n",
            "Iteration 824 Loss = 0.17491132609983265\n",
            "Iteration 825 Loss = 0.17483366564954092\n",
            "Iteration 826 Loss = 0.1747560500995459\n",
            "Iteration 827 Loss = 0.17467847939524106\n",
            "Iteration 828 Loss = 0.17460095348269006\n",
            "Iteration 829 Loss = 0.1745234723086107\n",
            "Iteration 830 Loss = 0.17444603582036328\n",
            "Iteration 831 Loss = 0.17436864396593504\n",
            "Iteration 832 Loss = 0.17429129669392773\n",
            "Iteration 833 Loss = 0.1742139939535446\n",
            "Iteration 834 Loss = 0.17413673569457722\n",
            "Iteration 835 Loss = 0.17405952186739299\n",
            "Iteration 836 Loss = 0.17398235242292365\n",
            "Iteration 837 Loss = 0.17390522731265265\n",
            "Iteration 838 Loss = 0.17382814648860356\n",
            "Iteration 839 Loss = 0.1737511099033284\n",
            "Iteration 840 Loss = 0.17367411750989714\n",
            "Iteration 841 Loss = 0.17359716926188645\n",
            "Iteration 842 Loss = 0.17352026511336852\n",
            "Iteration 843 Loss = 0.1734434050189011\n",
            "Iteration 844 Loss = 0.17336658893351728\n",
            "Iteration 845 Loss = 0.17328981681271516\n",
            "Iteration 846 Loss = 0.1732130886124482\n",
            "Iteration 847 Loss = 0.17313640428911525\n",
            "Iteration 848 Loss = 0.17305976379955232\n",
            "Iteration 849 Loss = 0.17298316710102168\n",
            "Iteration 850 Loss = 0.17290661415120429\n",
            "Iteration 851 Loss = 0.17283010490819029\n",
            "Iteration 852 Loss = 0.17275363933047094\n",
            "Iteration 853 Loss = 0.17267721737692887\n",
            "Iteration 854 Loss = 0.17260083900683276\n",
            "Iteration 855 Loss = 0.17252450417982515\n",
            "Iteration 856 Loss = 0.17244821285591816\n",
            "Iteration 857 Loss = 0.17237196499548385\n",
            "Iteration 858 Loss = 0.1722957605592478\n",
            "Iteration 859 Loss = 0.1722195995082806\n",
            "Iteration 860 Loss = 0.17214348180399106\n",
            "Iteration 861 Loss = 0.17206740740811985\n",
            "Iteration 862 Loss = 0.17199137628273162\n",
            "Iteration 863 Loss = 0.17191538839020915\n",
            "Iteration 864 Loss = 0.17183944369324552\n",
            "Iteration 865 Loss = 0.1717635421548388\n",
            "Iteration 866 Loss = 0.17168768373828538\n",
            "Iteration 867 Loss = 0.17161186840717335\n",
            "Iteration 868 Loss = 0.17153609612537662\n",
            "Iteration 869 Loss = 0.17146036685704985\n",
            "Iteration 870 Loss = 0.17138468056662148\n",
            "Iteration 871 Loss = 0.1713090372187886\n",
            "Iteration 872 Loss = 0.1712334367785112\n",
            "Iteration 873 Loss = 0.17115787921100714\n",
            "Iteration 874 Loss = 0.1710823644817466\n",
            "Iteration 875 Loss = 0.17100689255644627\n",
            "Iteration 876 Loss = 0.17093146340106552\n",
            "Iteration 877 Loss = 0.17085607698180072\n",
            "Iteration 878 Loss = 0.1707807332650798\n",
            "Iteration 879 Loss = 0.17070543221755896\n",
            "Iteration 880 Loss = 0.1706301738061165\n",
            "Iteration 881 Loss = 0.17055495799784917\n",
            "Iteration 882 Loss = 0.17047978476006787\n",
            "Iteration 883 Loss = 0.17040465406029212\n",
            "Iteration 884 Loss = 0.17032956586624698\n",
            "Iteration 885 Loss = 0.1702545201458587\n",
            "Iteration 886 Loss = 0.17017951686724986\n",
            "Iteration 887 Loss = 0.17010455599873625\n",
            "Iteration 888 Loss = 0.17002963750882336\n",
            "Iteration 889 Loss = 0.1699547613661999\n",
            "Iteration 890 Loss = 0.16987992753973827\n",
            "Iteration 891 Loss = 0.16980513599848662\n",
            "Iteration 892 Loss = 0.16973038671166873\n",
            "Iteration 893 Loss = 0.16965567964867836\n",
            "Iteration 894 Loss = 0.1695810147790765\n",
            "Iteration 895 Loss = 0.16950639207258855\n",
            "Iteration 896 Loss = 0.1694318114990998\n",
            "Iteration 897 Loss = 0.16935727302865372\n",
            "Iteration 898 Loss = 0.16928277663144722\n",
            "Iteration 899 Loss = 0.1692083222778291\n",
            "Iteration 900 Loss = 0.169133909938296\n",
            "Iteration 901 Loss = 0.16905953958349018\n",
            "Iteration 902 Loss = 0.16898521118419568\n",
            "Iteration 903 Loss = 0.16891092471133623\n",
            "Iteration 904 Loss = 0.16883668013597306\n",
            "Iteration 905 Loss = 0.16876247742930095\n",
            "Iteration 906 Loss = 0.1686883165626458\n",
            "Iteration 907 Loss = 0.16861419750746331\n",
            "Iteration 908 Loss = 0.1685401202353353\n",
            "Iteration 909 Loss = 0.16846608471796692\n",
            "Iteration 910 Loss = 0.16839209092718516\n",
            "Iteration 911 Loss = 0.16831813883493674\n",
            "Iteration 912 Loss = 0.16824422841328437\n",
            "Iteration 913 Loss = 0.16817035963440566\n",
            "Iteration 914 Loss = 0.1680965324705907\n",
            "Iteration 915 Loss = 0.16802274689423952\n",
            "Iteration 916 Loss = 0.1679490028778607\n",
            "Iteration 917 Loss = 0.16787530039406826\n",
            "Iteration 918 Loss = 0.16780163941558066\n",
            "Iteration 919 Loss = 0.1677280199152178\n",
            "Iteration 920 Loss = 0.16765444186590067\n",
            "Iteration 921 Loss = 0.16758090524064725\n",
            "Iteration 922 Loss = 0.16750741001257258\n",
            "Iteration 923 Loss = 0.1674339561548857\n",
            "Iteration 924 Loss = 0.16736054364088893\n",
            "Iteration 925 Loss = 0.16728717244397517\n",
            "Iteration 926 Loss = 0.16721384253762672\n",
            "Iteration 927 Loss = 0.16714055389541355\n",
            "Iteration 928 Loss = 0.16706730649099144\n",
            "Iteration 929 Loss = 0.16699410029810055\n",
            "Iteration 930 Loss = 0.16692093529056415\n",
            "Iteration 931 Loss = 0.1668478114422866\n",
            "Iteration 932 Loss = 0.16677472872725219\n",
            "Iteration 933 Loss = 0.16670168711952338\n",
            "Iteration 934 Loss = 0.16662868659323993\n",
            "Iteration 935 Loss = 0.16655572712261663\n",
            "Iteration 936 Loss = 0.16648280868194268\n",
            "Iteration 937 Loss = 0.16640993124558034\n",
            "Iteration 938 Loss = 0.16633709478796313\n",
            "Iteration 939 Loss = 0.16626429928359462\n",
            "Iteration 940 Loss = 0.1661915447070476\n",
            "Iteration 941 Loss = 0.1661188310329627\n",
            "Iteration 942 Loss = 0.16604615823604713\n",
            "Iteration 943 Loss = 0.16597352629107318\n",
            "Iteration 944 Loss = 0.16590093517287763\n",
            "Iteration 945 Loss = 0.1658283848563602\n",
            "Iteration 946 Loss = 0.16575587531648292\n",
            "Iteration 947 Loss = 0.16568340652826852\n",
            "Iteration 948 Loss = 0.16561097846679948\n",
            "Iteration 949 Loss = 0.16553859110721744\n",
            "Iteration 950 Loss = 0.165466244424722\n",
            "Iteration 951 Loss = 0.16539393839456895\n",
            "Iteration 952 Loss = 0.16532167299207068\n",
            "Iteration 953 Loss = 0.1652494481925941\n",
            "Iteration 954 Loss = 0.1651772639715606\n",
            "Iteration 955 Loss = 0.1651051203044443\n",
            "Iteration 956 Loss = 0.1650330171667716\n",
            "Iteration 957 Loss = 0.16496095453412057\n",
            "Iteration 958 Loss = 0.16488893238211957\n",
            "Iteration 959 Loss = 0.1648169506864464\n",
            "Iteration 960 Loss = 0.16474500942282844\n",
            "Iteration 961 Loss = 0.16467310856704065\n",
            "Iteration 962 Loss = 0.16460124809490578\n",
            "Iteration 963 Loss = 0.1645294279822924\n",
            "Iteration 964 Loss = 0.16445764820511552\n",
            "Iteration 965 Loss = 0.1643859087393351\n",
            "Iteration 966 Loss = 0.16431420956095533\n",
            "Iteration 967 Loss = 0.16424255064602447\n",
            "Iteration 968 Loss = 0.16417093197063323\n",
            "Iteration 969 Loss = 0.16409935351091537\n",
            "Iteration 970 Loss = 0.16402781524304552\n",
            "Iteration 971 Loss = 0.1639563171432402\n",
            "Iteration 972 Loss = 0.1638848591877556\n",
            "Iteration 973 Loss = 0.163813441352888\n",
            "Iteration 974 Loss = 0.16374206361497376\n",
            "Iteration 975 Loss = 0.16367072595038618\n",
            "Iteration 976 Loss = 0.16359942833553814\n",
            "Iteration 977 Loss = 0.1635281707468789\n",
            "Iteration 978 Loss = 0.16345695316089567\n",
            "Iteration 979 Loss = 0.16338577555411155\n",
            "Iteration 980 Loss = 0.16331463790308554\n",
            "Iteration 981 Loss = 0.16324354018441212\n",
            "Iteration 982 Loss = 0.16317248237472032\n",
            "Iteration 983 Loss = 0.16310146445067422\n",
            "Iteration 984 Loss = 0.1630304863889714\n",
            "Iteration 985 Loss = 0.16295954816634273\n",
            "Iteration 986 Loss = 0.1628886497595524\n",
            "Iteration 987 Loss = 0.16281779114539705\n",
            "Iteration 988 Loss = 0.16274697230070506\n",
            "Iteration 989 Loss = 0.16267619320233667\n",
            "Iteration 990 Loss = 0.1626054538271837\n",
            "Iteration 991 Loss = 0.162534754152168\n",
            "Iteration 992 Loss = 0.16246409415424293\n",
            "Iteration 993 Loss = 0.16239347381039065\n",
            "Iteration 994 Loss = 0.162322893097624\n",
            "Iteration 995 Loss = 0.1622523519929843\n",
            "Iteration 996 Loss = 0.1621818504735421\n",
            "Iteration 997 Loss = 0.1621113885163964\n",
            "Iteration 998 Loss = 0.16204096609867422\n",
            "Iteration 999 Loss = 0.16197058319753072\n",
            "Iteration 1000 Loss = 0.16190023979014811\n",
            "Iteration 1001 Loss = 0.16182993585373606\n",
            "Iteration 1002 Loss = 0.1617596713655309\n",
            "Iteration 1003 Loss = 0.16168944630279555\n",
            "Iteration 1004 Loss = 0.1616192606428189\n",
            "Iteration 1005 Loss = 0.1615491143629161\n",
            "Iteration 1006 Loss = 0.16147900744042695\n",
            "Iteration 1007 Loss = 0.16140893985271781\n",
            "Iteration 1008 Loss = 0.1613389115771791\n",
            "Iteration 1009 Loss = 0.16126892259122633\n",
            "Iteration 1010 Loss = 0.1611989728722994\n",
            "Iteration 1011 Loss = 0.16112906239786215\n",
            "Iteration 1012 Loss = 0.16105919114540243\n",
            "Iteration 1013 Loss = 0.16098935909243184\n",
            "Iteration 1014 Loss = 0.16091956621648565\n",
            "Iteration 1015 Loss = 0.1608498124951214\n",
            "Iteration 1016 Loss = 0.16078009790592074\n",
            "Iteration 1017 Loss = 0.16071042242648648\n",
            "Iteration 1018 Loss = 0.16064078603444504\n",
            "Iteration 1019 Loss = 0.16057118870744472\n",
            "Iteration 1020 Loss = 0.1605016304231559\n",
            "Iteration 1021 Loss = 0.1604321111592703\n",
            "Iteration 1022 Loss = 0.16036263089350172\n",
            "Iteration 1023 Loss = 0.160293189603585\n",
            "Iteration 1024 Loss = 0.1602237872672761\n",
            "Iteration 1025 Loss = 0.16015442386235226\n",
            "Iteration 1026 Loss = 0.16008509936661086\n",
            "Iteration 1027 Loss = 0.16001581375787038\n",
            "Iteration 1028 Loss = 0.15994656701396925\n",
            "Iteration 1029 Loss = 0.15987735911276665\n",
            "Iteration 1030 Loss = 0.1598081900321411\n",
            "Iteration 1031 Loss = 0.15973905974999147\n",
            "Iteration 1032 Loss = 0.1596699682442361\n",
            "Iteration 1033 Loss = 0.15960091549281233\n",
            "Iteration 1034 Loss = 0.15953190147367777\n",
            "Iteration 1035 Loss = 0.15946292616480828\n",
            "Iteration 1036 Loss = 0.15939398954419942\n",
            "Iteration 1037 Loss = 0.15932509158986544\n",
            "Iteration 1038 Loss = 0.1592562322798387\n",
            "Iteration 1039 Loss = 0.15918741159217092\n",
            "Iteration 1040 Loss = 0.15911862950493158\n",
            "Iteration 1041 Loss = 0.15904988599620887\n",
            "Iteration 1042 Loss = 0.15898118104410874\n",
            "Iteration 1043 Loss = 0.15891251462675532\n",
            "Iteration 1044 Loss = 0.15884388672229088\n",
            "Iteration 1045 Loss = 0.15877529730887469\n",
            "Iteration 1046 Loss = 0.15870674636468413\n",
            "Iteration 1047 Loss = 0.15863823386791373\n",
            "Iteration 1048 Loss = 0.1585697597967758\n",
            "Iteration 1049 Loss = 0.15850132412949944\n",
            "Iteration 1050 Loss = 0.1584329268443312\n",
            "Iteration 1051 Loss = 0.1583645679195341\n",
            "Iteration 1052 Loss = 0.15829624733338843\n",
            "Iteration 1053 Loss = 0.15822796506419132\n",
            "Iteration 1054 Loss = 0.15815972109025647\n",
            "Iteration 1055 Loss = 0.15809151538991376\n",
            "Iteration 1056 Loss = 0.15802334794151005\n",
            "Iteration 1057 Loss = 0.1579552187234078\n",
            "Iteration 1058 Loss = 0.15788712771398675\n",
            "Iteration 1059 Loss = 0.1578190748916417\n",
            "Iteration 1060 Loss = 0.15775106023478452\n",
            "Iteration 1061 Loss = 0.1576830837218423\n",
            "Iteration 1062 Loss = 0.15761514533125787\n",
            "Iteration 1063 Loss = 0.15754724504149042\n",
            "Iteration 1064 Loss = 0.15747938283101448\n",
            "Iteration 1065 Loss = 0.15741155867832032\n",
            "Iteration 1066 Loss = 0.15734377256191368\n",
            "Iteration 1067 Loss = 0.15727602446031522\n",
            "Iteration 1068 Loss = 0.15720831435206198\n",
            "Iteration 1069 Loss = 0.1571406422157052\n",
            "Iteration 1070 Loss = 0.15707300802981192\n",
            "Iteration 1071 Loss = 0.15700541177296418\n",
            "Iteration 1072 Loss = 0.1569378534237591\n",
            "Iteration 1073 Loss = 0.15687033296080838\n",
            "Iteration 1074 Loss = 0.15680285036273883\n",
            "Iteration 1075 Loss = 0.15673540560819224\n",
            "Iteration 1076 Loss = 0.15666799867582515\n",
            "Iteration 1077 Loss = 0.1566006295443082\n",
            "Iteration 1078 Loss = 0.15653329819232736\n",
            "Iteration 1079 Loss = 0.15646600459858298\n",
            "Iteration 1080 Loss = 0.1563987487417893\n",
            "Iteration 1081 Loss = 0.15633153060067573\n",
            "Iteration 1082 Loss = 0.1562643501539857\n",
            "Iteration 1083 Loss = 0.15619720738047696\n",
            "Iteration 1084 Loss = 0.1561301022589212\n",
            "Iteration 1085 Loss = 0.15606303476810504\n",
            "Iteration 1086 Loss = 0.15599600488682827\n",
            "Iteration 1087 Loss = 0.1559290125939054\n",
            "Iteration 1088 Loss = 0.15586205786816457\n",
            "Iteration 1089 Loss = 0.1557951406884485\n",
            "Iteration 1090 Loss = 0.15572826103361284\n",
            "Iteration 1091 Loss = 0.15566141888252766\n",
            "Iteration 1092 Loss = 0.15559461421407714\n",
            "Iteration 1093 Loss = 0.1555278470071583\n",
            "Iteration 1094 Loss = 0.15546111724068265\n",
            "Iteration 1095 Loss = 0.15539442489357522\n",
            "Iteration 1096 Loss = 0.1553277699447741\n",
            "Iteration 1097 Loss = 0.15526115237323165\n",
            "Iteration 1098 Loss = 0.15519457215791338\n",
            "Iteration 1099 Loss = 0.15512802927779806\n",
            "Iteration 1100 Loss = 0.15506152371187842\n",
            "Iteration 1101 Loss = 0.15499505543915987\n",
            "Iteration 1102 Loss = 0.1549286244386619\n",
            "Iteration 1103 Loss = 0.15486223068941654\n",
            "Iteration 1104 Loss = 0.1547958741704701\n",
            "Iteration 1105 Loss = 0.15472955486088083\n",
            "Iteration 1106 Loss = 0.15466327273972114\n",
            "Iteration 1107 Loss = 0.15459702778607604\n",
            "Iteration 1108 Loss = 0.15453081997904397\n",
            "Iteration 1109 Loss = 0.15446464929773607\n",
            "Iteration 1110 Loss = 0.15439851572127694\n",
            "Iteration 1111 Loss = 0.15433241922880403\n",
            "Iteration 1112 Loss = 0.1542663597994673\n",
            "Iteration 1113 Loss = 0.1542003374124302\n",
            "Iteration 1114 Loss = 0.15413435204686868\n",
            "Iteration 1115 Loss = 0.15406840368197197\n",
            "Iteration 1116 Loss = 0.15400249229694174\n",
            "Iteration 1117 Loss = 0.15393661787099253\n",
            "Iteration 1118 Loss = 0.15387078038335164\n",
            "Iteration 1119 Loss = 0.15380497981325905\n",
            "Iteration 1120 Loss = 0.1537392161399677\n",
            "Iteration 1121 Loss = 0.15367348934274294\n",
            "Iteration 1122 Loss = 0.15360779940086258\n",
            "Iteration 1123 Loss = 0.1535421462936176\n",
            "Iteration 1124 Loss = 0.15347653000031092\n",
            "Iteration 1125 Loss = 0.1534109505002585\n",
            "Iteration 1126 Loss = 0.15334540777278843\n",
            "Iteration 1127 Loss = 0.15327990179724174\n",
            "Iteration 1128 Loss = 0.15321443255297149\n",
            "Iteration 1129 Loss = 0.15314900001934345\n",
            "Iteration 1130 Loss = 0.15308360417573577\n",
            "Iteration 1131 Loss = 0.15301824500153857\n",
            "Iteration 1132 Loss = 0.1529529224761553\n",
            "Iteration 1133 Loss = 0.15288763657900073\n",
            "Iteration 1134 Loss = 0.1528223872895026\n",
            "Iteration 1135 Loss = 0.15275717458710047\n",
            "Iteration 1136 Loss = 0.15269199845124648\n",
            "Iteration 1137 Loss = 0.15262685886140492\n",
            "Iteration 1138 Loss = 0.15256175579705272\n",
            "Iteration 1139 Loss = 0.15249668923767798\n",
            "Iteration 1140 Loss = 0.15243165916278217\n",
            "Iteration 1141 Loss = 0.15236666555187817\n",
            "Iteration 1142 Loss = 0.15230170838449097\n",
            "Iteration 1143 Loss = 0.15223678764015822\n",
            "Iteration 1144 Loss = 0.15217190329842922\n",
            "Iteration 1145 Loss = 0.1521070553388655\n",
            "Iteration 1146 Loss = 0.15204224374104086\n",
            "Iteration 1147 Loss = 0.1519774684845402\n",
            "Iteration 1148 Loss = 0.1519127295489618\n",
            "Iteration 1149 Loss = 0.151848026913915\n",
            "Iteration 1150 Loss = 0.1517833605590213\n",
            "Iteration 1151 Loss = 0.1517187304639145\n",
            "Iteration 1152 Loss = 0.15165413660823976\n",
            "Iteration 1153 Loss = 0.15158957897165468\n",
            "Iteration 1154 Loss = 0.15152505753382872\n",
            "Iteration 1155 Loss = 0.15146057227444273\n",
            "Iteration 1156 Loss = 0.15139612317318996\n",
            "Iteration 1157 Loss = 0.15133171020977543\n",
            "Iteration 1158 Loss = 0.15126733336391562\n",
            "Iteration 1159 Loss = 0.1512029926153393\n",
            "Iteration 1160 Loss = 0.1511386879437868\n",
            "Iteration 1161 Loss = 0.15107441932901042\n",
            "Iteration 1162 Loss = 0.1510101867507738\n",
            "Iteration 1163 Loss = 0.1509459901888528\n",
            "Iteration 1164 Loss = 0.15088182962303495\n",
            "Iteration 1165 Loss = 0.15081770503311964\n",
            "Iteration 1166 Loss = 0.15075361639891718\n",
            "Iteration 1167 Loss = 0.15068956370025044\n",
            "Iteration 1168 Loss = 0.15062554691695376\n",
            "Iteration 1169 Loss = 0.15056156602887277\n",
            "Iteration 1170 Loss = 0.15049762101586528\n",
            "Iteration 1171 Loss = 0.15043371185780038\n",
            "Iteration 1172 Loss = 0.15036983853455896\n",
            "Iteration 1173 Loss = 0.15030600102603345\n",
            "Iteration 1174 Loss = 0.15024219931212762\n",
            "Iteration 1175 Loss = 0.15017843337275766\n",
            "Iteration 1176 Loss = 0.15011470318785025\n",
            "Iteration 1177 Loss = 0.1500510087373443\n",
            "Iteration 1178 Loss = 0.1499873500011902\n",
            "Iteration 1179 Loss = 0.1499237269593498\n",
            "Iteration 1180 Loss = 0.14986013959179606\n",
            "Iteration 1181 Loss = 0.1497965878785142\n",
            "Iteration 1182 Loss = 0.1497330717995004\n",
            "Iteration 1183 Loss = 0.14966959133476238\n",
            "Iteration 1184 Loss = 0.14960614646431963\n",
            "Iteration 1185 Loss = 0.14954273716820263\n",
            "Iteration 1186 Loss = 0.14947936342645382\n",
            "Iteration 1187 Loss = 0.1494160252191268\n",
            "Iteration 1188 Loss = 0.14935272252628637\n",
            "Iteration 1189 Loss = 0.14928945532800905\n",
            "Iteration 1190 Loss = 0.14922622360438292\n",
            "Iteration 1191 Loss = 0.1491630273355069\n",
            "Iteration 1192 Loss = 0.1490998665014917\n",
            "Iteration 1193 Loss = 0.1490367410824593\n",
            "Iteration 1194 Loss = 0.14897365105854296\n",
            "Iteration 1195 Loss = 0.14891059640988746\n",
            "Iteration 1196 Loss = 0.14884757711664867\n",
            "Iteration 1197 Loss = 0.14878459315899406\n",
            "Iteration 1198 Loss = 0.14872164451710204\n",
            "Iteration 1199 Loss = 0.14865873117116268\n",
            "Iteration 1200 Loss = 0.14859585310137707\n",
            "Iteration 1201 Loss = 0.148533010287958\n",
            "Iteration 1202 Loss = 0.14847020271112887\n",
            "Iteration 1203 Loss = 0.14840743035112475\n",
            "Iteration 1204 Loss = 0.14834469318819218\n",
            "Iteration 1205 Loss = 0.14828199120258848\n",
            "Iteration 1206 Loss = 0.14821932437458227\n",
            "Iteration 1207 Loss = 0.1481566926844536\n",
            "Iteration 1208 Loss = 0.14809409611249383\n",
            "Iteration 1209 Loss = 0.1480315346390048\n",
            "Iteration 1210 Loss = 0.1479690082443009\n",
            "Iteration 1211 Loss = 0.14790651690870626\n",
            "Iteration 1212 Loss = 0.14784406061255678\n",
            "Iteration 1213 Loss = 0.14778163933619973\n",
            "Iteration 1214 Loss = 0.14771925305999342\n",
            "Iteration 1215 Loss = 0.14765690176430685\n",
            "Iteration 1216 Loss = 0.14759458542952086\n",
            "Iteration 1217 Loss = 0.14753230403602688\n",
            "Iteration 1218 Loss = 0.1474700575642278\n",
            "Iteration 1219 Loss = 0.14740784599453752\n",
            "Iteration 1220 Loss = 0.14734566930738077\n",
            "Iteration 1221 Loss = 0.14728352748319376\n",
            "Iteration 1222 Loss = 0.1472214205024235\n",
            "Iteration 1223 Loss = 0.14715934834552813\n",
            "Iteration 1224 Loss = 0.1470973109929773\n",
            "Iteration 1225 Loss = 0.14703530842525103\n",
            "Iteration 1226 Loss = 0.14697334062284054\n",
            "Iteration 1227 Loss = 0.1469114075662485\n",
            "Iteration 1228 Loss = 0.14684950923598838\n",
            "Iteration 1229 Loss = 0.1467876456125844\n",
            "Iteration 1230 Loss = 0.14672581667657228\n",
            "Iteration 1231 Loss = 0.14666402240849838\n",
            "Iteration 1232 Loss = 0.14660226278892008\n",
            "Iteration 1233 Loss = 0.14654053779840603\n",
            "Iteration 1234 Loss = 0.1464788474175355\n",
            "Iteration 1235 Loss = 0.146417191626899\n",
            "Iteration 1236 Loss = 0.146355570407098\n",
            "Iteration 1237 Loss = 0.14629398373874486\n",
            "Iteration 1238 Loss = 0.14623243160246255\n",
            "Iteration 1239 Loss = 0.14617091397888582\n",
            "Iteration 1240 Loss = 0.1461094308486593\n",
            "Iteration 1241 Loss = 0.14604798219243947\n",
            "Iteration 1242 Loss = 0.1459865679908933\n",
            "Iteration 1243 Loss = 0.14592518822469855\n",
            "Iteration 1244 Loss = 0.14586384287454415\n",
            "Iteration 1245 Loss = 0.14580253192112974\n",
            "Iteration 1246 Loss = 0.14574125534516622\n",
            "Iteration 1247 Loss = 0.14568001312737466\n",
            "Iteration 1248 Loss = 0.14561880524848783\n",
            "Iteration 1249 Loss = 0.14555763168924848\n",
            "Iteration 1250 Loss = 0.14549649243041152\n",
            "Iteration 1251 Loss = 0.14543538745274062\n",
            "Iteration 1252 Loss = 0.1453743167370128\n",
            "Iteration 1253 Loss = 0.1453132802640142\n",
            "Iteration 1254 Loss = 0.1452522780145421\n",
            "Iteration 1255 Loss = 0.14519130996940496\n",
            "Iteration 1256 Loss = 0.14513037610942187\n",
            "Iteration 1257 Loss = 0.1450694764154226\n",
            "Iteration 1258 Loss = 0.14500861086824784\n",
            "Iteration 1259 Loss = 0.14494777944874943\n",
            "Iteration 1260 Loss = 0.14488698213778922\n",
            "Iteration 1261 Loss = 0.1448262189162403\n",
            "Iteration 1262 Loss = 0.1447654897649868\n",
            "Iteration 1263 Loss = 0.14470479466492292\n",
            "Iteration 1264 Loss = 0.14464413359695416\n",
            "Iteration 1265 Loss = 0.14458350654199673\n",
            "Iteration 1266 Loss = 0.1445229134809772\n",
            "Iteration 1267 Loss = 0.14446235439483346\n",
            "Iteration 1268 Loss = 0.14440182926451373\n",
            "Iteration 1269 Loss = 0.14434133807097668\n",
            "Iteration 1270 Loss = 0.14428088079519255\n",
            "Iteration 1271 Loss = 0.14422045741814132\n",
            "Iteration 1272 Loss = 0.1441600679208145\n",
            "Iteration 1273 Loss = 0.1440997122842137\n",
            "Iteration 1274 Loss = 0.14403939048935166\n",
            "Iteration 1275 Loss = 0.14397910251725127\n",
            "Iteration 1276 Loss = 0.14391884834894694\n",
            "Iteration 1277 Loss = 0.1438586279654826\n",
            "Iteration 1278 Loss = 0.14379844134791384\n",
            "Iteration 1279 Loss = 0.14373828847730644\n",
            "Iteration 1280 Loss = 0.1436781693347372\n",
            "Iteration 1281 Loss = 0.14361808390129277\n",
            "Iteration 1282 Loss = 0.14355803215807117\n",
            "Iteration 1283 Loss = 0.14349801408618082\n",
            "Iteration 1284 Loss = 0.1434380296667408\n",
            "Iteration 1285 Loss = 0.14337807888088083\n",
            "Iteration 1286 Loss = 0.14331816170974104\n",
            "Iteration 1287 Loss = 0.14325827813447242\n",
            "Iteration 1288 Loss = 0.14319842813623632\n",
            "Iteration 1289 Loss = 0.14313861169620465\n",
            "Iteration 1290 Loss = 0.14307882879556028\n",
            "Iteration 1291 Loss = 0.1430190794154964\n",
            "Iteration 1292 Loss = 0.1429593635372167\n",
            "Iteration 1293 Loss = 0.14289968114193574\n",
            "Iteration 1294 Loss = 0.14284003221087818\n",
            "Iteration 1295 Loss = 0.14278041672527944\n",
            "Iteration 1296 Loss = 0.1427208346663857\n",
            "Iteration 1297 Loss = 0.14266128601545333\n",
            "Iteration 1298 Loss = 0.14260177075374944\n",
            "Iteration 1299 Loss = 0.14254228886255169\n",
            "Iteration 1300 Loss = 0.142482840323148\n",
            "Iteration 1301 Loss = 0.1424234251168372\n",
            "Iteration 1302 Loss = 0.14236404322492827\n",
            "Iteration 1303 Loss = 0.14230469462874087\n",
            "Iteration 1304 Loss = 0.1422453793096051\n",
            "Iteration 1305 Loss = 0.14218609724886155\n",
            "Iteration 1306 Loss = 0.14212684842786114\n",
            "Iteration 1307 Loss = 0.14206763282796583\n",
            "Iteration 1308 Loss = 0.1420084504305472\n",
            "Iteration 1309 Loss = 0.14194930121698798\n",
            "Iteration 1310 Loss = 0.1418901851686811\n",
            "Iteration 1311 Loss = 0.14183110226702966\n",
            "Iteration 1312 Loss = 0.1417720524934479\n",
            "Iteration 1313 Loss = 0.14171303582935987\n",
            "Iteration 1314 Loss = 0.14165405225620029\n",
            "Iteration 1315 Loss = 0.14159510175541407\n",
            "Iteration 1316 Loss = 0.14153618430845719\n",
            "Iteration 1317 Loss = 0.14147729989679514\n",
            "Iteration 1318 Loss = 0.14141844850190471\n",
            "Iteration 1319 Loss = 0.14135963010527233\n",
            "Iteration 1320 Loss = 0.1413008446883951\n",
            "Iteration 1321 Loss = 0.14124209223278084\n",
            "Iteration 1322 Loss = 0.14118337271994724\n",
            "Iteration 1323 Loss = 0.14112468613142273\n",
            "Iteration 1324 Loss = 0.14106603244874588\n",
            "Iteration 1325 Loss = 0.14100741165346584\n",
            "Iteration 1326 Loss = 0.14094882372714185\n",
            "Iteration 1327 Loss = 0.14089026865134352\n",
            "Iteration 1328 Loss = 0.14083174640765117\n",
            "Iteration 1329 Loss = 0.14077325697765516\n",
            "Iteration 1330 Loss = 0.14071480034295605\n",
            "Iteration 1331 Loss = 0.14065637648516538\n",
            "Iteration 1332 Loss = 0.14059798538590404\n",
            "Iteration 1333 Loss = 0.14053962702680375\n",
            "Iteration 1334 Loss = 0.1404813013895069\n",
            "Iteration 1335 Loss = 0.14042300845566544\n",
            "Iteration 1336 Loss = 0.1403647482069423\n",
            "Iteration 1337 Loss = 0.14030652062501042\n",
            "Iteration 1338 Loss = 0.14024832569155252\n",
            "Iteration 1339 Loss = 0.14019016338826257\n",
            "Iteration 1340 Loss = 0.14013203369684404\n",
            "Iteration 1341 Loss = 0.14007393659901093\n",
            "Iteration 1342 Loss = 0.14001587207648783\n",
            "Iteration 1343 Loss = 0.13995784011100887\n",
            "Iteration 1344 Loss = 0.13989984068431902\n",
            "Iteration 1345 Loss = 0.1398418737781733\n",
            "Iteration 1346 Loss = 0.13978393937433686\n",
            "Iteration 1347 Loss = 0.13972603745458545\n",
            "Iteration 1348 Loss = 0.1396681680007045\n",
            "Iteration 1349 Loss = 0.1396103309944901\n",
            "Iteration 1350 Loss = 0.13955252641774826\n",
            "Iteration 1351 Loss = 0.1394947542522954\n",
            "Iteration 1352 Loss = 0.13943701447995832\n",
            "Iteration 1353 Loss = 0.1393793070825732\n",
            "Iteration 1354 Loss = 0.1393216320419878\n",
            "Iteration 1355 Loss = 0.13926398934005849\n",
            "Iteration 1356 Loss = 0.1392063789586531\n",
            "Iteration 1357 Loss = 0.13914880087964882\n",
            "Iteration 1358 Loss = 0.13909125508493334\n",
            "Iteration 1359 Loss = 0.13903374155640455\n",
            "Iteration 1360 Loss = 0.1389762602759702\n",
            "Iteration 1361 Loss = 0.13891881122554875\n",
            "Iteration 1362 Loss = 0.13886139438706832\n",
            "Iteration 1363 Loss = 0.13880400974246734\n",
            "Iteration 1364 Loss = 0.13874665727369406\n",
            "Iteration 1365 Loss = 0.1386893369627075\n",
            "Iteration 1366 Loss = 0.13863204879147628\n",
            "Iteration 1367 Loss = 0.13857479274197937\n",
            "Iteration 1368 Loss = 0.1385175687962054\n",
            "Iteration 1369 Loss = 0.13846037693615393\n",
            "Iteration 1370 Loss = 0.138403217143834\n",
            "Iteration 1371 Loss = 0.13834608940126503\n",
            "Iteration 1372 Loss = 0.13828899369047606\n",
            "Iteration 1373 Loss = 0.13823192999350684\n",
            "Iteration 1374 Loss = 0.13817489829240684\n",
            "Iteration 1375 Loss = 0.13811789856923565\n",
            "Iteration 1376 Loss = 0.13806093080606288\n",
            "Iteration 1377 Loss = 0.13800399498496846\n",
            "Iteration 1378 Loss = 0.13794709108804168\n",
            "Iteration 1379 Loss = 0.13789021909738308\n",
            "Iteration 1380 Loss = 0.1378333789951021\n",
            "Iteration 1381 Loss = 0.13777657076331828\n",
            "Iteration 1382 Loss = 0.13771979438416224\n",
            "Iteration 1383 Loss = 0.13766304983977345\n",
            "Iteration 1384 Loss = 0.13760633711230208\n",
            "Iteration 1385 Loss = 0.13754965618390808\n",
            "Iteration 1386 Loss = 0.13749300703676154\n",
            "Iteration 1387 Loss = 0.13743638965304206\n",
            "Iteration 1388 Loss = 0.13737980401494002\n",
            "Iteration 1389 Loss = 0.13732325010465518\n",
            "Iteration 1390 Loss = 0.13726672790439765\n",
            "Iteration 1391 Loss = 0.13721023739638713\n",
            "Iteration 1392 Loss = 0.1371537785628538\n",
            "Iteration 1393 Loss = 0.13709735138603724\n",
            "Iteration 1394 Loss = 0.13704095584818732\n",
            "Iteration 1395 Loss = 0.13698459193156412\n",
            "Iteration 1396 Loss = 0.1369282596184371\n",
            "Iteration 1397 Loss = 0.13687195889108592\n",
            "Iteration 1398 Loss = 0.13681568973180033\n",
            "Iteration 1399 Loss = 0.13675945212287985\n",
            "Iteration 1400 Loss = 0.13670324604663395\n",
            "Iteration 1401 Loss = 0.1366470714853817\n",
            "Iteration 1402 Loss = 0.13659092842145293\n",
            "Iteration 1403 Loss = 0.1365348168371865\n",
            "Iteration 1404 Loss = 0.1364787367149317\n",
            "Iteration 1405 Loss = 0.13642268803704752\n",
            "Iteration 1406 Loss = 0.13636667078590314\n",
            "Iteration 1407 Loss = 0.13631068494387674\n",
            "Iteration 1408 Loss = 0.1362547304933576\n",
            "Iteration 1409 Loss = 0.13619880741674384\n",
            "Iteration 1410 Loss = 0.13614291569644416\n",
            "Iteration 1411 Loss = 0.13608705531487694\n",
            "Iteration 1412 Loss = 0.13603122625446998\n",
            "Iteration 1413 Loss = 0.13597542849766156\n",
            "Iteration 1414 Loss = 0.13591966202689962\n",
            "Iteration 1415 Loss = 0.13586392682464166\n",
            "Iteration 1416 Loss = 0.1358082228733553\n",
            "Iteration 1417 Loss = 0.13575255015551804\n",
            "Iteration 1418 Loss = 0.13569690865361672\n",
            "Iteration 1419 Loss = 0.1356412983501487\n",
            "Iteration 1420 Loss = 0.1355857192276208\n",
            "Iteration 1421 Loss = 0.13553017126854958\n",
            "Iteration 1422 Loss = 0.13547465445546136\n",
            "Iteration 1423 Loss = 0.13541916877089277\n",
            "Iteration 1424 Loss = 0.13536371419738935\n",
            "Iteration 1425 Loss = 0.1353082907175075\n",
            "Iteration 1426 Loss = 0.13525289831381238\n",
            "Iteration 1427 Loss = 0.13519753696887946\n",
            "Iteration 1428 Loss = 0.13514220666529406\n",
            "Iteration 1429 Loss = 0.13508690738565118\n",
            "Iteration 1430 Loss = 0.13503163911255522\n",
            "Iteration 1431 Loss = 0.13497640182862064\n",
            "Iteration 1432 Loss = 0.13492119551647186\n",
            "Iteration 1433 Loss = 0.13486602015874274\n",
            "Iteration 1434 Loss = 0.1348108757380769\n",
            "Iteration 1435 Loss = 0.13475576223712757\n",
            "Iteration 1436 Loss = 0.13470067963855836\n",
            "Iteration 1437 Loss = 0.13464562792504187\n",
            "Iteration 1438 Loss = 0.13459060707926046\n",
            "Iteration 1439 Loss = 0.13453561708390668\n",
            "Iteration 1440 Loss = 0.13448065792168262\n",
            "Iteration 1441 Loss = 0.13442572957529955\n",
            "Iteration 1442 Loss = 0.13437083202747924\n",
            "Iteration 1443 Loss = 0.13431596526095277\n",
            "Iteration 1444 Loss = 0.13426112925846065\n",
            "Iteration 1445 Loss = 0.13420632400275323\n",
            "Iteration 1446 Loss = 0.13415154947659116\n",
            "Iteration 1447 Loss = 0.1340968056627437\n",
            "Iteration 1448 Loss = 0.13404209254399044\n",
            "Iteration 1449 Loss = 0.13398741010312065\n",
            "Iteration 1450 Loss = 0.13393275832293267\n",
            "Iteration 1451 Loss = 0.1338781371862354\n",
            "Iteration 1452 Loss = 0.1338235466758463\n",
            "Iteration 1453 Loss = 0.13376898677459348\n",
            "Iteration 1454 Loss = 0.13371445746531416\n",
            "Iteration 1455 Loss = 0.1336599587308552\n",
            "Iteration 1456 Loss = 0.13360549055407295\n",
            "Iteration 1457 Loss = 0.13355105291783395\n",
            "Iteration 1458 Loss = 0.13349664580501347\n",
            "Iteration 1459 Loss = 0.1334422691984974\n",
            "Iteration 1460 Loss = 0.1333879230811803\n",
            "Iteration 1461 Loss = 0.13333360743596684\n",
            "Iteration 1462 Loss = 0.13327932224577121\n",
            "Iteration 1463 Loss = 0.13322506749351729\n",
            "Iteration 1464 Loss = 0.13317084316213798\n",
            "Iteration 1465 Loss = 0.13311664923457625\n",
            "Iteration 1466 Loss = 0.13306248569378498\n",
            "Iteration 1467 Loss = 0.13300835252272536\n",
            "Iteration 1468 Loss = 0.13295424970436953\n",
            "Iteration 1469 Loss = 0.13290017722169833\n",
            "Iteration 1470 Loss = 0.13284613505770249\n",
            "Iteration 1471 Loss = 0.13279212319538203\n",
            "Iteration 1472 Loss = 0.13273814161774675\n",
            "Iteration 1473 Loss = 0.13268419030781606\n",
            "Iteration 1474 Loss = 0.13263026924861843\n",
            "Iteration 1475 Loss = 0.1325763784231922\n",
            "Iteration 1476 Loss = 0.13252251781458513\n",
            "Iteration 1477 Loss = 0.13246868740585466\n",
            "Iteration 1478 Loss = 0.13241488718006733\n",
            "Iteration 1479 Loss = 0.13236111712029955\n",
            "Iteration 1480 Loss = 0.132307377209637\n",
            "Iteration 1481 Loss = 0.13225366743117525\n",
            "Iteration 1482 Loss = 0.13219998776801853\n",
            "Iteration 1483 Loss = 0.13214633820328123\n",
            "Iteration 1484 Loss = 0.13209271872008724\n",
            "Iteration 1485 Loss = 0.13203912930156944\n",
            "Iteration 1486 Loss = 0.1319855699308706\n",
            "Iteration 1487 Loss = 0.13193204059114264\n",
            "Iteration 1488 Loss = 0.13187854126554713\n",
            "Iteration 1489 Loss = 0.13182507193725496\n",
            "Iteration 1490 Loss = 0.1317716325894464\n",
            "Iteration 1491 Loss = 0.13171822320531135\n",
            "Iteration 1492 Loss = 0.13166484376804907\n",
            "Iteration 1493 Loss = 0.1316114942608682\n",
            "Iteration 1494 Loss = 0.13155817466698672\n",
            "Iteration 1495 Loss = 0.13150488496963214\n",
            "Iteration 1496 Loss = 0.1314516251520413\n",
            "Iteration 1497 Loss = 0.13139839519746044\n",
            "Iteration 1498 Loss = 0.13134519508914552\n",
            "Iteration 1499 Loss = 0.1312920248103614\n",
            "Iteration 1500 Loss = 0.13123888434438247\n",
            "Iteration 1501 Loss = 0.1311857736744925\n",
            "Iteration 1502 Loss = 0.1311326927839848\n",
            "Iteration 1503 Loss = 0.13107964165616184\n",
            "Iteration 1504 Loss = 0.13102662027433576\n",
            "Iteration 1505 Loss = 0.13097362862182768\n",
            "Iteration 1506 Loss = 0.13092066668196806\n",
            "Iteration 1507 Loss = 0.13086773443809704\n",
            "Iteration 1508 Loss = 0.1308148318735641\n",
            "Iteration 1509 Loss = 0.13076195897172796\n",
            "Iteration 1510 Loss = 0.13070911571595634\n",
            "Iteration 1511 Loss = 0.1306563020896265\n",
            "Iteration 1512 Loss = 0.1306035180761251\n",
            "Iteration 1513 Loss = 0.1305507636588482\n",
            "Iteration 1514 Loss = 0.13049803882120115\n",
            "Iteration 1515 Loss = 0.1304453435465985\n",
            "Iteration 1516 Loss = 0.13039267781846398\n",
            "Iteration 1517 Loss = 0.13034004162023072\n",
            "Iteration 1518 Loss = 0.13028743493534134\n",
            "Iteration 1519 Loss = 0.1302348577472475\n",
            "Iteration 1520 Loss = 0.13018231003941008\n",
            "Iteration 1521 Loss = 0.1301297917952995\n",
            "Iteration 1522 Loss = 0.13007730299839543\n",
            "Iteration 1523 Loss = 0.13002484363218653\n",
            "Iteration 1524 Loss = 0.12997241368017093\n",
            "Iteration 1525 Loss = 0.12992001312585574\n",
            "Iteration 1526 Loss = 0.12986764195275782\n",
            "Iteration 1527 Loss = 0.12981530014440296\n",
            "Iteration 1528 Loss = 0.12976298768432615\n",
            "Iteration 1529 Loss = 0.12971070455607162\n",
            "Iteration 1530 Loss = 0.12965845074319282\n",
            "Iteration 1531 Loss = 0.12960622622925294\n",
            "Iteration 1532 Loss = 0.1295540309978234\n",
            "Iteration 1533 Loss = 0.12950186503248548\n",
            "Iteration 1534 Loss = 0.1294497283168299\n",
            "Iteration 1535 Loss = 0.12939762083445586\n",
            "Iteration 1536 Loss = 0.12934554256897218\n",
            "Iteration 1537 Loss = 0.12929349350399716\n",
            "Iteration 1538 Loss = 0.12924147362315713\n",
            "Iteration 1539 Loss = 0.12918948291008917\n",
            "Iteration 1540 Loss = 0.12913752134843864\n",
            "Iteration 1541 Loss = 0.12908558892186012\n",
            "Iteration 1542 Loss = 0.12903368561401748\n",
            "Iteration 1543 Loss = 0.12898181140858359\n",
            "Iteration 1544 Loss = 0.12892996628924064\n",
            "Iteration 1545 Loss = 0.12887815023968\n",
            "Iteration 1546 Loss = 0.12882636324360225\n",
            "Iteration 1547 Loss = 0.1287746052847167\n",
            "Iteration 1548 Loss = 0.12872287634674215\n",
            "Iteration 1549 Loss = 0.12867117641340647\n",
            "Iteration 1550 Loss = 0.12861950546844678\n",
            "Iteration 1551 Loss = 0.12856786349560895\n",
            "Iteration 1552 Loss = 0.1285162504786484\n",
            "Iteration 1553 Loss = 0.12846466640132911\n",
            "Iteration 1554 Loss = 0.12841311124742483\n",
            "Iteration 1555 Loss = 0.12836158500071787\n",
            "Iteration 1556 Loss = 0.12831008764499993\n",
            "Iteration 1557 Loss = 0.12825861916407152\n",
            "Iteration 1558 Loss = 0.12820717954174285\n",
            "Iteration 1559 Loss = 0.12815576876183232\n",
            "Iteration 1560 Loss = 0.12810438680816805\n",
            "Iteration 1561 Loss = 0.1280530336645869\n",
            "Iteration 1562 Loss = 0.12800170931493532\n",
            "Iteration 1563 Loss = 0.12795041374306793\n",
            "Iteration 1564 Loss = 0.12789914693284934\n",
            "Iteration 1565 Loss = 0.12784790886815242\n",
            "Iteration 1566 Loss = 0.12779669953285958\n",
            "Iteration 1567 Loss = 0.127745518910862\n",
            "Iteration 1568 Loss = 0.1276943669860602\n",
            "Iteration 1569 Loss = 0.1276432437423634\n",
            "Iteration 1570 Loss = 0.1275921491636901\n",
            "Iteration 1571 Loss = 0.12754108323396782\n",
            "Iteration 1572 Loss = 0.12749004593713237\n",
            "Iteration 1573 Loss = 0.12743903725712968\n",
            "Iteration 1574 Loss = 0.12738805717791407\n",
            "Iteration 1575 Loss = 0.12733710568344886\n",
            "Iteration 1576 Loss = 0.12728618275770673\n",
            "Iteration 1577 Loss = 0.12723528838466872\n",
            "Iteration 1578 Loss = 0.12718442254832532\n",
            "Iteration 1579 Loss = 0.1271335852326761\n",
            "Iteration 1580 Loss = 0.12708277642172908\n",
            "Iteration 1581 Loss = 0.12703199609950172\n",
            "Iteration 1582 Loss = 0.12698124425002003\n",
            "Iteration 1583 Loss = 0.12693052085731943\n",
            "Iteration 1584 Loss = 0.12687982590544422\n",
            "Iteration 1585 Loss = 0.1268291593784472\n",
            "Iteration 1586 Loss = 0.1267785212603906\n",
            "Iteration 1587 Loss = 0.12672791153534532\n",
            "Iteration 1588 Loss = 0.1266773301873913\n",
            "Iteration 1589 Loss = 0.1266267772006175\n",
            "Iteration 1590 Loss = 0.12657625255912147\n",
            "Iteration 1591 Loss = 0.12652575624700996\n",
            "Iteration 1592 Loss = 0.1264752882483986\n",
            "Iteration 1593 Loss = 0.12642484854741182\n",
            "Iteration 1594 Loss = 0.12637443712818316\n",
            "Iteration 1595 Loss = 0.12632405397485486\n",
            "Iteration 1596 Loss = 0.1262736990715778\n",
            "Iteration 1597 Loss = 0.12622337240251266\n",
            "Iteration 1598 Loss = 0.126173073951828\n",
            "Iteration 1599 Loss = 0.12612280370370155\n",
            "Iteration 1600 Loss = 0.12607256164232028\n",
            "Iteration 1601 Loss = 0.12602234775187954\n",
            "Iteration 1602 Loss = 0.12597216201658412\n",
            "Iteration 1603 Loss = 0.12592200442064697\n",
            "Iteration 1604 Loss = 0.1258718749482901\n",
            "Iteration 1605 Loss = 0.12582177358374505\n",
            "Iteration 1606 Loss = 0.12577170031125123\n",
            "Iteration 1607 Loss = 0.12572165511505728\n",
            "Iteration 1608 Loss = 0.1256716379794208\n",
            "Iteration 1609 Loss = 0.125621648888608\n",
            "Iteration 1610 Loss = 0.1255716878268944\n",
            "Iteration 1611 Loss = 0.12552175477856353\n",
            "Iteration 1612 Loss = 0.12547184972790848\n",
            "Iteration 1613 Loss = 0.12542197265923063\n",
            "Iteration 1614 Loss = 0.1253721235568404\n",
            "Iteration 1615 Loss = 0.12532230240505712\n",
            "Iteration 1616 Loss = 0.12527250918820854\n",
            "Iteration 1617 Loss = 0.12522274389063162\n",
            "Iteration 1618 Loss = 0.12517300649667173\n",
            "Iteration 1619 Loss = 0.12512329699068317\n",
            "Iteration 1620 Loss = 0.1250736153570293\n",
            "Iteration 1621 Loss = 0.12502396158008156\n",
            "Iteration 1622 Loss = 0.12497433564422106\n",
            "Iteration 1623 Loss = 0.1249247375338369\n",
            "Iteration 1624 Loss = 0.12487516723332708\n",
            "Iteration 1625 Loss = 0.12482562472709884\n",
            "Iteration 1626 Loss = 0.12477610999956754\n",
            "Iteration 1627 Loss = 0.12472662303515764\n",
            "Iteration 1628 Loss = 0.12467716381830223\n",
            "Iteration 1629 Loss = 0.12462773233344315\n",
            "Iteration 1630 Loss = 0.12457832856503108\n",
            "Iteration 1631 Loss = 0.12452895249752502\n",
            "Iteration 1632 Loss = 0.12447960411539322\n",
            "Iteration 1633 Loss = 0.12443028340311198\n",
            "Iteration 1634 Loss = 0.12438099034516703\n",
            "Iteration 1635 Loss = 0.12433172492605228\n",
            "Iteration 1636 Loss = 0.12428248713027062\n",
            "Iteration 1637 Loss = 0.12423327694233395\n",
            "Iteration 1638 Loss = 0.12418409434676149\n",
            "Iteration 1639 Loss = 0.12413493932808273\n",
            "Iteration 1640 Loss = 0.12408581187083506\n",
            "Iteration 1641 Loss = 0.12403671195956475\n",
            "Iteration 1642 Loss = 0.12398763957882629\n",
            "Iteration 1643 Loss = 0.12393859471318368\n",
            "Iteration 1644 Loss = 0.12388957734720883\n",
            "Iteration 1645 Loss = 0.12384058746548263\n",
            "Iteration 1646 Loss = 0.1237916250525945\n",
            "Iteration 1647 Loss = 0.12374269009314245\n",
            "Iteration 1648 Loss = 0.12369378257173352\n",
            "Iteration 1649 Loss = 0.12364490247298267\n",
            "Iteration 1650 Loss = 0.123596049781514\n",
            "Iteration 1651 Loss = 0.1235472244819605\n",
            "Iteration 1652 Loss = 0.12349842655896293\n",
            "Iteration 1653 Loss = 0.12344965599717145\n",
            "Iteration 1654 Loss = 0.12340091278124464\n",
            "Iteration 1655 Loss = 0.12335219689584903\n",
            "Iteration 1656 Loss = 0.12330350832566067\n",
            "Iteration 1657 Loss = 0.12325484705536349\n",
            "Iteration 1658 Loss = 0.12320621306965067\n",
            "Iteration 1659 Loss = 0.12315760635322341\n",
            "Iteration 1660 Loss = 0.12310902689079194\n",
            "Iteration 1661 Loss = 0.12306047466707465\n",
            "Iteration 1662 Loss = 0.12301194966679854\n",
            "Iteration 1663 Loss = 0.12296345187469944\n",
            "Iteration 1664 Loss = 0.12291498127552188\n",
            "Iteration 1665 Loss = 0.12286653785401826\n",
            "Iteration 1666 Loss = 0.12281812159495008\n",
            "Iteration 1667 Loss = 0.12276973248308724\n",
            "Iteration 1668 Loss = 0.12272137050320835\n",
            "Iteration 1669 Loss = 0.12267303564010025\n",
            "Iteration 1670 Loss = 0.12262472787855837\n",
            "Iteration 1671 Loss = 0.12257644720338676\n",
            "Iteration 1672 Loss = 0.12252819359939807\n",
            "Iteration 1673 Loss = 0.12247996705141359\n",
            "Iteration 1674 Loss = 0.12243176754426231\n",
            "Iteration 1675 Loss = 0.12238359506278275\n",
            "Iteration 1676 Loss = 0.12233544959182162\n",
            "Iteration 1677 Loss = 0.12228733111623345\n",
            "Iteration 1678 Loss = 0.12223923962088228\n",
            "Iteration 1679 Loss = 0.12219117509064009\n",
            "Iteration 1680 Loss = 0.12214313751038751\n",
            "Iteration 1681 Loss = 0.1220951268650133\n",
            "Iteration 1682 Loss = 0.1220471431394152\n",
            "Iteration 1683 Loss = 0.12199918631849882\n",
            "Iteration 1684 Loss = 0.12195125638717931\n",
            "Iteration 1685 Loss = 0.12190335333037881\n",
            "Iteration 1686 Loss = 0.12185547713302898\n",
            "Iteration 1687 Loss = 0.12180762778006966\n",
            "Iteration 1688 Loss = 0.12175980525644882\n",
            "Iteration 1689 Loss = 0.12171200954712345\n",
            "Iteration 1690 Loss = 0.12166424063705858\n",
            "Iteration 1691 Loss = 0.12161649851122755\n",
            "Iteration 1692 Loss = 0.12156878315461263\n",
            "Iteration 1693 Loss = 0.12152109455220382\n",
            "Iteration 1694 Loss = 0.12147343268900018\n",
            "Iteration 1695 Loss = 0.12142579755000907\n",
            "Iteration 1696 Loss = 0.1213781891202455\n",
            "Iteration 1697 Loss = 0.12133060738473445\n",
            "Iteration 1698 Loss = 0.12128305232850774\n",
            "Iteration 1699 Loss = 0.12123552393660594\n",
            "Iteration 1700 Loss = 0.1211880221940787\n",
            "Iteration 1701 Loss = 0.1211405470859833\n",
            "Iteration 1702 Loss = 0.12109309859738605\n",
            "Iteration 1703 Loss = 0.12104567671336101\n",
            "Iteration 1704 Loss = 0.12099828141899088\n",
            "Iteration 1705 Loss = 0.1209509126993669\n",
            "Iteration 1706 Loss = 0.12090357053958839\n",
            "Iteration 1707 Loss = 0.12085625492476314\n",
            "Iteration 1708 Loss = 0.12080896584000721\n",
            "Iteration 1709 Loss = 0.12076170327044512\n",
            "Iteration 1710 Loss = 0.12071446720120978\n",
            "Iteration 1711 Loss = 0.12066725761744229\n",
            "Iteration 1712 Loss = 0.12062007450429203\n",
            "Iteration 1713 Loss = 0.12057291784691702\n",
            "Iteration 1714 Loss = 0.12052578763048312\n",
            "Iteration 1715 Loss = 0.12047868384016486\n",
            "Iteration 1716 Loss = 0.12043160646114522\n",
            "Iteration 1717 Loss = 0.12038455547861508\n",
            "Iteration 1718 Loss = 0.1203375308777739\n",
            "Iteration 1719 Loss = 0.12029053264382916\n",
            "Iteration 1720 Loss = 0.12024356076199712\n",
            "Iteration 1721 Loss = 0.12019661521750175\n",
            "Iteration 1722 Loss = 0.12014969599557598\n",
            "Iteration 1723 Loss = 0.12010280308146012\n",
            "Iteration 1724 Loss = 0.12005593646040356\n",
            "Iteration 1725 Loss = 0.12000909611766383\n",
            "Iteration 1726 Loss = 0.11996228203850617\n",
            "Iteration 1727 Loss = 0.11991549420820462\n",
            "Iteration 1728 Loss = 0.11986873261204165\n",
            "Iteration 1729 Loss = 0.11982199723530716\n",
            "Iteration 1730 Loss = 0.11977528806330003\n",
            "Iteration 1731 Loss = 0.11972860508132743\n",
            "Iteration 1732 Loss = 0.11968194827470402\n",
            "Iteration 1733 Loss = 0.11963531762875351\n",
            "Iteration 1734 Loss = 0.11958871312880746\n",
            "Iteration 1735 Loss = 0.11954213476020543\n",
            "Iteration 1736 Loss = 0.11949558250829574\n",
            "Iteration 1737 Loss = 0.11944905635843456\n",
            "Iteration 1738 Loss = 0.11940255629598621\n",
            "Iteration 1739 Loss = 0.11935608230632358\n",
            "Iteration 1740 Loss = 0.11930963437482764\n",
            "Iteration 1741 Loss = 0.11926321248688708\n",
            "Iteration 1742 Loss = 0.11921681662789933\n",
            "Iteration 1743 Loss = 0.11917044678327003\n",
            "Iteration 1744 Loss = 0.11912410293841275\n",
            "Iteration 1745 Loss = 0.11907778507874912\n",
            "Iteration 1746 Loss = 0.11903149318970918\n",
            "Iteration 1747 Loss = 0.11898522725673134\n",
            "Iteration 1748 Loss = 0.11893898726526152\n",
            "Iteration 1749 Loss = 0.11889277320075473\n",
            "Iteration 1750 Loss = 0.11884658504867297\n",
            "Iteration 1751 Loss = 0.11880042279448767\n",
            "Iteration 1752 Loss = 0.11875428642367723\n",
            "Iteration 1753 Loss = 0.11870817592172903\n",
            "Iteration 1754 Loss = 0.11866209127413824\n",
            "Iteration 1755 Loss = 0.1186160324664081\n",
            "Iteration 1756 Loss = 0.1185699994840503\n",
            "Iteration 1757 Loss = 0.11852399231258426\n",
            "Iteration 1758 Loss = 0.11847801093753801\n",
            "Iteration 1759 Loss = 0.11843205534444676\n",
            "Iteration 1760 Loss = 0.11838612551885491\n",
            "Iteration 1761 Loss = 0.11834022144631452\n",
            "Iteration 1762 Loss = 0.11829434311238571\n",
            "Iteration 1763 Loss = 0.11824849050263661\n",
            "Iteration 1764 Loss = 0.11820266360264353\n",
            "Iteration 1765 Loss = 0.11815686239799118\n",
            "Iteration 1766 Loss = 0.11811108687427181\n",
            "Iteration 1767 Loss = 0.11806533701708614\n",
            "Iteration 1768 Loss = 0.1180196128120427\n",
            "Iteration 1769 Loss = 0.1179739142447583\n",
            "Iteration 1770 Loss = 0.1179282413008577\n",
            "Iteration 1771 Loss = 0.11788259396597406\n",
            "Iteration 1772 Loss = 0.11783697222574797\n",
            "Iteration 1773 Loss = 0.11779137606582826\n",
            "Iteration 1774 Loss = 0.11774580547187227\n",
            "Iteration 1775 Loss = 0.11770026042954494\n",
            "Iteration 1776 Loss = 0.1176547409245194\n",
            "Iteration 1777 Loss = 0.11760924694247653\n",
            "Iteration 1778 Loss = 0.11756377846910579\n",
            "Iteration 1779 Loss = 0.1175183354901043\n",
            "Iteration 1780 Loss = 0.11747291799117718\n",
            "Iteration 1781 Loss = 0.11742752595803772\n",
            "Iteration 1782 Loss = 0.11738215937640696\n",
            "Iteration 1783 Loss = 0.11733681823201428\n",
            "Iteration 1784 Loss = 0.11729150251059701\n",
            "Iteration 1785 Loss = 0.11724621219790025\n",
            "Iteration 1786 Loss = 0.11720094727967693\n",
            "Iteration 1787 Loss = 0.11715570774168865\n",
            "Iteration 1788 Loss = 0.11711049356970478\n",
            "Iteration 1789 Loss = 0.11706530474950198\n",
            "Iteration 1790 Loss = 0.11702014126686563\n",
            "Iteration 1791 Loss = 0.11697500310758886\n",
            "Iteration 1792 Loss = 0.11692989025747312\n",
            "Iteration 1793 Loss = 0.1168848027023266\n",
            "Iteration 1794 Loss = 0.11683974042796703\n",
            "Iteration 1795 Loss = 0.11679470342021891\n",
            "Iteration 1796 Loss = 0.11674969166491563\n",
            "Iteration 1797 Loss = 0.11670470514789756\n",
            "Iteration 1798 Loss = 0.11665974385501385\n",
            "Iteration 1799 Loss = 0.11661480777212087\n",
            "Iteration 1800 Loss = 0.1165698968850835\n",
            "Iteration 1801 Loss = 0.11652501117977418\n",
            "Iteration 1802 Loss = 0.11648015064207368\n",
            "Iteration 1803 Loss = 0.11643531525786983\n",
            "Iteration 1804 Loss = 0.11639050501305996\n",
            "Iteration 1805 Loss = 0.11634571989354722\n",
            "Iteration 1806 Loss = 0.11630095988524448\n",
            "Iteration 1807 Loss = 0.11625622497407145\n",
            "Iteration 1808 Loss = 0.116211515145956\n",
            "Iteration 1809 Loss = 0.11616683038683422\n",
            "Iteration 1810 Loss = 0.11612217068264968\n",
            "Iteration 1811 Loss = 0.11607753601935397\n",
            "Iteration 1812 Loss = 0.11603292638290646\n",
            "Iteration 1813 Loss = 0.1159883417592746\n",
            "Iteration 1814 Loss = 0.11594378213443352\n",
            "Iteration 1815 Loss = 0.11589924749436631\n",
            "Iteration 1816 Loss = 0.11585473782506407\n",
            "Iteration 1817 Loss = 0.11581025311252545\n",
            "Iteration 1818 Loss = 0.11576579334275694\n",
            "Iteration 1819 Loss = 0.1157213585017733\n",
            "Iteration 1820 Loss = 0.11567694857559666\n",
            "Iteration 1821 Loss = 0.11563256355025725\n",
            "Iteration 1822 Loss = 0.11558820341179303\n",
            "Iteration 1823 Loss = 0.11554386814624977\n",
            "Iteration 1824 Loss = 0.11549955773968125\n",
            "Iteration 1825 Loss = 0.11545527217814902\n",
            "Iteration 1826 Loss = 0.11541101144772202\n",
            "Iteration 1827 Loss = 0.11536677553447762\n",
            "Iteration 1828 Loss = 0.11532256442450062\n",
            "Iteration 1829 Loss = 0.1152783781038837\n",
            "Iteration 1830 Loss = 0.11523421655872731\n",
            "Iteration 1831 Loss = 0.11519007977513997\n",
            "Iteration 1832 Loss = 0.11514596773923758\n",
            "Iteration 1833 Loss = 0.115101880437144\n",
            "Iteration 1834 Loss = 0.11505781785499107\n",
            "Iteration 1835 Loss = 0.11501377997891786\n",
            "Iteration 1836 Loss = 0.11496976679507177\n",
            "Iteration 1837 Loss = 0.11492577828960793\n",
            "Iteration 1838 Loss = 0.11488181444868877\n",
            "Iteration 1839 Loss = 0.11483787525848485\n",
            "Iteration 1840 Loss = 0.11479396070517464\n",
            "Iteration 1841 Loss = 0.11475007077494387\n",
            "Iteration 1842 Loss = 0.1147062054539861\n",
            "Iteration 1843 Loss = 0.11466236472850332\n",
            "Iteration 1844 Loss = 0.11461854858470429\n",
            "Iteration 1845 Loss = 0.11457475700880626\n",
            "Iteration 1846 Loss = 0.11453098998703375\n",
            "Iteration 1847 Loss = 0.11448724750561927\n",
            "Iteration 1848 Loss = 0.11444352955080271\n",
            "Iteration 1849 Loss = 0.11439983610883218\n",
            "Iteration 1850 Loss = 0.11435616716596307\n",
            "Iteration 1851 Loss = 0.1143125227084585\n",
            "Iteration 1852 Loss = 0.11426890272258962\n",
            "Iteration 1853 Loss = 0.11422530719463514\n",
            "Iteration 1854 Loss = 0.11418173611088105\n",
            "Iteration 1855 Loss = 0.1141381894576219\n",
            "Iteration 1856 Loss = 0.11409466722115881\n",
            "Iteration 1857 Loss = 0.11405116938780159\n",
            "Iteration 1858 Loss = 0.11400769594386734\n",
            "Iteration 1859 Loss = 0.11396424687568052\n",
            "Iteration 1860 Loss = 0.11392082216957371\n",
            "Iteration 1861 Loss = 0.11387742181188705\n",
            "Iteration 1862 Loss = 0.11383404578896819\n",
            "Iteration 1863 Loss = 0.11379069408717239\n",
            "Iteration 1864 Loss = 0.1137473666928629\n",
            "Iteration 1865 Loss = 0.11370406359241036\n",
            "Iteration 1866 Loss = 0.11366078477219299\n",
            "Iteration 1867 Loss = 0.11361753021859708\n",
            "Iteration 1868 Loss = 0.11357429991801575\n",
            "Iteration 1869 Loss = 0.11353109385685069\n",
            "Iteration 1870 Loss = 0.11348791202151047\n",
            "Iteration 1871 Loss = 0.1134447543984116\n",
            "Iteration 1872 Loss = 0.11340162097397817\n",
            "Iteration 1873 Loss = 0.11335851173464193\n",
            "Iteration 1874 Loss = 0.11331542666684222\n",
            "Iteration 1875 Loss = 0.11327236575702589\n",
            "Iteration 1876 Loss = 0.11322932899164738\n",
            "Iteration 1877 Loss = 0.11318631635716898\n",
            "Iteration 1878 Loss = 0.11314332784006037\n",
            "Iteration 1879 Loss = 0.11310036342679881\n",
            "Iteration 1880 Loss = 0.11305742310386911\n",
            "Iteration 1881 Loss = 0.1130145068577636\n",
            "Iteration 1882 Loss = 0.11297161467498269\n",
            "Iteration 1883 Loss = 0.1129287465420336\n",
            "Iteration 1884 Loss = 0.11288590244543159\n",
            "Iteration 1885 Loss = 0.1128430823716993\n",
            "Iteration 1886 Loss = 0.11280028630736726\n",
            "Iteration 1887 Loss = 0.11275751423897315\n",
            "Iteration 1888 Loss = 0.1127147661530622\n",
            "Iteration 1889 Loss = 0.1126720420361875\n",
            "Iteration 1890 Loss = 0.11262934187490956\n",
            "Iteration 1891 Loss = 0.1125866656557962\n",
            "Iteration 1892 Loss = 0.11254401336542302\n",
            "Iteration 1893 Loss = 0.11250138499037317\n",
            "Iteration 1894 Loss = 0.11245878051723686\n",
            "Iteration 1895 Loss = 0.11241619993261273\n",
            "Iteration 1896 Loss = 0.11237364322310599\n",
            "Iteration 1897 Loss = 0.11233111037533001\n",
            "Iteration 1898 Loss = 0.11228860137590514\n",
            "Iteration 1899 Loss = 0.11224611621145975\n",
            "Iteration 1900 Loss = 0.11220365486862934\n",
            "Iteration 1901 Loss = 0.11216121733405708\n",
            "Iteration 1902 Loss = 0.11211880359439354\n",
            "Iteration 1903 Loss = 0.11207641363629668\n",
            "Iteration 1904 Loss = 0.11203404744643229\n",
            "Iteration 1905 Loss = 0.11199170501147356\n",
            "Iteration 1906 Loss = 0.11194938631810052\n",
            "Iteration 1907 Loss = 0.11190709135300159\n",
            "Iteration 1908 Loss = 0.111864820102872\n",
            "Iteration 1909 Loss = 0.11182257255441498\n",
            "Iteration 1910 Loss = 0.11178034869434038\n",
            "Iteration 1911 Loss = 0.11173814850936631\n",
            "Iteration 1912 Loss = 0.1116959719862181\n",
            "Iteration 1913 Loss = 0.11165381911162843\n",
            "Iteration 1914 Loss = 0.11161168987233729\n",
            "Iteration 1915 Loss = 0.11156958425509246\n",
            "Iteration 1916 Loss = 0.11152750224664897\n",
            "Iteration 1917 Loss = 0.11148544383376902\n",
            "Iteration 1918 Loss = 0.11144340900322298\n",
            "Iteration 1919 Loss = 0.1114013977417874\n",
            "Iteration 1920 Loss = 0.11135941003624746\n",
            "Iteration 1921 Loss = 0.11131744587339525\n",
            "Iteration 1922 Loss = 0.1112755052400298\n",
            "Iteration 1923 Loss = 0.11123358812295893\n",
            "Iteration 1924 Loss = 0.11119169450899638\n",
            "Iteration 1925 Loss = 0.11114982438496392\n",
            "Iteration 1926 Loss = 0.1111079777376907\n",
            "Iteration 1927 Loss = 0.11106615455401297\n",
            "Iteration 1928 Loss = 0.111024354820775\n",
            "Iteration 1929 Loss = 0.1109825785248276\n",
            "Iteration 1930 Loss = 0.11094082565302978\n",
            "Iteration 1931 Loss = 0.11089909619224708\n",
            "Iteration 1932 Loss = 0.11085739012935321\n",
            "Iteration 1933 Loss = 0.11081570745122882\n",
            "Iteration 1934 Loss = 0.11077404814476173\n",
            "Iteration 1935 Loss = 0.11073241219684761\n",
            "Iteration 1936 Loss = 0.11069079959438916\n",
            "Iteration 1937 Loss = 0.1106492103242964\n",
            "Iteration 1938 Loss = 0.11060764437348677\n",
            "Iteration 1939 Loss = 0.11056610172888512\n",
            "Iteration 1940 Loss = 0.11052458237742356\n",
            "Iteration 1941 Loss = 0.11048308630604158\n",
            "Iteration 1942 Loss = 0.11044161350168584\n",
            "Iteration 1943 Loss = 0.11040016395131032\n",
            "Iteration 1944 Loss = 0.11035873764187652\n",
            "Iteration 1945 Loss = 0.11031733456035309\n",
            "Iteration 1946 Loss = 0.11027595469371639\n",
            "Iteration 1947 Loss = 0.11023459802894935\n",
            "Iteration 1948 Loss = 0.11019326455304258\n",
            "Iteration 1949 Loss = 0.1101519542529941\n",
            "Iteration 1950 Loss = 0.11011066711580912\n",
            "Iteration 1951 Loss = 0.11006940312850008\n",
            "Iteration 1952 Loss = 0.11002816227808676\n",
            "Iteration 1953 Loss = 0.10998694455159619\n",
            "Iteration 1954 Loss = 0.10994574993606276\n",
            "Iteration 1955 Loss = 0.1099045784185279\n",
            "Iteration 1956 Loss = 0.10986342998604078\n",
            "Iteration 1957 Loss = 0.10982230462565698\n",
            "Iteration 1958 Loss = 0.10978120232444033\n",
            "Iteration 1959 Loss = 0.1097401230694613\n",
            "Iteration 1960 Loss = 0.10969906684779776\n",
            "Iteration 1961 Loss = 0.10965803364653495\n",
            "Iteration 1962 Loss = 0.10961702345276499\n",
            "Iteration 1963 Loss = 0.10957603625358758\n",
            "Iteration 1964 Loss = 0.10953507203610965\n",
            "Iteration 1965 Loss = 0.10949413078744523\n",
            "Iteration 1966 Loss = 0.10945321249471564\n",
            "Iteration 1967 Loss = 0.1094123171450493\n",
            "Iteration 1968 Loss = 0.10937144472558187\n",
            "Iteration 1969 Loss = 0.10933059522345645\n",
            "Iteration 1970 Loss = 0.10928976862582322\n",
            "Iteration 1971 Loss = 0.10924896491983921\n",
            "Iteration 1972 Loss = 0.1092081840926694\n",
            "Iteration 1973 Loss = 0.10916742613148518\n",
            "Iteration 1974 Loss = 0.10912669102346563\n",
            "Iteration 1975 Loss = 0.10908597875579708\n",
            "Iteration 1976 Loss = 0.10904528931567244\n",
            "Iteration 1977 Loss = 0.1090046226902927\n",
            "Iteration 1978 Loss = 0.10896397886686511\n",
            "Iteration 1979 Loss = 0.10892335783260465\n",
            "Iteration 1980 Loss = 0.10888275957473331\n",
            "Iteration 1981 Loss = 0.10884218408048052\n",
            "Iteration 1982 Loss = 0.1088016313370823\n",
            "Iteration 1983 Loss = 0.10876110133178231\n",
            "Iteration 1984 Loss = 0.108720594051831\n",
            "Iteration 1985 Loss = 0.10868010948448666\n",
            "Iteration 1986 Loss = 0.10863964761701365\n",
            "Iteration 1987 Loss = 0.10859920843668433\n",
            "Iteration 1988 Loss = 0.10855879193077797\n",
            "Iteration 1989 Loss = 0.10851839808658079\n",
            "Iteration 1990 Loss = 0.10847802689138632\n",
            "Iteration 1991 Loss = 0.108437678332495\n",
            "Iteration 1992 Loss = 0.10839735239721512\n",
            "Iteration 1993 Loss = 0.108357049072861\n",
            "Iteration 1994 Loss = 0.10831676834675469\n",
            "Iteration 1995 Loss = 0.10827651020622521\n",
            "Iteration 1996 Loss = 0.10823627463860898\n",
            "Iteration 1997 Loss = 0.10819606163124897\n",
            "Iteration 1998 Loss = 0.1081558711714957\n",
            "Iteration 1999 Loss = 0.10811570324670676\n",
            "Iteration 2000 Loss = 0.10807555784424655\n",
            "Iteration 2001 Loss = 0.10803543495148661\n",
            "Iteration 2002 Loss = 0.10799533455580568\n",
            "Iteration 2003 Loss = 0.10795525664458988\n",
            "Iteration 2004 Loss = 0.10791520120523163\n",
            "Iteration 2005 Loss = 0.10787516822513109\n",
            "Iteration 2006 Loss = 0.10783515769169519\n",
            "Iteration 2007 Loss = 0.10779516959233804\n",
            "Iteration 2008 Loss = 0.10775520391448067\n",
            "Iteration 2009 Loss = 0.1077152606455513\n",
            "Iteration 2010 Loss = 0.10767533977298525\n",
            "Iteration 2011 Loss = 0.10763544128422464\n",
            "Iteration 2012 Loss = 0.10759556516671874\n",
            "Iteration 2013 Loss = 0.10755571140792415\n",
            "Iteration 2014 Loss = 0.10751587999530396\n",
            "Iteration 2015 Loss = 0.10747607091632864\n",
            "Iteration 2016 Loss = 0.10743628415847553\n",
            "Iteration 2017 Loss = 0.10739651970922953\n",
            "Iteration 2018 Loss = 0.10735677755608171\n",
            "Iteration 2019 Loss = 0.1073170576865305\n",
            "Iteration 2020 Loss = 0.10727736008808167\n",
            "Iteration 2021 Loss = 0.10723768474824752\n",
            "Iteration 2022 Loss = 0.10719803165454754\n",
            "Iteration 2023 Loss = 0.10715840079450828\n",
            "Iteration 2024 Loss = 0.10711879215566326\n",
            "Iteration 2025 Loss = 0.10707920572555295\n",
            "Iteration 2026 Loss = 0.10703964149172492\n",
            "Iteration 2027 Loss = 0.10700009944173322\n",
            "Iteration 2028 Loss = 0.10696057956313981\n",
            "Iteration 2029 Loss = 0.10692108184351266\n",
            "Iteration 2030 Loss = 0.10688160627042728\n",
            "Iteration 2031 Loss = 0.10684215283146614\n",
            "Iteration 2032 Loss = 0.10680272151421842\n",
            "Iteration 2033 Loss = 0.10676331230628053\n",
            "Iteration 2034 Loss = 0.10672392519525539\n",
            "Iteration 2035 Loss = 0.1066845601687535\n",
            "Iteration 2036 Loss = 0.10664521721439178\n",
            "Iteration 2037 Loss = 0.10660589631979424\n",
            "Iteration 2038 Loss = 0.10656659747259198\n",
            "Iteration 2039 Loss = 0.106527320660423\n",
            "Iteration 2040 Loss = 0.106488065870932\n",
            "Iteration 2041 Loss = 0.10644883309177076\n",
            "Iteration 2042 Loss = 0.10640962231059811\n",
            "Iteration 2043 Loss = 0.10637043351507956\n",
            "Iteration 2044 Loss = 0.10633126669288792\n",
            "Iteration 2045 Loss = 0.10629212183170231\n",
            "Iteration 2046 Loss = 0.10625299891920925\n",
            "Iteration 2047 Loss = 0.10621389794310214\n",
            "Iteration 2048 Loss = 0.1061748188910809\n",
            "Iteration 2049 Loss = 0.1061357617508529\n",
            "Iteration 2050 Loss = 0.10609672651013188\n",
            "Iteration 2051 Loss = 0.10605771315663867\n",
            "Iteration 2052 Loss = 0.10601872167810125\n",
            "Iteration 2053 Loss = 0.10597975206225387\n",
            "Iteration 2054 Loss = 0.10594080429683846\n",
            "Iteration 2055 Loss = 0.105901878369603\n",
            "Iteration 2056 Loss = 0.10586297426830289\n",
            "Iteration 2057 Loss = 0.10582409198070021\n",
            "Iteration 2058 Loss = 0.10578523149456413\n",
            "Iteration 2059 Loss = 0.10574639279767006\n",
            "Iteration 2060 Loss = 0.10570757587780101\n",
            "Iteration 2061 Loss = 0.10566878072274642\n",
            "Iteration 2062 Loss = 0.10563000732030266\n",
            "Iteration 2063 Loss = 0.10559125565827288\n",
            "Iteration 2064 Loss = 0.10555252572446722\n",
            "Iteration 2065 Loss = 0.10551381750670259\n",
            "Iteration 2066 Loss = 0.10547513099280265\n",
            "Iteration 2067 Loss = 0.10543646617059795\n",
            "Iteration 2068 Loss = 0.10539782302792594\n",
            "Iteration 2069 Loss = 0.10535920155263066\n",
            "Iteration 2070 Loss = 0.10532060173256336\n",
            "Iteration 2071 Loss = 0.10528202355558153\n",
            "Iteration 2072 Loss = 0.10524346700955\n",
            "Iteration 2073 Loss = 0.10520493208234026\n",
            "Iteration 2074 Loss = 0.10516641876183046\n",
            "Iteration 2075 Loss = 0.10512792703590566\n",
            "Iteration 2076 Loss = 0.10508945689245751\n",
            "Iteration 2077 Loss = 0.10505100831938485\n",
            "Iteration 2078 Loss = 0.10501258130459312\n",
            "Iteration 2079 Loss = 0.10497417583599429\n",
            "Iteration 2080 Loss = 0.10493579190150745\n",
            "Iteration 2081 Loss = 0.10489742948905814\n",
            "Iteration 2082 Loss = 0.10485908858657919\n",
            "Iteration 2083 Loss = 0.10482076918200962\n",
            "Iteration 2084 Loss = 0.10478247126329539\n",
            "Iteration 2085 Loss = 0.10474419481838955\n",
            "Iteration 2086 Loss = 0.10470593983525153\n",
            "Iteration 2087 Loss = 0.10466770630184757\n",
            "Iteration 2088 Loss = 0.10462949420615078\n",
            "Iteration 2089 Loss = 0.10459130353614085\n",
            "Iteration 2090 Loss = 0.10455313427980437\n",
            "Iteration 2091 Loss = 0.10451498642513467\n",
            "Iteration 2092 Loss = 0.10447685996013166\n",
            "Iteration 2093 Loss = 0.10443875487280216\n",
            "Iteration 2094 Loss = 0.10440067115115925\n",
            "Iteration 2095 Loss = 0.10436260878322369\n",
            "Iteration 2096 Loss = 0.10432456775702188\n",
            "Iteration 2097 Loss = 0.10428654806058761\n",
            "Iteration 2098 Loss = 0.10424854968196114\n",
            "Iteration 2099 Loss = 0.10421057260918962\n",
            "Iteration 2100 Loss = 0.10417261683032646\n",
            "Iteration 2101 Loss = 0.10413468233343257\n",
            "Iteration 2102 Loss = 0.10409676910657448\n",
            "Iteration 2103 Loss = 0.10405887713782638\n",
            "Iteration 2104 Loss = 0.10402100641526871\n",
            "Iteration 2105 Loss = 0.10398315692698835\n",
            "Iteration 2106 Loss = 0.10394532866107961\n",
            "Iteration 2107 Loss = 0.10390752160564265\n",
            "Iteration 2108 Loss = 0.10386973574878484\n",
            "Iteration 2109 Loss = 0.10383197107862012\n",
            "Iteration 2110 Loss = 0.1037942275832687\n",
            "Iteration 2111 Loss = 0.10375650525085797\n",
            "Iteration 2112 Loss = 0.10371880406952193\n",
            "Iteration 2113 Loss = 0.10368112402740076\n",
            "Iteration 2114 Loss = 0.1036434651126419\n",
            "Iteration 2115 Loss = 0.10360582731339889\n",
            "Iteration 2116 Loss = 0.10356821061783245\n",
            "Iteration 2117 Loss = 0.10353061501410941\n",
            "Iteration 2118 Loss = 0.10349304049040366\n",
            "Iteration 2119 Loss = 0.10345548703489556\n",
            "Iteration 2120 Loss = 0.1034179546357718\n",
            "Iteration 2121 Loss = 0.10338044328122625\n",
            "Iteration 2122 Loss = 0.10334295295945907\n",
            "Iteration 2123 Loss = 0.10330548365867712\n",
            "Iteration 2124 Loss = 0.10326803536709361\n",
            "Iteration 2125 Loss = 0.10323060807292883\n",
            "Iteration 2126 Loss = 0.10319320176440933\n",
            "Iteration 2127 Loss = 0.10315581642976855\n",
            "Iteration 2128 Loss = 0.10311845205724608\n",
            "Iteration 2129 Loss = 0.10308110863508851\n",
            "Iteration 2130 Loss = 0.10304378615154895\n",
            "Iteration 2131 Loss = 0.10300648459488677\n",
            "Iteration 2132 Loss = 0.10296920395336838\n",
            "Iteration 2133 Loss = 0.1029319442152666\n",
            "Iteration 2134 Loss = 0.10289470536886072\n",
            "Iteration 2135 Loss = 0.10285748740243668\n",
            "Iteration 2136 Loss = 0.10282029030428705\n",
            "Iteration 2137 Loss = 0.10278311406271093\n",
            "Iteration 2138 Loss = 0.1027459586660137\n",
            "Iteration 2139 Loss = 0.10270882410250778\n",
            "Iteration 2140 Loss = 0.10267171036051177\n",
            "Iteration 2141 Loss = 0.10263461742835091\n",
            "Iteration 2142 Loss = 0.10259754529435743\n",
            "Iteration 2143 Loss = 0.10256049394686935\n",
            "Iteration 2144 Loss = 0.10252346337423157\n",
            "Iteration 2145 Loss = 0.10248645356479574\n",
            "Iteration 2146 Loss = 0.1024494645069196\n",
            "Iteration 2147 Loss = 0.10241249618896794\n",
            "Iteration 2148 Loss = 0.10237554859931151\n",
            "Iteration 2149 Loss = 0.10233862172632824\n",
            "Iteration 2150 Loss = 0.10230171555840181\n",
            "Iteration 2151 Loss = 0.102264830083923\n",
            "Iteration 2152 Loss = 0.10222796529128882\n",
            "Iteration 2153 Loss = 0.10219112116890282\n",
            "Iteration 2154 Loss = 0.10215429770517534\n",
            "Iteration 2155 Loss = 0.10211749488852283\n",
            "Iteration 2156 Loss = 0.10208071270736856\n",
            "Iteration 2157 Loss = 0.10204395115014171\n",
            "Iteration 2158 Loss = 0.10200721020527878\n",
            "Iteration 2159 Loss = 0.10197048986122202\n",
            "Iteration 2160 Loss = 0.10193379010642083\n",
            "Iteration 2161 Loss = 0.10189711092933025\n",
            "Iteration 2162 Loss = 0.10186045231841265\n",
            "Iteration 2163 Loss = 0.10182381426213635\n",
            "Iteration 2164 Loss = 0.10178719674897624\n",
            "Iteration 2165 Loss = 0.10175059976741381\n",
            "Iteration 2166 Loss = 0.10171402330593676\n",
            "Iteration 2167 Loss = 0.1016774673530396\n",
            "Iteration 2168 Loss = 0.10164093189722277\n",
            "Iteration 2169 Loss = 0.10160441692699378\n",
            "Iteration 2170 Loss = 0.1015679224308661\n",
            "Iteration 2171 Loss = 0.10153144839735982\n",
            "Iteration 2172 Loss = 0.10149499481500156\n",
            "Iteration 2173 Loss = 0.10145856167232414\n",
            "Iteration 2174 Loss = 0.10142214895786715\n",
            "Iteration 2175 Loss = 0.10138575666017606\n",
            "Iteration 2176 Loss = 0.10134938476780347\n",
            "Iteration 2177 Loss = 0.10131303326930773\n",
            "Iteration 2178 Loss = 0.10127670215325409\n",
            "Iteration 2179 Loss = 0.10124039140821404\n",
            "Iteration 2180 Loss = 0.10120410102276536\n",
            "Iteration 2181 Loss = 0.10116783098549247\n",
            "Iteration 2182 Loss = 0.10113158128498576\n",
            "Iteration 2183 Loss = 0.10109535190984259\n",
            "Iteration 2184 Loss = 0.10105914284866643\n",
            "Iteration 2185 Loss = 0.10102295409006708\n",
            "Iteration 2186 Loss = 0.1009867856226607\n",
            "Iteration 2187 Loss = 0.1009506374350702\n",
            "Iteration 2188 Loss = 0.1009145095159244\n",
            "Iteration 2189 Loss = 0.10087840185385873\n",
            "Iteration 2190 Loss = 0.10084231443751503\n",
            "Iteration 2191 Loss = 0.10080624725554127\n",
            "Iteration 2192 Loss = 0.1007702002965921\n",
            "Iteration 2193 Loss = 0.10073417354932825\n",
            "Iteration 2194 Loss = 0.10069816700241718\n",
            "Iteration 2195 Loss = 0.10066218064453232\n",
            "Iteration 2196 Loss = 0.10062621446435355\n",
            "Iteration 2197 Loss = 0.10059026845056715\n",
            "Iteration 2198 Loss = 0.10055434259186569\n",
            "Iteration 2199 Loss = 0.10051843687694831\n",
            "Iteration 2200 Loss = 0.10048255129452022\n",
            "Iteration 2201 Loss = 0.10044668583329303\n",
            "Iteration 2202 Loss = 0.10041084048198462\n",
            "Iteration 2203 Loss = 0.10037501522931945\n",
            "Iteration 2204 Loss = 0.10033921006402788\n",
            "Iteration 2205 Loss = 0.10030342497484698\n",
            "Iteration 2206 Loss = 0.10026765995052012\n",
            "Iteration 2207 Loss = 0.10023191497979662\n",
            "Iteration 2208 Loss = 0.10019619005143242\n",
            "Iteration 2209 Loss = 0.10016048515418981\n",
            "Iteration 2210 Loss = 0.10012480027683694\n",
            "Iteration 2211 Loss = 0.10008913540814895\n",
            "Iteration 2212 Loss = 0.10005349053690653\n",
            "Iteration 2213 Loss = 0.10001786565189727\n",
            "Iteration 2214 Loss = 0.09998226074191481\n",
            "Iteration 2215 Loss = 0.09994667579575894\n",
            "Iteration 2216 Loss = 0.09991111080223597\n",
            "Iteration 2217 Loss = 0.09987556575015825\n",
            "Iteration 2218 Loss = 0.0998400406283446\n",
            "Iteration 2219 Loss = 0.09980453542562008\n",
            "Iteration 2220 Loss = 0.09976905013081591\n",
            "Iteration 2221 Loss = 0.09973358473276965\n",
            "Iteration 2222 Loss = 0.099698139220325\n",
            "Iteration 2223 Loss = 0.09966271358233214\n",
            "Iteration 2224 Loss = 0.09962730780764738\n",
            "Iteration 2225 Loss = 0.0995919218851332\n",
            "Iteration 2226 Loss = 0.09955655580365845\n",
            "Iteration 2227 Loss = 0.09952120955209826\n",
            "Iteration 2228 Loss = 0.09948588311933367\n",
            "Iteration 2229 Loss = 0.09945057649425242\n",
            "Iteration 2230 Loss = 0.09941528966574814\n",
            "Iteration 2231 Loss = 0.09938002262272103\n",
            "Iteration 2232 Loss = 0.09934477535407696\n",
            "Iteration 2233 Loss = 0.09930954784872842\n",
            "Iteration 2234 Loss = 0.0992743400955943\n",
            "Iteration 2235 Loss = 0.09923915208359907\n",
            "Iteration 2236 Loss = 0.09920398380167414\n",
            "Iteration 2237 Loss = 0.09916883523875655\n",
            "Iteration 2238 Loss = 0.09913370638378981\n",
            "Iteration 2239 Loss = 0.0990985972257237\n",
            "Iteration 2240 Loss = 0.099063507753514\n",
            "Iteration 2241 Loss = 0.09902843795612268\n",
            "Iteration 2242 Loss = 0.0989933878225182\n",
            "Iteration 2243 Loss = 0.09895835734167481\n",
            "Iteration 2244 Loss = 0.09892334650257305\n",
            "Iteration 2245 Loss = 0.09888835529419998\n",
            "Iteration 2246 Loss = 0.09885338370554826\n",
            "Iteration 2247 Loss = 0.09881843172561726\n",
            "Iteration 2248 Loss = 0.0987834993434122\n",
            "Iteration 2249 Loss = 0.09874858654794452\n",
            "Iteration 2250 Loss = 0.09871369332823193\n",
            "Iteration 2251 Loss = 0.09867881967329828\n",
            "Iteration 2252 Loss = 0.09864396557217331\n",
            "Iteration 2253 Loss = 0.09860913101389324\n",
            "Iteration 2254 Loss = 0.09857431598750024\n",
            "Iteration 2255 Loss = 0.09853952048204283\n",
            "Iteration 2256 Loss = 0.0985047444865755\n",
            "Iteration 2257 Loss = 0.0984699879901586\n",
            "Iteration 2258 Loss = 0.09843525098185946\n",
            "Iteration 2259 Loss = 0.09840053345075081\n",
            "Iteration 2260 Loss = 0.09836583538591138\n",
            "Iteration 2261 Loss = 0.09833115677642676\n",
            "Iteration 2262 Loss = 0.0982964976113881\n",
            "Iteration 2263 Loss = 0.09826185787989278\n",
            "Iteration 2264 Loss = 0.09822723757104447\n",
            "Iteration 2265 Loss = 0.09819263667395234\n",
            "Iteration 2266 Loss = 0.09815805517773298\n",
            "Iteration 2267 Loss = 0.09812349307150761\n",
            "Iteration 2268 Loss = 0.09808895034440421\n",
            "Iteration 2269 Loss = 0.09805442698555701\n",
            "Iteration 2270 Loss = 0.09801992298410603\n",
            "Iteration 2271 Loss = 0.09798543832919746\n",
            "Iteration 2272 Loss = 0.09795097300998382\n",
            "Iteration 2273 Loss = 0.09791652701562328\n",
            "Iteration 2274 Loss = 0.0978821003352804\n",
            "Iteration 2275 Loss = 0.09784769295812572\n",
            "Iteration 2276 Loss = 0.09781330487333603\n",
            "Iteration 2277 Loss = 0.09777893607009366\n",
            "Iteration 2278 Loss = 0.0977445865375877\n",
            "Iteration 2279 Loss = 0.09771025626501258\n",
            "Iteration 2280 Loss = 0.09767594524156967\n",
            "Iteration 2281 Loss = 0.09764165345646547\n",
            "Iteration 2282 Loss = 0.09760738089891324\n",
            "Iteration 2283 Loss = 0.09757312755813169\n",
            "Iteration 2284 Loss = 0.09753889342334637\n",
            "Iteration 2285 Loss = 0.09750467848378788\n",
            "Iteration 2286 Loss = 0.09747048272869387\n",
            "Iteration 2287 Loss = 0.09743630614730721\n",
            "Iteration 2288 Loss = 0.09740214872887706\n",
            "Iteration 2289 Loss = 0.09736801046265885\n",
            "Iteration 2290 Loss = 0.09733389133791391\n",
            "Iteration 2291 Loss = 0.09729979134390934\n",
            "Iteration 2292 Loss = 0.09726571046991855\n",
            "Iteration 2293 Loss = 0.09723164870522095\n",
            "Iteration 2294 Loss = 0.0971976060391016\n",
            "Iteration 2295 Loss = 0.09716358246085206\n",
            "Iteration 2296 Loss = 0.0971295779597698\n",
            "Iteration 2297 Loss = 0.09709559252515779\n",
            "Iteration 2298 Loss = 0.0970616261463257\n",
            "Iteration 2299 Loss = 0.09702767881258871\n",
            "Iteration 2300 Loss = 0.0969937505132682\n",
            "Iteration 2301 Loss = 0.0969598412376916\n",
            "Iteration 2302 Loss = 0.09692595097519209\n",
            "Iteration 2303 Loss = 0.09689207971510909\n",
            "Iteration 2304 Loss = 0.09685822744678763\n",
            "Iteration 2305 Loss = 0.09682439415957908\n",
            "Iteration 2306 Loss = 0.09679057984284066\n",
            "Iteration 2307 Loss = 0.09675678448593547\n",
            "Iteration 2308 Loss = 0.09672300807823281\n",
            "Iteration 2309 Loss = 0.09668925060910767\n",
            "Iteration 2310 Loss = 0.0966555120679411\n",
            "Iteration 2311 Loss = 0.09662179244412009\n",
            "Iteration 2312 Loss = 0.09658809172703757\n",
            "Iteration 2313 Loss = 0.09655440990609253\n",
            "Iteration 2314 Loss = 0.09652074697068992\n",
            "Iteration 2315 Loss = 0.09648710291024039\n",
            "Iteration 2316 Loss = 0.09645347771416075\n",
            "Iteration 2317 Loss = 0.09641987137187356\n",
            "Iteration 2318 Loss = 0.09638628387280748\n",
            "Iteration 2319 Loss = 0.09635271520639707\n",
            "Iteration 2320 Loss = 0.09631916536208276\n",
            "Iteration 2321 Loss = 0.09628563432931096\n",
            "Iteration 2322 Loss = 0.09625212209753393\n",
            "Iteration 2323 Loss = 0.09621862865620985\n",
            "Iteration 2324 Loss = 0.09618515399480271\n",
            "Iteration 2325 Loss = 0.09615169810278273\n",
            "Iteration 2326 Loss = 0.09611826096962571\n",
            "Iteration 2327 Loss = 0.09608484258481353\n",
            "Iteration 2328 Loss = 0.09605144293783385\n",
            "Iteration 2329 Loss = 0.0960180620181804\n",
            "Iteration 2330 Loss = 0.09598469981535249\n",
            "Iteration 2331 Loss = 0.09595135631885587\n",
            "Iteration 2332 Loss = 0.0959180315182014\n",
            "Iteration 2333 Loss = 0.09588472540290648\n",
            "Iteration 2334 Loss = 0.09585143796249415\n",
            "Iteration 2335 Loss = 0.09581816918649327\n",
            "Iteration 2336 Loss = 0.09578491906443866\n",
            "Iteration 2337 Loss = 0.09575168758587096\n",
            "Iteration 2338 Loss = 0.09571847474033658\n",
            "Iteration 2339 Loss = 0.09568528051738805\n",
            "Iteration 2340 Loss = 0.09565210490658356\n",
            "Iteration 2341 Loss = 0.09561894789748711\n",
            "Iteration 2342 Loss = 0.09558580947966873\n",
            "Iteration 2343 Loss = 0.09555268964270418\n",
            "Iteration 2344 Loss = 0.0955195883761752\n",
            "Iteration 2345 Loss = 0.09548650566966914\n",
            "Iteration 2346 Loss = 0.09545344151277932\n",
            "Iteration 2347 Loss = 0.095420395895105\n",
            "Iteration 2348 Loss = 0.09538736880625108\n",
            "Iteration 2349 Loss = 0.09535436023582844\n",
            "Iteration 2350 Loss = 0.09532137017345343\n",
            "Iteration 2351 Loss = 0.0952883986087489\n",
            "Iteration 2352 Loss = 0.09525544553134291\n",
            "Iteration 2353 Loss = 0.09522251093086956\n",
            "Iteration 2354 Loss = 0.09518959479696887\n",
            "Iteration 2355 Loss = 0.09515669711928648\n",
            "Iteration 2356 Loss = 0.09512381788747382\n",
            "Iteration 2357 Loss = 0.09509095709118831\n",
            "Iteration 2358 Loss = 0.09505811472009312\n",
            "Iteration 2359 Loss = 0.09502529076385702\n",
            "Iteration 2360 Loss = 0.09499248521215486\n",
            "Iteration 2361 Loss = 0.09495969805466718\n",
            "Iteration 2362 Loss = 0.09492692928108012\n",
            "Iteration 2363 Loss = 0.09489417888108562\n",
            "Iteration 2364 Loss = 0.09486144684438186\n",
            "Iteration 2365 Loss = 0.0948287331606722\n",
            "Iteration 2366 Loss = 0.09479603781966625\n",
            "Iteration 2367 Loss = 0.0947633608110791\n",
            "Iteration 2368 Loss = 0.09473070212463133\n",
            "Iteration 2369 Loss = 0.09469806175005044\n",
            "Iteration 2370 Loss = 0.09466543967706802\n",
            "Iteration 2371 Loss = 0.09463283589542296\n",
            "Iteration 2372 Loss = 0.0946002503948588\n",
            "Iteration 2373 Loss = 0.09456768316512525\n",
            "Iteration 2374 Loss = 0.0945351341959781\n",
            "Iteration 2375 Loss = 0.0945026034771784\n",
            "Iteration 2376 Loss = 0.09447009099849298\n",
            "Iteration 2377 Loss = 0.09443759674969453\n",
            "Iteration 2378 Loss = 0.0944051207205619\n",
            "Iteration 2379 Loss = 0.09437266290087873\n",
            "Iteration 2380 Loss = 0.09434022328043501\n",
            "Iteration 2381 Loss = 0.09430780184902657\n",
            "Iteration 2382 Loss = 0.0942753985964546\n",
            "Iteration 2383 Loss = 0.09424301351252608\n",
            "Iteration 2384 Loss = 0.09421064658705383\n",
            "Iteration 2385 Loss = 0.09417829780985652\n",
            "Iteration 2386 Loss = 0.09414596717075804\n",
            "Iteration 2387 Loss = 0.09411365465958836\n",
            "Iteration 2388 Loss = 0.09408136026618329\n",
            "Iteration 2389 Loss = 0.09404908398038372\n",
            "Iteration 2390 Loss = 0.09401682579203689\n",
            "Iteration 2391 Loss = 0.0939845856909955\n",
            "Iteration 2392 Loss = 0.0939523636671179\n",
            "Iteration 2393 Loss = 0.09392015971026804\n",
            "Iteration 2394 Loss = 0.09388797381031577\n",
            "Iteration 2395 Loss = 0.09385580595713633\n",
            "Iteration 2396 Loss = 0.09382365614061104\n",
            "Iteration 2397 Loss = 0.0937915243506266\n",
            "Iteration 2398 Loss = 0.09375941057707544\n",
            "Iteration 2399 Loss = 0.09372731480985567\n",
            "Iteration 2400 Loss = 0.0936952370388711\n",
            "Iteration 2401 Loss = 0.09366317725403107\n",
            "Iteration 2402 Loss = 0.0936311354452508\n",
            "Iteration 2403 Loss = 0.09359911160245091\n",
            "Iteration 2404 Loss = 0.0935671057155578\n",
            "Iteration 2405 Loss = 0.09353511777450373\n",
            "Iteration 2406 Loss = 0.09350314776922601\n",
            "Iteration 2407 Loss = 0.0934711956896683\n",
            "Iteration 2408 Loss = 0.09343926152577942\n",
            "Iteration 2409 Loss = 0.09340734526751399\n",
            "Iteration 2410 Loss = 0.09337544690483242\n",
            "Iteration 2411 Loss = 0.09334356642770018\n",
            "Iteration 2412 Loss = 0.09331170382608917\n",
            "Iteration 2413 Loss = 0.09327985908997635\n",
            "Iteration 2414 Loss = 0.09324803220934445\n",
            "Iteration 2415 Loss = 0.09321622317418174\n",
            "Iteration 2416 Loss = 0.09318443197448253\n",
            "Iteration 2417 Loss = 0.0931526586002459\n",
            "Iteration 2418 Loss = 0.09312090304147741\n",
            "Iteration 2419 Loss = 0.09308916528818771\n",
            "Iteration 2420 Loss = 0.09305744533039326\n",
            "Iteration 2421 Loss = 0.09302574315811613\n",
            "Iteration 2422 Loss = 0.09299405876138342\n",
            "Iteration 2423 Loss = 0.09296239213022905\n",
            "Iteration 2424 Loss = 0.09293074325469118\n",
            "Iteration 2425 Loss = 0.09289911212481433\n",
            "Iteration 2426 Loss = 0.0928674987306486\n",
            "Iteration 2427 Loss = 0.09283590306224936\n",
            "Iteration 2428 Loss = 0.09280432510967765\n",
            "Iteration 2429 Loss = 0.09277276486300017\n",
            "Iteration 2430 Loss = 0.09274122231228912\n",
            "Iteration 2431 Loss = 0.09270969744762261\n",
            "Iteration 2432 Loss = 0.0926781902590835\n",
            "Iteration 2433 Loss = 0.092646700736761\n",
            "Iteration 2434 Loss = 0.09261522887074944\n",
            "Iteration 2435 Loss = 0.09258377465114907\n",
            "Iteration 2436 Loss = 0.0925523380680653\n",
            "Iteration 2437 Loss = 0.09252091911160926\n",
            "Iteration 2438 Loss = 0.09248951777189776\n",
            "Iteration 2439 Loss = 0.09245813403905284\n",
            "Iteration 2440 Loss = 0.09242676790320233\n",
            "Iteration 2441 Loss = 0.09239541935447955\n",
            "Iteration 2442 Loss = 0.09236408838302333\n",
            "Iteration 2443 Loss = 0.09233277497897796\n",
            "Iteration 2444 Loss = 0.09230147913249322\n",
            "Iteration 2445 Loss = 0.09227020083372475\n",
            "Iteration 2446 Loss = 0.09223894007283336\n",
            "Iteration 2447 Loss = 0.0922076968399855\n",
            "Iteration 2448 Loss = 0.0921764711253531\n",
            "Iteration 2449 Loss = 0.09214526291911354\n",
            "Iteration 2450 Loss = 0.0921140722114499\n",
            "Iteration 2451 Loss = 0.09208289899255068\n",
            "Iteration 2452 Loss = 0.09205174325260967\n",
            "Iteration 2453 Loss = 0.09202060498182661\n",
            "Iteration 2454 Loss = 0.09198948417040617\n",
            "Iteration 2455 Loss = 0.0919583808085592\n",
            "Iteration 2456 Loss = 0.09192729488650109\n",
            "Iteration 2457 Loss = 0.0918962263944538\n",
            "Iteration 2458 Loss = 0.09186517532264397\n",
            "Iteration 2459 Loss = 0.09183414166130403\n",
            "Iteration 2460 Loss = 0.09180312540067206\n",
            "Iteration 2461 Loss = 0.09177212653099125\n",
            "Iteration 2462 Loss = 0.09174114504251028\n",
            "Iteration 2463 Loss = 0.09171018092548369\n",
            "Iteration 2464 Loss = 0.09167923417017106\n",
            "Iteration 2465 Loss = 0.09164830476683757\n",
            "Iteration 2466 Loss = 0.09161739270575406\n",
            "Iteration 2467 Loss = 0.09158649797719656\n",
            "Iteration 2468 Loss = 0.09155562057144667\n",
            "Iteration 2469 Loss = 0.09152476047879131\n",
            "Iteration 2470 Loss = 0.09149391768952315\n",
            "Iteration 2471 Loss = 0.09146309219393998\n",
            "Iteration 2472 Loss = 0.09143228398234521\n",
            "Iteration 2473 Loss = 0.0914014930450476\n",
            "Iteration 2474 Loss = 0.09137071937236134\n",
            "Iteration 2475 Loss = 0.09133996295460618\n",
            "Iteration 2476 Loss = 0.09130922378210715\n",
            "Iteration 2477 Loss = 0.09127850184519475\n",
            "Iteration 2478 Loss = 0.09124779713420492\n",
            "Iteration 2479 Loss = 0.09121710963947904\n",
            "Iteration 2480 Loss = 0.09118643935136389\n",
            "Iteration 2481 Loss = 0.09115578626021155\n",
            "Iteration 2482 Loss = 0.0911251503563797\n",
            "Iteration 2483 Loss = 0.09109453163023128\n",
            "Iteration 2484 Loss = 0.09106393007213465\n",
            "Iteration 2485 Loss = 0.09103334567246377\n",
            "Iteration 2486 Loss = 0.09100277842159771\n",
            "Iteration 2487 Loss = 0.09097222830992109\n",
            "Iteration 2488 Loss = 0.09094169532782388\n",
            "Iteration 2489 Loss = 0.09091117946570162\n",
            "Iteration 2490 Loss = 0.0908806807139548\n",
            "Iteration 2491 Loss = 0.09085019906298966\n",
            "Iteration 2492 Loss = 0.0908197345032177\n",
            "Iteration 2493 Loss = 0.09078928702505572\n",
            "Iteration 2494 Loss = 0.09075885661892642\n",
            "Iteration 2495 Loss = 0.09072844327525706\n",
            "Iteration 2496 Loss = 0.09069804698448053\n",
            "Iteration 2497 Loss = 0.0906676677370355\n",
            "Iteration 2498 Loss = 0.09063730552336544\n",
            "Iteration 2499 Loss = 0.09060696033391982\n",
            "Iteration 2500 Loss = 0.09057663215915278\n",
            "Iteration 2501 Loss = 0.09054632098952418\n",
            "Iteration 2502 Loss = 0.09051602681549917\n",
            "Iteration 2503 Loss = 0.09048574962754814\n",
            "Iteration 2504 Loss = 0.09045548941614719\n",
            "Iteration 2505 Loss = 0.09042524617177715\n",
            "Iteration 2506 Loss = 0.0903950198849249\n",
            "Iteration 2507 Loss = 0.09036481054608202\n",
            "Iteration 2508 Loss = 0.09033461814574559\n",
            "Iteration 2509 Loss = 0.09030444267441856\n",
            "Iteration 2510 Loss = 0.09027428412260835\n",
            "Iteration 2511 Loss = 0.0902441424808283\n",
            "Iteration 2512 Loss = 0.09021401773959684\n",
            "Iteration 2513 Loss = 0.09018390988943764\n",
            "Iteration 2514 Loss = 0.09015381892088017\n",
            "Iteration 2515 Loss = 0.09012374482445847\n",
            "Iteration 2516 Loss = 0.0900936875907123\n",
            "Iteration 2517 Loss = 0.09006364721018678\n",
            "Iteration 2518 Loss = 0.09003362367343236\n",
            "Iteration 2519 Loss = 0.09000361697100452\n",
            "Iteration 2520 Loss = 0.08997362709346418\n",
            "Iteration 2521 Loss = 0.08994365403137763\n",
            "Iteration 2522 Loss = 0.08991369777531608\n",
            "Iteration 2523 Loss = 0.08988375831585685\n",
            "Iteration 2524 Loss = 0.08985383564358146\n",
            "Iteration 2525 Loss = 0.08982392974907769\n",
            "Iteration 2526 Loss = 0.08979404062293798\n",
            "Iteration 2527 Loss = 0.08976416825576024\n",
            "Iteration 2528 Loss = 0.08973431263814753\n",
            "Iteration 2529 Loss = 0.08970447376070856\n",
            "Iteration 2530 Loss = 0.0896746516140569\n",
            "Iteration 2531 Loss = 0.08964484618881154\n",
            "Iteration 2532 Loss = 0.08961505747559662\n",
            "Iteration 2533 Loss = 0.08958528546504178\n",
            "Iteration 2534 Loss = 0.08955553014778184\n",
            "Iteration 2535 Loss = 0.08952579151445643\n",
            "Iteration 2536 Loss = 0.08949606955571127\n",
            "Iteration 2537 Loss = 0.08946636426219642\n",
            "Iteration 2538 Loss = 0.08943667562456788\n",
            "Iteration 2539 Loss = 0.0894070036334865\n",
            "Iteration 2540 Loss = 0.08937734827961874\n",
            "Iteration 2541 Loss = 0.08934770955363554\n",
            "Iteration 2542 Loss = 0.08931808744621425\n",
            "Iteration 2543 Loss = 0.08928848194803607\n",
            "Iteration 2544 Loss = 0.08925889304978855\n",
            "Iteration 2545 Loss = 0.08922932074216405\n",
            "Iteration 2546 Loss = 0.08919976501585988\n",
            "Iteration 2547 Loss = 0.08917022586157905\n",
            "Iteration 2548 Loss = 0.08914070327002938\n",
            "Iteration 2549 Loss = 0.08911119723192412\n",
            "Iteration 2550 Loss = 0.08908170773798203\n",
            "Iteration 2551 Loss = 0.08905223477892614\n",
            "Iteration 2552 Loss = 0.0890227783454857\n",
            "Iteration 2553 Loss = 0.08899333842839449\n",
            "Iteration 2554 Loss = 0.0889639150183919\n",
            "Iteration 2555 Loss = 0.08893450810622214\n",
            "Iteration 2556 Loss = 0.08890511768263502\n",
            "Iteration 2557 Loss = 0.08887574373838501\n",
            "Iteration 2558 Loss = 0.08884638626423251\n",
            "Iteration 2559 Loss = 0.08881704525094249\n",
            "Iteration 2560 Loss = 0.08878772068928506\n",
            "Iteration 2561 Loss = 0.0887584125700358\n",
            "Iteration 2562 Loss = 0.08872912088397551\n",
            "Iteration 2563 Loss = 0.08869984562189001\n",
            "Iteration 2564 Loss = 0.0886705867745703\n",
            "Iteration 2565 Loss = 0.08864134433281261\n",
            "Iteration 2566 Loss = 0.08861211828741797\n",
            "Iteration 2567 Loss = 0.08858290862919325\n",
            "Iteration 2568 Loss = 0.0885537153489498\n",
            "Iteration 2569 Loss = 0.0885245384375046\n",
            "Iteration 2570 Loss = 0.08849537788567956\n",
            "Iteration 2571 Loss = 0.08846623368430173\n",
            "Iteration 2572 Loss = 0.08843710582420335\n",
            "Iteration 2573 Loss = 0.0884079942962218\n",
            "Iteration 2574 Loss = 0.08837889909119949\n",
            "Iteration 2575 Loss = 0.08834982019998427\n",
            "Iteration 2576 Loss = 0.08832075761342881\n",
            "Iteration 2577 Loss = 0.08829171132239093\n",
            "Iteration 2578 Loss = 0.08826268131773382\n",
            "Iteration 2579 Loss = 0.08823366759032557\n",
            "Iteration 2580 Loss = 0.08820467013103955\n",
            "Iteration 2581 Loss = 0.08817568893075396\n",
            "Iteration 2582 Loss = 0.08814672398035238\n",
            "Iteration 2583 Loss = 0.08811777527072347\n",
            "Iteration 2584 Loss = 0.08808884279276114\n",
            "Iteration 2585 Loss = 0.0880599265373638\n",
            "Iteration 2586 Loss = 0.08803102649543566\n",
            "Iteration 2587 Loss = 0.08800214265788574\n",
            "Iteration 2588 Loss = 0.08797327501562822\n",
            "Iteration 2589 Loss = 0.08794442355958221\n",
            "Iteration 2590 Loss = 0.08791558828067209\n",
            "Iteration 2591 Loss = 0.0878867691698273\n",
            "Iteration 2592 Loss = 0.08785796621798232\n",
            "Iteration 2593 Loss = 0.08782917941607668\n",
            "Iteration 2594 Loss = 0.08780040875505493\n",
            "Iteration 2595 Loss = 0.08777165422586712\n",
            "Iteration 2596 Loss = 0.08774291581946789\n",
            "Iteration 2597 Loss = 0.08771419352681696\n",
            "Iteration 2598 Loss = 0.08768548733887967\n",
            "Iteration 2599 Loss = 0.08765679724662573\n",
            "Iteration 2600 Loss = 0.08762812324103024\n",
            "Iteration 2601 Loss = 0.08759946531307349\n",
            "Iteration 2602 Loss = 0.08757082345374047\n",
            "Iteration 2603 Loss = 0.0875421976540217\n",
            "Iteration 2604 Loss = 0.0875135879049123\n",
            "Iteration 2605 Loss = 0.08748499419741261\n",
            "Iteration 2606 Loss = 0.08745641652252811\n",
            "Iteration 2607 Loss = 0.08742785487126928\n",
            "Iteration 2608 Loss = 0.08739930923465132\n",
            "Iteration 2609 Loss = 0.08737077960369516\n",
            "Iteration 2610 Loss = 0.0873422659694261\n",
            "Iteration 2611 Loss = 0.08731376832287488\n",
            "Iteration 2612 Loss = 0.08728528665507698\n",
            "Iteration 2613 Loss = 0.08725682095707318\n",
            "Iteration 2614 Loss = 0.08722837121990894\n",
            "Iteration 2615 Loss = 0.08719993743463508\n",
            "Iteration 2616 Loss = 0.0871715195923073\n",
            "Iteration 2617 Loss = 0.08714311768398648\n",
            "Iteration 2618 Loss = 0.08711473170073801\n",
            "Iteration 2619 Loss = 0.0870863616336329\n",
            "Iteration 2620 Loss = 0.08705800747374695\n",
            "Iteration 2621 Loss = 0.08702966921216064\n",
            "Iteration 2622 Loss = 0.08700134683995979\n",
            "Iteration 2623 Loss = 0.0869730403482353\n",
            "Iteration 2624 Loss = 0.08694474972808293\n",
            "Iteration 2625 Loss = 0.0869164749706032\n",
            "Iteration 2626 Loss = 0.08688821606690199\n",
            "Iteration 2627 Loss = 0.0868599730080901\n",
            "Iteration 2628 Loss = 0.08683174578528317\n",
            "Iteration 2629 Loss = 0.08680353438960171\n",
            "Iteration 2630 Loss = 0.08677533881217178\n",
            "Iteration 2631 Loss = 0.08674715904412351\n",
            "Iteration 2632 Loss = 0.08671899507659296\n",
            "Iteration 2633 Loss = 0.08669084690072032\n",
            "Iteration 2634 Loss = 0.08666271450765152\n",
            "Iteration 2635 Loss = 0.08663459788853672\n",
            "Iteration 2636 Loss = 0.08660649703453172\n",
            "Iteration 2637 Loss = 0.0865784119367967\n",
            "Iteration 2638 Loss = 0.08655034258649717\n",
            "Iteration 2639 Loss = 0.08652228897480355\n",
            "Iteration 2640 Loss = 0.08649425109289086\n",
            "Iteration 2641 Loss = 0.08646622893193971\n",
            "Iteration 2642 Loss = 0.08643822248313508\n",
            "Iteration 2643 Loss = 0.08641023173766703\n",
            "Iteration 2644 Loss = 0.0863822566867308\n",
            "Iteration 2645 Loss = 0.08635429732152641\n",
            "Iteration 2646 Loss = 0.08632635363325859\n",
            "Iteration 2647 Loss = 0.08629842561313737\n",
            "Iteration 2648 Loss = 0.08627051325237761\n",
            "Iteration 2649 Loss = 0.0862426165421988\n",
            "Iteration 2650 Loss = 0.08621473547382583\n",
            "Iteration 2651 Loss = 0.08618687003848824\n",
            "Iteration 2652 Loss = 0.08615902022742031\n",
            "Iteration 2653 Loss = 0.0861311860318618\n",
            "Iteration 2654 Loss = 0.08610336744305673\n",
            "Iteration 2655 Loss = 0.08607556445225448\n",
            "Iteration 2656 Loss = 0.08604777705070898\n",
            "Iteration 2657 Loss = 0.08602000522967958\n",
            "Iteration 2658 Loss = 0.08599224898043005\n",
            "Iteration 2659 Loss = 0.08596450829422916\n",
            "Iteration 2660 Loss = 0.08593678316235077\n",
            "Iteration 2661 Loss = 0.08590907357607344\n",
            "Iteration 2662 Loss = 0.08588137952668055\n",
            "Iteration 2663 Loss = 0.08585370100546084\n",
            "Iteration 2664 Loss = 0.08582603800370722\n",
            "Iteration 2665 Loss = 0.0857983905127182\n",
            "Iteration 2666 Loss = 0.0857707585237966\n",
            "Iteration 2667 Loss = 0.08574314202825044\n",
            "Iteration 2668 Loss = 0.08571554101739241\n",
            "Iteration 2669 Loss = 0.0856879554825403\n",
            "Iteration 2670 Loss = 0.08566038541501657\n",
            "Iteration 2671 Loss = 0.08563283080614865\n",
            "Iteration 2672 Loss = 0.08560529164726882\n",
            "Iteration 2673 Loss = 0.08557776792971421\n",
            "Iteration 2674 Loss = 0.08555025964482672\n",
            "Iteration 2675 Loss = 0.08552276678395332\n",
            "Iteration 2676 Loss = 0.08549528933844566\n",
            "Iteration 2677 Loss = 0.08546782729966022\n",
            "Iteration 2678 Loss = 0.08544038065895834\n",
            "Iteration 2679 Loss = 0.08541294940770634\n",
            "Iteration 2680 Loss = 0.08538553353727532\n",
            "Iteration 2681 Loss = 0.0853581330390411\n",
            "Iteration 2682 Loss = 0.08533074790438455\n",
            "Iteration 2683 Loss = 0.08530337812469112\n",
            "Iteration 2684 Loss = 0.08527602369135127\n",
            "Iteration 2685 Loss = 0.08524868459576013\n",
            "Iteration 2686 Loss = 0.08522136082931793\n",
            "Iteration 2687 Loss = 0.08519405238342939\n",
            "Iteration 2688 Loss = 0.08516675924950426\n",
            "Iteration 2689 Loss = 0.08513948141895725\n",
            "Iteration 2690 Loss = 0.08511221888320725\n",
            "Iteration 2691 Loss = 0.08508497163367885\n",
            "Iteration 2692 Loss = 0.0850577396618008\n",
            "Iteration 2693 Loss = 0.08503052295900689\n",
            "Iteration 2694 Loss = 0.08500332151673567\n",
            "Iteration 2695 Loss = 0.08497613532643043\n",
            "Iteration 2696 Loss = 0.0849489643795394\n",
            "Iteration 2697 Loss = 0.08492180866751559\n",
            "Iteration 2698 Loss = 0.08489466818181661\n",
            "Iteration 2699 Loss = 0.08486754291390507\n",
            "Iteration 2700 Loss = 0.08484043285524827\n",
            "Iteration 2701 Loss = 0.08481333799731836\n",
            "Iteration 2702 Loss = 0.08478625833159222\n",
            "Iteration 2703 Loss = 0.08475919384955136\n",
            "Iteration 2704 Loss = 0.08473214454268249\n",
            "Iteration 2705 Loss = 0.08470511040247666\n",
            "Iteration 2706 Loss = 0.08467809142042984\n",
            "Iteration 2707 Loss = 0.08465108758804277\n",
            "Iteration 2708 Loss = 0.08462409889682117\n",
            "Iteration 2709 Loss = 0.08459712533827513\n",
            "Iteration 2710 Loss = 0.08457016690391966\n",
            "Iteration 2711 Loss = 0.08454322358527463\n",
            "Iteration 2712 Loss = 0.08451629537386478\n",
            "Iteration 2713 Loss = 0.08448938226121913\n",
            "Iteration 2714 Loss = 0.0844624842388719\n",
            "Iteration 2715 Loss = 0.08443560129836188\n",
            "Iteration 2716 Loss = 0.08440873343123247\n",
            "Iteration 2717 Loss = 0.08438188062903222\n",
            "Iteration 2718 Loss = 0.08435504288331411\n",
            "Iteration 2719 Loss = 0.08432822018563553\n",
            "Iteration 2720 Loss = 0.08430141252755942\n",
            "Iteration 2721 Loss = 0.08427461990065292\n",
            "Iteration 2722 Loss = 0.0842478422964878\n",
            "Iteration 2723 Loss = 0.08422107970664096\n",
            "Iteration 2724 Loss = 0.08419433212269364\n",
            "Iteration 2725 Loss = 0.08416759953623207\n",
            "Iteration 2726 Loss = 0.08414088193884704\n",
            "Iteration 2727 Loss = 0.08411417932213412\n",
            "Iteration 2728 Loss = 0.08408749167769364\n",
            "Iteration 2729 Loss = 0.08406081899713058\n",
            "Iteration 2730 Loss = 0.08403416127205462\n",
            "Iteration 2731 Loss = 0.08400751849408002\n",
            "Iteration 2732 Loss = 0.08398089065482604\n",
            "Iteration 2733 Loss = 0.08395427774591635\n",
            "Iteration 2734 Loss = 0.08392767975897962\n",
            "Iteration 2735 Loss = 0.08390109668564884\n",
            "Iteration 2736 Loss = 0.083874528517562\n",
            "Iteration 2737 Loss = 0.08384797524636164\n",
            "Iteration 2738 Loss = 0.08382143686369499\n",
            "Iteration 2739 Loss = 0.08379491336121402\n",
            "Iteration 2740 Loss = 0.08376840473057531\n",
            "Iteration 2741 Loss = 0.08374191096344029\n",
            "Iteration 2742 Loss = 0.08371543205147461\n",
            "Iteration 2743 Loss = 0.0836889679863492\n",
            "Iteration 2744 Loss = 0.0836625187597393\n",
            "Iteration 2745 Loss = 0.08363608436332501\n",
            "Iteration 2746 Loss = 0.08360966478879066\n",
            "Iteration 2747 Loss = 0.08358326002782572\n",
            "Iteration 2748 Loss = 0.08355687007212434\n",
            "Iteration 2749 Loss = 0.08353049491338477\n",
            "Iteration 2750 Loss = 0.08350413454331068\n",
            "Iteration 2751 Loss = 0.08347778895360974\n",
            "Iteration 2752 Loss = 0.08345145813599471\n",
            "Iteration 2753 Loss = 0.08342514208218264\n",
            "Iteration 2754 Loss = 0.08339884078389544\n",
            "Iteration 2755 Loss = 0.08337255423285968\n",
            "Iteration 2756 Loss = 0.08334628242080659\n",
            "Iteration 2757 Loss = 0.0833200253394718\n",
            "Iteration 2758 Loss = 0.08329378298059577\n",
            "Iteration 2759 Loss = 0.08326755533592364\n",
            "Iteration 2760 Loss = 0.08324134239720504\n",
            "Iteration 2761 Loss = 0.0832151441561942\n",
            "Iteration 2762 Loss = 0.08318896060465017\n",
            "Iteration 2763 Loss = 0.08316279173433658\n",
            "Iteration 2764 Loss = 0.08313663753702144\n",
            "Iteration 2765 Loss = 0.08311049800447759\n",
            "Iteration 2766 Loss = 0.0830843731284822\n",
            "Iteration 2767 Loss = 0.08305826290081772\n",
            "Iteration 2768 Loss = 0.08303216731327055\n",
            "Iteration 2769 Loss = 0.08300608635763167\n",
            "Iteration 2770 Loss = 0.08298002002569739\n",
            "Iteration 2771 Loss = 0.08295396830926773\n",
            "Iteration 2772 Loss = 0.08292793120014803\n",
            "Iteration 2773 Loss = 0.08290190869014764\n",
            "Iteration 2774 Loss = 0.08287590077108091\n",
            "Iteration 2775 Loss = 0.08284990743476656\n",
            "Iteration 2776 Loss = 0.08282392867302815\n",
            "Iteration 2777 Loss = 0.08279796447769334\n",
            "Iteration 2778 Loss = 0.08277201484059508\n",
            "Iteration 2779 Loss = 0.08274607975357023\n",
            "Iteration 2780 Loss = 0.08272015920846061\n",
            "Iteration 2781 Loss = 0.08269425319711235\n",
            "Iteration 2782 Loss = 0.08266836171137645\n",
            "Iteration 2783 Loss = 0.08264248474310841\n",
            "Iteration 2784 Loss = 0.082616622284168\n",
            "Iteration 2785 Loss = 0.08259077432642\n",
            "Iteration 2786 Loss = 0.08256494086173345\n",
            "Iteration 2787 Loss = 0.08253912188198197\n",
            "Iteration 2788 Loss = 0.08251331737904379\n",
            "Iteration 2789 Loss = 0.08248752734480179\n",
            "Iteration 2790 Loss = 0.08246175177114326\n",
            "Iteration 2791 Loss = 0.08243599064996004\n",
            "Iteration 2792 Loss = 0.08241024397314864\n",
            "Iteration 2793 Loss = 0.08238451173261008\n",
            "Iteration 2794 Loss = 0.08235879392024967\n",
            "Iteration 2795 Loss = 0.08233309052797776\n",
            "Iteration 2796 Loss = 0.08230740154770871\n",
            "Iteration 2797 Loss = 0.08228172697136193\n",
            "Iteration 2798 Loss = 0.08225606679086053\n",
            "Iteration 2799 Loss = 0.08223042099813337\n",
            "Iteration 2800 Loss = 0.08220478958511257\n",
            "Iteration 2801 Loss = 0.08217917254373584\n",
            "Iteration 2802 Loss = 0.08215356986594456\n",
            "Iteration 2803 Loss = 0.08212798154368539\n",
            "Iteration 2804 Loss = 0.08210240756890874\n",
            "Iteration 2805 Loss = 0.08207684793357028\n",
            "Iteration 2806 Loss = 0.0820513026296294\n",
            "Iteration 2807 Loss = 0.08202577164905082\n",
            "Iteration 2808 Loss = 0.08200025498380323\n",
            "Iteration 2809 Loss = 0.08197475262585978\n",
            "Iteration 2810 Loss = 0.08194926456719857\n",
            "Iteration 2811 Loss = 0.08192379079980182\n",
            "Iteration 2812 Loss = 0.08189833131565634\n",
            "Iteration 2813 Loss = 0.08187288610675345\n",
            "Iteration 2814 Loss = 0.08184745516508897\n",
            "Iteration 2815 Loss = 0.08182203848266321\n",
            "Iteration 2816 Loss = 0.08179663605148102\n",
            "Iteration 2817 Loss = 0.08177124786355137\n",
            "Iteration 2818 Loss = 0.08174587391088835\n",
            "Iteration 2819 Loss = 0.08172051418551016\n",
            "Iteration 2820 Loss = 0.08169516867943936\n",
            "Iteration 2821 Loss = 0.08166983738470313\n",
            "Iteration 2822 Loss = 0.08164452029333315\n",
            "Iteration 2823 Loss = 0.08161921739736563\n",
            "Iteration 2824 Loss = 0.08159392868884094\n",
            "Iteration 2825 Loss = 0.08156865415980427\n",
            "Iteration 2826 Loss = 0.08154339380230506\n",
            "Iteration 2827 Loss = 0.08151814760839735\n",
            "Iteration 2828 Loss = 0.08149291557013941\n",
            "Iteration 2829 Loss = 0.0814676976795942\n",
            "Iteration 2830 Loss = 0.08144249392882903\n",
            "Iteration 2831 Loss = 0.08141730430991552\n",
            "Iteration 2832 Loss = 0.08139212881493028\n",
            "Iteration 2833 Loss = 0.08136696743595359\n",
            "Iteration 2834 Loss = 0.08134182016507063\n",
            "Iteration 2835 Loss = 0.08131668699437104\n",
            "Iteration 2836 Loss = 0.08129156791594859\n",
            "Iteration 2837 Loss = 0.08126646292190176\n",
            "Iteration 2838 Loss = 0.08124137200433344\n",
            "Iteration 2839 Loss = 0.08121629515535093\n",
            "Iteration 2840 Loss = 0.08119123236706578\n",
            "Iteration 2841 Loss = 0.08116618363159414\n",
            "Iteration 2842 Loss = 0.08114114894105649\n",
            "Iteration 2843 Loss = 0.08111612828757797\n",
            "Iteration 2844 Loss = 0.08109112166328777\n",
            "Iteration 2845 Loss = 0.0810661290603197\n",
            "Iteration 2846 Loss = 0.08104115047081191\n",
            "Iteration 2847 Loss = 0.08101618588690704\n",
            "Iteration 2848 Loss = 0.0809912353007521\n",
            "Iteration 2849 Loss = 0.08096629870449848\n",
            "Iteration 2850 Loss = 0.08094137609030204\n",
            "Iteration 2851 Loss = 0.08091646745032267\n",
            "Iteration 2852 Loss = 0.0808915727767254\n",
            "Iteration 2853 Loss = 0.080866692061679\n",
            "Iteration 2854 Loss = 0.08084182529735692\n",
            "Iteration 2855 Loss = 0.08081697247593683\n",
            "Iteration 2856 Loss = 0.08079213358960083\n",
            "Iteration 2857 Loss = 0.08076730863053577\n",
            "Iteration 2858 Loss = 0.08074249759093222\n",
            "Iteration 2859 Loss = 0.08071770046298565\n",
            "Iteration 2860 Loss = 0.08069291723889566\n",
            "Iteration 2861 Loss = 0.08066814791086648\n",
            "Iteration 2862 Loss = 0.08064339247110631\n",
            "Iteration 2863 Loss = 0.08061865091182803\n",
            "Iteration 2864 Loss = 0.08059392322524879\n",
            "Iteration 2865 Loss = 0.08056920940359011\n",
            "Iteration 2866 Loss = 0.08054450943907797\n",
            "Iteration 2867 Loss = 0.08051982332394217\n",
            "Iteration 2868 Loss = 0.0804951510504179\n",
            "Iteration 2869 Loss = 0.08047049261074385\n",
            "Iteration 2870 Loss = 0.0804458479971633\n",
            "Iteration 2871 Loss = 0.08042121720192386\n",
            "Iteration 2872 Loss = 0.08039660021727758\n",
            "Iteration 2873 Loss = 0.08037199703548079\n",
            "Iteration 2874 Loss = 0.08034740764879415\n",
            "Iteration 2875 Loss = 0.08032283204948283\n",
            "Iteration 2876 Loss = 0.08029827022981602\n",
            "Iteration 2877 Loss = 0.08027372218206745\n",
            "Iteration 2878 Loss = 0.08024918789851522\n",
            "Iteration 2879 Loss = 0.0802246673714418\n",
            "Iteration 2880 Loss = 0.08020016059313334\n",
            "Iteration 2881 Loss = 0.08017566755588129\n",
            "Iteration 2882 Loss = 0.08015118825198106\n",
            "Iteration 2883 Loss = 0.08012672267373218\n",
            "Iteration 2884 Loss = 0.08010227081343838\n",
            "Iteration 2885 Loss = 0.08007783266340823\n",
            "Iteration 2886 Loss = 0.08005340821595433\n",
            "Iteration 2887 Loss = 0.08002899746339344\n",
            "Iteration 2888 Loss = 0.08000460039804683\n",
            "Iteration 2889 Loss = 0.07998021701223999\n",
            "Iteration 2890 Loss = 0.07995584729830287\n",
            "Iteration 2891 Loss = 0.07993149124856931\n",
            "Iteration 2892 Loss = 0.07990714885537813\n",
            "Iteration 2893 Loss = 0.07988282011107165\n",
            "Iteration 2894 Loss = 0.07985850500799718\n",
            "Iteration 2895 Loss = 0.07983420353850583\n",
            "Iteration 2896 Loss = 0.07980991569495331\n",
            "Iteration 2897 Loss = 0.07978564146969949\n",
            "Iteration 2898 Loss = 0.07976138085510846\n",
            "Iteration 2899 Loss = 0.07973713384354882\n",
            "Iteration 2900 Loss = 0.07971290042739308\n",
            "Iteration 2901 Loss = 0.07968868059901835\n",
            "Iteration 2902 Loss = 0.07966447435080587\n",
            "Iteration 2903 Loss = 0.07964028167514113\n",
            "Iteration 2904 Loss = 0.07961610256441418\n",
            "Iteration 2905 Loss = 0.07959193701101884\n",
            "Iteration 2906 Loss = 0.07956778500735358\n",
            "Iteration 2907 Loss = 0.07954364654582095\n",
            "Iteration 2908 Loss = 0.07951952161882782\n",
            "Iteration 2909 Loss = 0.07949541021878534\n",
            "Iteration 2910 Loss = 0.07947131233810883\n",
            "Iteration 2911 Loss = 0.07944722796921827\n",
            "Iteration 2912 Loss = 0.07942315710453703\n",
            "Iteration 2913 Loss = 0.07939909973649358\n",
            "Iteration 2914 Loss = 0.07937505585752015\n",
            "Iteration 2915 Loss = 0.07935102546005329\n",
            "Iteration 2916 Loss = 0.07932700853653415\n",
            "Iteration 2917 Loss = 0.07930300507940766\n",
            "Iteration 2918 Loss = 0.07927901508112296\n",
            "Iteration 2919 Loss = 0.07925503853413392\n",
            "Iteration 2920 Loss = 0.07923107543089822\n",
            "Iteration 2921 Loss = 0.07920712576387776\n",
            "Iteration 2922 Loss = 0.07918318952553904\n",
            "Iteration 2923 Loss = 0.07915926670835251\n",
            "Iteration 2924 Loss = 0.07913535730479278\n",
            "Iteration 2925 Loss = 0.07911146130733877\n",
            "Iteration 2926 Loss = 0.07908757870847338\n",
            "Iteration 2927 Loss = 0.07906370950068446\n",
            "Iteration 2928 Loss = 0.0790398536764633\n",
            "Iteration 2929 Loss = 0.07901601122830566\n",
            "Iteration 2930 Loss = 0.07899218214871159\n",
            "Iteration 2931 Loss = 0.07896836643018539\n",
            "Iteration 2932 Loss = 0.07894456406523512\n",
            "Iteration 2933 Loss = 0.0789207750463737\n",
            "Iteration 2934 Loss = 0.07889699936611776\n",
            "Iteration 2935 Loss = 0.07887323701698838\n",
            "Iteration 2936 Loss = 0.07884948799151058\n",
            "Iteration 2937 Loss = 0.07882575228221385\n",
            "Iteration 2938 Loss = 0.07880202988163172\n",
            "Iteration 2939 Loss = 0.078778320782302\n",
            "Iteration 2940 Loss = 0.0787546249767666\n",
            "Iteration 2941 Loss = 0.07873094245757148\n",
            "Iteration 2942 Loss = 0.07870727321726716\n",
            "Iteration 2943 Loss = 0.0786836172484078\n",
            "Iteration 2944 Loss = 0.07865997454355253\n",
            "Iteration 2945 Loss = 0.07863634509526377\n",
            "Iteration 2946 Loss = 0.07861272889610853\n",
            "Iteration 2947 Loss = 0.07858912593865805\n",
            "Iteration 2948 Loss = 0.07856553621548792\n",
            "Iteration 2949 Loss = 0.07854195971917698\n",
            "Iteration 2950 Loss = 0.07851839644230936\n",
            "Iteration 2951 Loss = 0.07849484637747281\n",
            "Iteration 2952 Loss = 0.07847130951725921\n",
            "Iteration 2953 Loss = 0.07844778585426468\n",
            "Iteration 2954 Loss = 0.07842427538108948\n",
            "Iteration 2955 Loss = 0.07840077809033806\n",
            "Iteration 2956 Loss = 0.07837729397461896\n",
            "Iteration 2957 Loss = 0.07835382302654477\n",
            "Iteration 2958 Loss = 0.07833036523873244\n",
            "Iteration 2959 Loss = 0.07830692060380295\n",
            "Iteration 2960 Loss = 0.07828348911438159\n",
            "Iteration 2961 Loss = 0.07826007076309721\n",
            "Iteration 2962 Loss = 0.07823666554258346\n",
            "Iteration 2963 Loss = 0.07821327344547796\n",
            "Iteration 2964 Loss = 0.07818989446442214\n",
            "Iteration 2965 Loss = 0.07816652859206194\n",
            "Iteration 2966 Loss = 0.07814317582104716\n",
            "Iteration 2967 Loss = 0.0781198361440318\n",
            "Iteration 2968 Loss = 0.07809650955367396\n",
            "Iteration 2969 Loss = 0.07807319604263621\n",
            "Iteration 2970 Loss = 0.07804989560358458\n",
            "Iteration 2971 Loss = 0.0780266082291895\n",
            "Iteration 2972 Loss = 0.07800333391212597\n",
            "Iteration 2973 Loss = 0.07798007264507227\n",
            "Iteration 2974 Loss = 0.07795682442071143\n",
            "Iteration 2975 Loss = 0.07793358923173042\n",
            "Iteration 2976 Loss = 0.07791036707081993\n",
            "Iteration 2977 Loss = 0.07788715793067526\n",
            "Iteration 2978 Loss = 0.07786396180399566\n",
            "Iteration 2979 Loss = 0.07784077868348432\n",
            "Iteration 2980 Loss = 0.07781760856184873\n",
            "Iteration 2981 Loss = 0.07779445143180033\n",
            "Iteration 2982 Loss = 0.07777130728605461\n",
            "Iteration 2983 Loss = 0.07774817611733127\n",
            "Iteration 2984 Loss = 0.07772505791835402\n",
            "Iteration 2985 Loss = 0.07770195268185075\n",
            "Iteration 2986 Loss = 0.07767886040055332\n",
            "Iteration 2987 Loss = 0.07765578106719753\n",
            "Iteration 2988 Loss = 0.07763271467452353\n",
            "Iteration 2989 Loss = 0.07760966121527549\n",
            "Iteration 2990 Loss = 0.07758662068220148\n",
            "Iteration 2991 Loss = 0.07756359306805378\n",
            "Iteration 2992 Loss = 0.07754057836558863\n",
            "Iteration 2993 Loss = 0.07751757656756646\n",
            "Iteration 2994 Loss = 0.0774945876667518\n",
            "Iteration 2995 Loss = 0.07747161165591285\n",
            "Iteration 2996 Loss = 0.07744864852782249\n",
            "Iteration 2997 Loss = 0.07742569827525705\n",
            "Iteration 2998 Loss = 0.07740276089099718\n",
            "Iteration 2999 Loss = 0.0773798363678277\n",
            "Iteration 3000 Loss = 0.07735692469853724\n",
            "Iteration 3001 Loss = 0.07733402587591864\n",
            "Iteration 3002 Loss = 0.07731113989276868\n",
            "Iteration 3003 Loss = 0.0772882667418882\n",
            "Iteration 3004 Loss = 0.07726540641608201\n",
            "Iteration 3005 Loss = 0.07724255890815927\n",
            "Iteration 3006 Loss = 0.0772197242109327\n",
            "Iteration 3007 Loss = 0.07719690231721948\n",
            "Iteration 3008 Loss = 0.07717409321984044\n",
            "Iteration 3009 Loss = 0.0771512969116207\n",
            "Iteration 3010 Loss = 0.07712851338538941\n",
            "Iteration 3011 Loss = 0.07710574263397951\n",
            "Iteration 3012 Loss = 0.07708298465022825\n",
            "Iteration 3013 Loss = 0.07706023942697658\n",
            "Iteration 3014 Loss = 0.07703750695706987\n",
            "Iteration 3015 Loss = 0.07701478723335693\n",
            "Iteration 3016 Loss = 0.07699208024869116\n",
            "Iteration 3017 Loss = 0.07696938599592969\n",
            "Iteration 3018 Loss = 0.07694670446793357\n",
            "Iteration 3019 Loss = 0.07692403565756821\n",
            "Iteration 3020 Loss = 0.07690137955770253\n",
            "Iteration 3021 Loss = 0.07687873616120987\n",
            "Iteration 3022 Loss = 0.07685610546096726\n",
            "Iteration 3023 Loss = 0.07683348744985598\n",
            "Iteration 3024 Loss = 0.07681088212076094\n",
            "Iteration 3025 Loss = 0.07678828946657167\n",
            "Iteration 3026 Loss = 0.07676570948018084\n",
            "Iteration 3027 Loss = 0.07674314215448594\n",
            "Iteration 3028 Loss = 0.07672058748238807\n",
            "Iteration 3029 Loss = 0.07669804545679203\n",
            "Iteration 3030 Loss = 0.076675516070607\n",
            "Iteration 3031 Loss = 0.07665299931674595\n",
            "Iteration 3032 Loss = 0.07663049518812605\n",
            "Iteration 3033 Loss = 0.07660800367766807\n",
            "Iteration 3034 Loss = 0.07658552477829712\n",
            "Iteration 3035 Loss = 0.076563058482942\n",
            "Iteration 3036 Loss = 0.07654060478453562\n",
            "Iteration 3037 Loss = 0.07651816367601458\n",
            "Iteration 3038 Loss = 0.07649573515032027\n",
            "Iteration 3039 Loss = 0.07647331920039696\n",
            "Iteration 3040 Loss = 0.07645091581919357\n",
            "Iteration 3041 Loss = 0.0764285249996626\n",
            "Iteration 3042 Loss = 0.07640614673476061\n",
            "Iteration 3043 Loss = 0.07638378101744864\n",
            "Iteration 3044 Loss = 0.07636142784069039\n",
            "Iteration 3045 Loss = 0.07633908719745515\n",
            "Iteration 3046 Loss = 0.07631675908071486\n",
            "Iteration 3047 Loss = 0.07629444348344601\n",
            "Iteration 3048 Loss = 0.07627214039862881\n",
            "Iteration 3049 Loss = 0.0762498498192474\n",
            "Iteration 3050 Loss = 0.07622757173829016\n",
            "Iteration 3051 Loss = 0.0762053061487489\n",
            "Iteration 3052 Loss = 0.07618305304361983\n",
            "Iteration 3053 Loss = 0.07616081241590282\n",
            "Iteration 3054 Loss = 0.07613858425860172\n",
            "Iteration 3055 Loss = 0.07611636856472413\n",
            "Iteration 3056 Loss = 0.07609416532728226\n",
            "Iteration 3057 Loss = 0.07607197453929132\n",
            "Iteration 3058 Loss = 0.07604979619377095\n",
            "Iteration 3059 Loss = 0.07602763028374447\n",
            "Iteration 3060 Loss = 0.07600547680223953\n",
            "Iteration 3061 Loss = 0.07598333574228722\n",
            "Iteration 3062 Loss = 0.07596120709692274\n",
            "Iteration 3063 Loss = 0.07593909085918524\n",
            "Iteration 3064 Loss = 0.07591698702211767\n",
            "Iteration 3065 Loss = 0.07589489557876677\n",
            "Iteration 3066 Loss = 0.07587281652218361\n",
            "Iteration 3067 Loss = 0.07585074984542263\n",
            "Iteration 3068 Loss = 0.07582869554154263\n",
            "Iteration 3069 Loss = 0.07580665360360582\n",
            "Iteration 3070 Loss = 0.07578462402467886\n",
            "Iteration 3071 Loss = 0.07576260679783187\n",
            "Iteration 3072 Loss = 0.07574060191613885\n",
            "Iteration 3073 Loss = 0.07571860937267796\n",
            "Iteration 3074 Loss = 0.07569662916053117\n",
            "Iteration 3075 Loss = 0.0756746612727841\n",
            "Iteration 3076 Loss = 0.07565270570252645\n",
            "Iteration 3077 Loss = 0.07563076244285187\n",
            "Iteration 3078 Loss = 0.07560883148685756\n",
            "Iteration 3079 Loss = 0.07558691282764489\n",
            "Iteration 3080 Loss = 0.07556500645831894\n",
            "Iteration 3081 Loss = 0.07554311237198895\n",
            "Iteration 3082 Loss = 0.0755212305617676\n",
            "Iteration 3083 Loss = 0.07549936102077154\n",
            "Iteration 3084 Loss = 0.07547750374212159\n",
            "Iteration 3085 Loss = 0.07545565871894204\n",
            "Iteration 3086 Loss = 0.07543382594436139\n",
            "Iteration 3087 Loss = 0.07541200541151134\n",
            "Iteration 3088 Loss = 0.07539019711352844\n",
            "Iteration 3089 Loss = 0.07536840104355223\n",
            "Iteration 3090 Loss = 0.07534661719472646\n",
            "Iteration 3091 Loss = 0.07532484556019875\n",
            "Iteration 3092 Loss = 0.07530308613312033\n",
            "Iteration 3093 Loss = 0.07528133890664669\n",
            "Iteration 3094 Loss = 0.0752596038739368\n",
            "Iteration 3095 Loss = 0.07523788102815351\n",
            "Iteration 3096 Loss = 0.0752161703624636\n",
            "Iteration 3097 Loss = 0.07519447187003758\n",
            "Iteration 3098 Loss = 0.07517278554404982\n",
            "Iteration 3099 Loss = 0.07515111137767864\n",
            "Iteration 3100 Loss = 0.07512944936410598\n",
            "Iteration 3101 Loss = 0.07510779949651794\n",
            "Iteration 3102 Loss = 0.07508616176810395\n",
            "Iteration 3103 Loss = 0.07506453617205777\n",
            "Iteration 3104 Loss = 0.07504292270157659\n",
            "Iteration 3105 Loss = 0.07502132134986145\n",
            "Iteration 3106 Loss = 0.0749997321101173\n",
            "Iteration 3107 Loss = 0.07497815497555324\n",
            "Iteration 3108 Loss = 0.07495658993938144\n",
            "Iteration 3109 Loss = 0.07493503699481874\n",
            "Iteration 3110 Loss = 0.07491349613508484\n",
            "Iteration 3111 Loss = 0.07489196735340398\n",
            "Iteration 3112 Loss = 0.07487045064300386\n",
            "Iteration 3113 Loss = 0.07484894599711604\n",
            "Iteration 3114 Loss = 0.07482745340897609\n",
            "Iteration 3115 Loss = 0.074805972871823\n",
            "Iteration 3116 Loss = 0.07478450437889979\n",
            "Iteration 3117 Loss = 0.07476304792345322\n",
            "Iteration 3118 Loss = 0.07474160349873377\n",
            "Iteration 3119 Loss = 0.07472017109799556\n",
            "Iteration 3120 Loss = 0.07469875071449711\n",
            "Iteration 3121 Loss = 0.07467734234150013\n",
            "Iteration 3122 Loss = 0.0746559459722702\n",
            "Iteration 3123 Loss = 0.0746345616000768\n",
            "Iteration 3124 Loss = 0.07461318921819325\n",
            "Iteration 3125 Loss = 0.07459182881989643\n",
            "Iteration 3126 Loss = 0.074570480398467\n",
            "Iteration 3127 Loss = 0.07454914394718973\n",
            "Iteration 3128 Loss = 0.07452781945935262\n",
            "Iteration 3129 Loss = 0.07450650692824783\n",
            "Iteration 3130 Loss = 0.07448520634717128\n",
            "Iteration 3131 Loss = 0.0744639177094224\n",
            "Iteration 3132 Loss = 0.07444264100830444\n",
            "Iteration 3133 Loss = 0.07442137623712472\n",
            "Iteration 3134 Loss = 0.07440012338919388\n",
            "Iteration 3135 Loss = 0.0743788824578266\n",
            "Iteration 3136 Loss = 0.07435765343634114\n",
            "Iteration 3137 Loss = 0.07433643631805972\n",
            "Iteration 3138 Loss = 0.074315231096308\n",
            "Iteration 3139 Loss = 0.07429403776441562\n",
            "Iteration 3140 Loss = 0.07427285631571581\n",
            "Iteration 3141 Loss = 0.07425168674354572\n",
            "Iteration 3142 Loss = 0.07423052904124611\n",
            "Iteration 3143 Loss = 0.07420938320216154\n",
            "Iteration 3144 Loss = 0.07418824921964007\n",
            "Iteration 3145 Loss = 0.07416712708703374\n",
            "Iteration 3146 Loss = 0.07414601679769837\n",
            "Iteration 3147 Loss = 0.0741249183449933\n",
            "Iteration 3148 Loss = 0.0741038317222817\n",
            "Iteration 3149 Loss = 0.07408275692293038\n",
            "Iteration 3150 Loss = 0.07406169394031012\n",
            "Iteration 3151 Loss = 0.07404064276779505\n",
            "Iteration 3152 Loss = 0.07401960339876304\n",
            "Iteration 3153 Loss = 0.0739985758265962\n",
            "Iteration 3154 Loss = 0.07397756004467988\n",
            "Iteration 3155 Loss = 0.07395655604640301\n",
            "Iteration 3156 Loss = 0.07393556382515859\n",
            "Iteration 3157 Loss = 0.07391458337434324\n",
            "Iteration 3158 Loss = 0.07389361468735717\n",
            "Iteration 3159 Loss = 0.07387265775760428\n",
            "Iteration 3160 Loss = 0.07385171257849255\n",
            "Iteration 3161 Loss = 0.07383077914343295\n",
            "Iteration 3162 Loss = 0.0738098574458407\n",
            "Iteration 3163 Loss = 0.07378894747913452\n",
            "Iteration 3164 Loss = 0.07376804923673706\n",
            "Iteration 3165 Loss = 0.07374716271207413\n",
            "Iteration 3166 Loss = 0.07372628789857581\n",
            "Iteration 3167 Loss = 0.07370542478967537\n",
            "Iteration 3168 Loss = 0.07368457337881024\n",
            "Iteration 3169 Loss = 0.07366373365942103\n",
            "Iteration 3170 Loss = 0.07364290562495239\n",
            "Iteration 3171 Loss = 0.0736220892688526\n",
            "Iteration 3172 Loss = 0.0736012845845734\n",
            "Iteration 3173 Loss = 0.0735804915655704\n",
            "Iteration 3174 Loss = 0.07355971020530304\n",
            "Iteration 3175 Loss = 0.07353894049723393\n",
            "Iteration 3176 Loss = 0.07351818243482984\n",
            "Iteration 3177 Loss = 0.07349743601156064\n",
            "Iteration 3178 Loss = 0.07347670122090078\n",
            "Iteration 3179 Loss = 0.07345597805632749\n",
            "Iteration 3180 Loss = 0.07343526651132189\n",
            "Iteration 3181 Loss = 0.07341456657936889\n",
            "Iteration 3182 Loss = 0.07339387825395713\n",
            "Iteration 3183 Loss = 0.07337320152857879\n",
            "Iteration 3184 Loss = 0.07335253639672927\n",
            "Iteration 3185 Loss = 0.07333188285190854\n",
            "Iteration 3186 Loss = 0.07331124088761946\n",
            "Iteration 3187 Loss = 0.0732906104973689\n",
            "Iteration 3188 Loss = 0.07326999167466712\n",
            "Iteration 3189 Loss = 0.07324938441302815\n",
            "Iteration 3190 Loss = 0.0732287887059697\n",
            "Iteration 3191 Loss = 0.07320820454701313\n",
            "Iteration 3192 Loss = 0.07318763192968321\n",
            "Iteration 3193 Loss = 0.07316707084750865\n",
            "Iteration 3194 Loss = 0.07314652129402144\n",
            "Iteration 3195 Loss = 0.07312598326275765\n",
            "Iteration 3196 Loss = 0.07310545674725659\n",
            "Iteration 3197 Loss = 0.07308494174106138\n",
            "Iteration 3198 Loss = 0.07306443823771856\n",
            "Iteration 3199 Loss = 0.07304394623077859\n",
            "Iteration 3200 Loss = 0.07302346571379545\n",
            "Iteration 3201 Loss = 0.07300299668032621\n",
            "Iteration 3202 Loss = 0.07298253912393265\n",
            "Iteration 3203 Loss = 0.07296209303817915\n",
            "Iteration 3204 Loss = 0.07294165841663412\n",
            "Iteration 3205 Loss = 0.0729212352528695\n",
            "Iteration 3206 Loss = 0.07290082354046093\n",
            "Iteration 3207 Loss = 0.07288042327298762\n",
            "Iteration 3208 Loss = 0.07286003444403229\n",
            "Iteration 3209 Loss = 0.07283965704718127\n",
            "Iteration 3210 Loss = 0.07281929107602468\n",
            "Iteration 3211 Loss = 0.07279893652415591\n",
            "Iteration 3212 Loss = 0.07277859338517238\n",
            "Iteration 3213 Loss = 0.07275826165267443\n",
            "Iteration 3214 Loss = 0.07273794132026665\n",
            "Iteration 3215 Loss = 0.07271763238155697\n",
            "Iteration 3216 Loss = 0.07269733483015688\n",
            "Iteration 3217 Loss = 0.07267704865968143\n",
            "Iteration 3218 Loss = 0.07265677386374934\n",
            "Iteration 3219 Loss = 0.07263651043598263\n",
            "Iteration 3220 Loss = 0.07261625837000735\n",
            "Iteration 3221 Loss = 0.07259601765945287\n",
            "Iteration 3222 Loss = 0.07257578829795214\n",
            "Iteration 3223 Loss = 0.07255557027914171\n",
            "Iteration 3224 Loss = 0.07253536359666156\n",
            "Iteration 3225 Loss = 0.07251516824415553\n",
            "Iteration 3226 Loss = 0.07249498421527079\n",
            "Iteration 3227 Loss = 0.07247481150365813\n",
            "Iteration 3228 Loss = 0.07245465010297186\n",
            "Iteration 3229 Loss = 0.07243450000686996\n",
            "Iteration 3230 Loss = 0.07241436120901393\n",
            "Iteration 3231 Loss = 0.07239423370306868\n",
            "Iteration 3232 Loss = 0.07237411748270282\n",
            "Iteration 3233 Loss = 0.07235401254158864\n",
            "Iteration 3234 Loss = 0.07233391887340165\n",
            "Iteration 3235 Loss = 0.07231383647182114\n",
            "Iteration 3236 Loss = 0.0722937653305298\n",
            "Iteration 3237 Loss = 0.07227370544321388\n",
            "Iteration 3238 Loss = 0.07225365680356347\n",
            "Iteration 3239 Loss = 0.07223361940527169\n",
            "Iteration 3240 Loss = 0.07221359324203575\n",
            "Iteration 3241 Loss = 0.07219357830755588\n",
            "Iteration 3242 Loss = 0.07217357459553603\n",
            "Iteration 3243 Loss = 0.07215358209968385\n",
            "Iteration 3244 Loss = 0.07213360081371038\n",
            "Iteration 3245 Loss = 0.07211363073133016\n",
            "Iteration 3246 Loss = 0.07209367184626135\n",
            "Iteration 3247 Loss = 0.07207372415222546\n",
            "Iteration 3248 Loss = 0.07205378764294768\n",
            "Iteration 3249 Loss = 0.07203386231215657\n",
            "Iteration 3250 Loss = 0.07201394815358443\n",
            "Iteration 3251 Loss = 0.07199404516096705\n",
            "Iteration 3252 Loss = 0.07197415332804344\n",
            "Iteration 3253 Loss = 0.07195427264855633\n",
            "Iteration 3254 Loss = 0.07193440311625213\n",
            "Iteration 3255 Loss = 0.07191454472488026\n",
            "Iteration 3256 Loss = 0.07189469746819414\n",
            "Iteration 3257 Loss = 0.07187486133995057\n",
            "Iteration 3258 Loss = 0.07185503633390965\n",
            "Iteration 3259 Loss = 0.07183522244383504\n",
            "Iteration 3260 Loss = 0.07181541966349428\n",
            "Iteration 3261 Loss = 0.07179562798665773\n",
            "Iteration 3262 Loss = 0.07177584740709982\n",
            "Iteration 3263 Loss = 0.07175607791859825\n",
            "Iteration 3264 Loss = 0.07173631951493419\n",
            "Iteration 3265 Loss = 0.07171657218989226\n",
            "Iteration 3266 Loss = 0.07169683593726071\n",
            "Iteration 3267 Loss = 0.07167711075083108\n",
            "Iteration 3268 Loss = 0.07165739662439866\n",
            "Iteration 3269 Loss = 0.07163769355176204\n",
            "Iteration 3270 Loss = 0.0716180015267231\n",
            "Iteration 3271 Loss = 0.07159832054308765\n",
            "Iteration 3272 Loss = 0.07157865059466458\n",
            "Iteration 3273 Loss = 0.07155899167526669\n",
            "Iteration 3274 Loss = 0.07153934377870945\n",
            "Iteration 3275 Loss = 0.07151970689881276\n",
            "Iteration 3276 Loss = 0.0715000810293993\n",
            "Iteration 3277 Loss = 0.07148046616429544\n",
            "Iteration 3278 Loss = 0.07146086229733116\n",
            "Iteration 3279 Loss = 0.07144126942233958\n",
            "Iteration 3280 Loss = 0.07142168753315761\n",
            "Iteration 3281 Loss = 0.07140211662362547\n",
            "Iteration 3282 Loss = 0.07138255668758658\n",
            "Iteration 3283 Loss = 0.07136300771888827\n",
            "Iteration 3284 Loss = 0.07134346971138111\n",
            "Iteration 3285 Loss = 0.07132394265891902\n",
            "Iteration 3286 Loss = 0.07130442655535953\n",
            "Iteration 3287 Loss = 0.07128492139456345\n",
            "Iteration 3288 Loss = 0.07126542717039527\n",
            "Iteration 3289 Loss = 0.0712459438767226\n",
            "Iteration 3290 Loss = 0.07122647150741683\n",
            "Iteration 3291 Loss = 0.07120701005635242\n",
            "Iteration 3292 Loss = 0.07118755951740766\n",
            "Iteration 3293 Loss = 0.071168119884464\n",
            "Iteration 3294 Loss = 0.07114869115140635\n",
            "Iteration 3295 Loss = 0.0711292733121233\n",
            "Iteration 3296 Loss = 0.07110986636050638\n",
            "Iteration 3297 Loss = 0.07109047029045107\n",
            "Iteration 3298 Loss = 0.07107108509585588\n",
            "Iteration 3299 Loss = 0.07105171077062288\n",
            "Iteration 3300 Loss = 0.07103234730865764\n",
            "Iteration 3301 Loss = 0.0710129947038691\n",
            "Iteration 3302 Loss = 0.07099365295016954\n",
            "Iteration 3303 Loss = 0.07097432204147483\n",
            "Iteration 3304 Loss = 0.07095500197170394\n",
            "Iteration 3305 Loss = 0.07093569273477948\n",
            "Iteration 3306 Loss = 0.07091639432462758\n",
            "Iteration 3307 Loss = 0.07089710673517749\n",
            "Iteration 3308 Loss = 0.0708778299603619\n",
            "Iteration 3309 Loss = 0.07085856399411722\n",
            "Iteration 3310 Loss = 0.07083930883038299\n",
            "Iteration 3311 Loss = 0.07082006446310207\n",
            "Iteration 3312 Loss = 0.07080083088622093\n",
            "Iteration 3313 Loss = 0.07078160809368947\n",
            "Iteration 3314 Loss = 0.07076239607946044\n",
            "Iteration 3315 Loss = 0.07074319483749081\n",
            "Iteration 3316 Loss = 0.07072400436174031\n",
            "Iteration 3317 Loss = 0.07070482464617252\n",
            "Iteration 3318 Loss = 0.07068565568475389\n",
            "Iteration 3319 Loss = 0.07066649747145465\n",
            "Iteration 3320 Loss = 0.07064735000024829\n",
            "Iteration 3321 Loss = 0.07062821326511172\n",
            "Iteration 3322 Loss = 0.07060908726002504\n",
            "Iteration 3323 Loss = 0.07058997197897202\n",
            "Iteration 3324 Loss = 0.07057086741593942\n",
            "Iteration 3325 Loss = 0.07055177356491774\n",
            "Iteration 3326 Loss = 0.07053269041990073\n",
            "Iteration 3327 Loss = 0.07051361797488553\n",
            "Iteration 3328 Loss = 0.07049455622387261\n",
            "Iteration 3329 Loss = 0.07047550516086563\n",
            "Iteration 3330 Loss = 0.07045646477987175\n",
            "Iteration 3331 Loss = 0.07043743507490184\n",
            "Iteration 3332 Loss = 0.07041841603996953\n",
            "Iteration 3333 Loss = 0.07039940766909214\n",
            "Iteration 3334 Loss = 0.07038040995629037\n",
            "Iteration 3335 Loss = 0.0703614228955882\n",
            "Iteration 3336 Loss = 0.07034244648101261\n",
            "Iteration 3337 Loss = 0.0703234807065948\n",
            "Iteration 3338 Loss = 0.0703045255663684\n",
            "Iteration 3339 Loss = 0.07028558105437106\n",
            "Iteration 3340 Loss = 0.07026664716464323\n",
            "Iteration 3341 Loss = 0.07024772389122907\n",
            "Iteration 3342 Loss = 0.07022881122817586\n",
            "Iteration 3343 Loss = 0.07020990916953458\n",
            "Iteration 3344 Loss = 0.07019101770935915\n",
            "Iteration 3345 Loss = 0.07017213684170677\n",
            "Iteration 3346 Loss = 0.07015326656063846\n",
            "Iteration 3347 Loss = 0.07013440686021814\n",
            "Iteration 3348 Loss = 0.07011555773451324\n",
            "Iteration 3349 Loss = 0.07009671917759445\n",
            "Iteration 3350 Loss = 0.07007789118353584\n",
            "Iteration 3351 Loss = 0.07005907374641479\n",
            "Iteration 3352 Loss = 0.07004026686031181\n",
            "Iteration 3353 Loss = 0.0700214705193109\n",
            "Iteration 3354 Loss = 0.07000268471749967\n",
            "Iteration 3355 Loss = 0.06998390944896841\n",
            "Iteration 3356 Loss = 0.06996514470781133\n",
            "Iteration 3357 Loss = 0.06994639048812559\n",
            "Iteration 3358 Loss = 0.06992764678401164\n",
            "Iteration 3359 Loss = 0.06990891358957363\n",
            "Iteration 3360 Loss = 0.06989019089891843\n",
            "Iteration 3361 Loss = 0.0698714787061567\n",
            "Iteration 3362 Loss = 0.06985277700540223\n",
            "Iteration 3363 Loss = 0.06983408579077195\n",
            "Iteration 3364 Loss = 0.06981540505638645\n",
            "Iteration 3365 Loss = 0.06979673479636932\n",
            "Iteration 3366 Loss = 0.0697780750048475\n",
            "Iteration 3367 Loss = 0.06975942567595132\n",
            "Iteration 3368 Loss = 0.06974078680381435\n",
            "Iteration 3369 Loss = 0.06972215838257323\n",
            "Iteration 3370 Loss = 0.06970354040636843\n",
            "Iteration 3371 Loss = 0.0696849328693432\n",
            "Iteration 3372 Loss = 0.06966633576564436\n",
            "Iteration 3373 Loss = 0.06964774908942158\n",
            "Iteration 3374 Loss = 0.06962917283482854\n",
            "Iteration 3375 Loss = 0.06961060699602153\n",
            "Iteration 3376 Loss = 0.0695920515671603\n",
            "Iteration 3377 Loss = 0.06957350654240832\n",
            "Iteration 3378 Loss = 0.06955497191593162\n",
            "Iteration 3379 Loss = 0.0695364476818999\n",
            "Iteration 3380 Loss = 0.06951793383448612\n",
            "Iteration 3381 Loss = 0.06949943036786664\n",
            "Iteration 3382 Loss = 0.06948093727622054\n",
            "Iteration 3383 Loss = 0.06946245455373079\n",
            "Iteration 3384 Loss = 0.06944398219458339\n",
            "Iteration 3385 Loss = 0.06942552019296727\n",
            "Iteration 3386 Loss = 0.06940706854307524\n",
            "Iteration 3387 Loss = 0.06938862723910294\n",
            "Iteration 3388 Loss = 0.0693701962752492\n",
            "Iteration 3389 Loss = 0.0693517756457164\n",
            "Iteration 3390 Loss = 0.06933336534471018\n",
            "Iteration 3391 Loss = 0.0693149653664391\n",
            "Iteration 3392 Loss = 0.06929657570511531\n",
            "Iteration 3393 Loss = 0.06927819635495405\n",
            "Iteration 3394 Loss = 0.06925982731017345\n",
            "Iteration 3395 Loss = 0.06924146856499569\n",
            "Iteration 3396 Loss = 0.06922312011364555\n",
            "Iteration 3397 Loss = 0.06920478195035122\n",
            "Iteration 3398 Loss = 0.06918645406934416\n",
            "Iteration 3399 Loss = 0.0691681364648591\n",
            "Iteration 3400 Loss = 0.06914982913113375\n",
            "Iteration 3401 Loss = 0.06913153206240957\n",
            "Iteration 3402 Loss = 0.06911324525293078\n",
            "Iteration 3403 Loss = 0.06909496869694483\n",
            "Iteration 3404 Loss = 0.06907670238870284\n",
            "Iteration 3405 Loss = 0.06905844632245858\n",
            "Iteration 3406 Loss = 0.06904020049246935\n",
            "Iteration 3407 Loss = 0.06902196489299584\n",
            "Iteration 3408 Loss = 0.06900373951830166\n",
            "Iteration 3409 Loss = 0.06898552436265365\n",
            "Iteration 3410 Loss = 0.068967319420322\n",
            "Iteration 3411 Loss = 0.06894912468558005\n",
            "Iteration 3412 Loss = 0.06893094015270439\n",
            "Iteration 3413 Loss = 0.06891276581597475\n",
            "Iteration 3414 Loss = 0.06889460166967419\n",
            "Iteration 3415 Loss = 0.0688764477080887\n",
            "Iteration 3416 Loss = 0.06885830392550792\n",
            "Iteration 3417 Loss = 0.06884017031622433\n",
            "Iteration 3418 Loss = 0.06882204687453361\n",
            "Iteration 3419 Loss = 0.06880393359473488\n",
            "Iteration 3420 Loss = 0.06878583047113032\n",
            "Iteration 3421 Loss = 0.06876773749802528\n",
            "Iteration 3422 Loss = 0.0687496546697283\n",
            "Iteration 3423 Loss = 0.06873158198055111\n",
            "Iteration 3424 Loss = 0.06871351942480877\n",
            "Iteration 3425 Loss = 0.06869546699681935\n",
            "Iteration 3426 Loss = 0.06867742469090414\n",
            "Iteration 3427 Loss = 0.06865939250138788\n",
            "Iteration 3428 Loss = 0.06864137042259785\n",
            "Iteration 3429 Loss = 0.0686233584488653\n",
            "Iteration 3430 Loss = 0.06860535657452407\n",
            "Iteration 3431 Loss = 0.06858736479391149\n",
            "Iteration 3432 Loss = 0.06856938310136791\n",
            "Iteration 3433 Loss = 0.06855141149123696\n",
            "Iteration 3434 Loss = 0.0685334499578653\n",
            "Iteration 3435 Loss = 0.06851549849560283\n",
            "Iteration 3436 Loss = 0.06849755709880284\n",
            "Iteration 3437 Loss = 0.06847962576182141\n",
            "Iteration 3438 Loss = 0.06846170447901793\n",
            "Iteration 3439 Loss = 0.06844379324475514\n",
            "Iteration 3440 Loss = 0.06842589205339858\n",
            "Iteration 3441 Loss = 0.06840800089931726\n",
            "Iteration 3442 Loss = 0.06839011977688329\n",
            "Iteration 3443 Loss = 0.06837224868047186\n",
            "Iteration 3444 Loss = 0.06835438760446111\n",
            "Iteration 3445 Loss = 0.06833653654323286\n",
            "Iteration 3446 Loss = 0.06831869549117167\n",
            "Iteration 3447 Loss = 0.0683008644426653\n",
            "Iteration 3448 Loss = 0.06828304339210484\n",
            "Iteration 3449 Loss = 0.06826523233388422\n",
            "Iteration 3450 Loss = 0.0682474312624009\n",
            "Iteration 3451 Loss = 0.068229640172055\n",
            "Iteration 3452 Loss = 0.06821185905725048\n",
            "Iteration 3453 Loss = 0.06819408791239345\n",
            "Iteration 3454 Loss = 0.06817632673189418\n",
            "Iteration 3455 Loss = 0.06815857551016542\n",
            "Iteration 3456 Loss = 0.06814083424162319\n",
            "Iteration 3457 Loss = 0.06812310292068685\n",
            "Iteration 3458 Loss = 0.06810538154177864\n",
            "Iteration 3459 Loss = 0.06808767009932412\n",
            "Iteration 3460 Loss = 0.0680699685877518\n",
            "Iteration 3461 Loss = 0.06805227700149334\n",
            "Iteration 3462 Loss = 0.06803459533498354\n",
            "Iteration 3463 Loss = 0.06801692358266062\n",
            "Iteration 3464 Loss = 0.06799926173896532\n",
            "Iteration 3465 Loss = 0.06798160979834214\n",
            "Iteration 3466 Loss = 0.06796396775523808\n",
            "Iteration 3467 Loss = 0.06794633560410374\n",
            "Iteration 3468 Loss = 0.06792871333939265\n",
            "Iteration 3469 Loss = 0.06791110095556135\n",
            "Iteration 3470 Loss = 0.06789349844706974\n",
            "Iteration 3471 Loss = 0.0678759058083806\n",
            "Iteration 3472 Loss = 0.06785832303395974\n",
            "Iteration 3473 Loss = 0.06784075011827649\n",
            "Iteration 3474 Loss = 0.06782318705580288\n",
            "Iteration 3475 Loss = 0.0678056338410141\n",
            "Iteration 3476 Loss = 0.06778809046838855\n",
            "Iteration 3477 Loss = 0.06777055693240785\n",
            "Iteration 3478 Loss = 0.0677530332275564\n",
            "Iteration 3479 Loss = 0.06773551934832174\n",
            "Iteration 3480 Loss = 0.06771801528919495\n",
            "Iteration 3481 Loss = 0.06770052104466948\n",
            "Iteration 3482 Loss = 0.06768303660924244\n",
            "Iteration 3483 Loss = 0.06766556197741376\n",
            "Iteration 3484 Loss = 0.06764809714368651\n",
            "Iteration 3485 Loss = 0.06763064210256693\n",
            "Iteration 3486 Loss = 0.06761319684856419\n",
            "Iteration 3487 Loss = 0.06759576137619062\n",
            "Iteration 3488 Loss = 0.06757833567996173\n",
            "Iteration 3489 Loss = 0.06756091975439575\n",
            "Iteration 3490 Loss = 0.06754351359401445\n",
            "Iteration 3491 Loss = 0.0675261171933424\n",
            "Iteration 3492 Loss = 0.0675087305469073\n",
            "Iteration 3493 Loss = 0.06749135364923986\n",
            "Iteration 3494 Loss = 0.06747398649487377\n",
            "Iteration 3495 Loss = 0.06745662907834624\n",
            "Iteration 3496 Loss = 0.06743928139419704\n",
            "Iteration 3497 Loss = 0.06742194343696917\n",
            "Iteration 3498 Loss = 0.06740461520120855\n",
            "Iteration 3499 Loss = 0.06738729668146463\n",
            "Iteration 3500 Loss = 0.0673699878722894\n",
            "Iteration 3501 Loss = 0.0673526887682383\n",
            "Iteration 3502 Loss = 0.06733539936386937\n",
            "Iteration 3503 Loss = 0.06731811965374404\n",
            "Iteration 3504 Loss = 0.0673008496324268\n",
            "Iteration 3505 Loss = 0.06728358929448493\n",
            "Iteration 3506 Loss = 0.06726633863448915\n",
            "Iteration 3507 Loss = 0.06724909764701269\n",
            "Iteration 3508 Loss = 0.06723186632663254\n",
            "Iteration 3509 Loss = 0.06721464466792786\n",
            "Iteration 3510 Loss = 0.0671974326654815\n",
            "Iteration 3511 Loss = 0.06718023031387935\n",
            "Iteration 3512 Loss = 0.0671630376077099\n",
            "Iteration 3513 Loss = 0.06714585454156478\n",
            "Iteration 3514 Loss = 0.0671286811100392\n",
            "Iteration 3515 Loss = 0.06711151730773067\n",
            "Iteration 3516 Loss = 0.06709436312924023\n",
            "Iteration 3517 Loss = 0.06707721856917162\n",
            "Iteration 3518 Loss = 0.06706008362213184\n",
            "Iteration 3519 Loss = 0.06704295828273077\n",
            "Iteration 3520 Loss = 0.06702584254558143\n",
            "Iteration 3521 Loss = 0.06700873640529963\n",
            "Iteration 3522 Loss = 0.06699163985650469\n",
            "Iteration 3523 Loss = 0.06697455289381833\n",
            "Iteration 3524 Loss = 0.06695747551186561\n",
            "Iteration 3525 Loss = 0.06694040770527472\n",
            "Iteration 3526 Loss = 0.06692334946867666\n",
            "Iteration 3527 Loss = 0.06690630079670537\n",
            "Iteration 3528 Loss = 0.0668892616839981\n",
            "Iteration 3529 Loss = 0.06687223212519461\n",
            "Iteration 3530 Loss = 0.06685521211493842\n",
            "Iteration 3531 Loss = 0.06683820164787542\n",
            "Iteration 3532 Loss = 0.06682120071865458\n",
            "Iteration 3533 Loss = 0.06680420932192813\n",
            "Iteration 3534 Loss = 0.06678722745235113\n",
            "Iteration 3535 Loss = 0.06677025510458177\n",
            "Iteration 3536 Loss = 0.06675329227328097\n",
            "Iteration 3537 Loss = 0.06673633895311289\n",
            "Iteration 3538 Loss = 0.06671939513874457\n",
            "Iteration 3539 Loss = 0.06670246082484599\n",
            "Iteration 3540 Loss = 0.06668553600609044\n",
            "Iteration 3541 Loss = 0.06666862067715364\n",
            "Iteration 3542 Loss = 0.06665171483271479\n",
            "Iteration 3543 Loss = 0.06663481846745585\n",
            "Iteration 3544 Loss = 0.06661793157606187\n",
            "Iteration 3545 Loss = 0.06660105415322073\n",
            "Iteration 3546 Loss = 0.06658418619362336\n",
            "Iteration 3547 Loss = 0.0665673276919637\n",
            "Iteration 3548 Loss = 0.0665504786429388\n",
            "Iteration 3549 Loss = 0.0665336390412482\n",
            "Iteration 3550 Loss = 0.0665168088815951\n",
            "Iteration 3551 Loss = 0.06649998815868495\n",
            "Iteration 3552 Loss = 0.06648317686722689\n",
            "Iteration 3553 Loss = 0.06646637500193239\n",
            "Iteration 3554 Loss = 0.06644958255751614\n",
            "Iteration 3555 Loss = 0.06643279952869612\n",
            "Iteration 3556 Loss = 0.06641602591019267\n",
            "Iteration 3557 Loss = 0.06639926169672954\n",
            "Iteration 3558 Loss = 0.0663825068830332\n",
            "Iteration 3559 Loss = 0.0663657614638331\n",
            "Iteration 3560 Loss = 0.066349025433862\n",
            "Iteration 3561 Loss = 0.06633229878785495\n",
            "Iteration 3562 Loss = 0.06631558152055053\n",
            "Iteration 3563 Loss = 0.06629887362669007\n",
            "Iteration 3564 Loss = 0.06628217510101767\n",
            "Iteration 3565 Loss = 0.0662654859382807\n",
            "Iteration 3566 Loss = 0.0662488061332292\n",
            "Iteration 3567 Loss = 0.06623213568061659\n",
            "Iteration 3568 Loss = 0.06621547457519851\n",
            "Iteration 3569 Loss = 0.06619882281173414\n",
            "Iteration 3570 Loss = 0.06618218038498545\n",
            "Iteration 3571 Loss = 0.06616554728971712\n",
            "Iteration 3572 Loss = 0.0661489235206972\n",
            "Iteration 3573 Loss = 0.06613230907269617\n",
            "Iteration 3574 Loss = 0.06611570394048792\n",
            "Iteration 3575 Loss = 0.06609910811884893\n",
            "Iteration 3576 Loss = 0.06608252160255869\n",
            "Iteration 3577 Loss = 0.06606594438639983\n",
            "Iteration 3578 Loss = 0.0660493764651575\n",
            "Iteration 3579 Loss = 0.06603281783361996\n",
            "Iteration 3580 Loss = 0.06601626848657866\n",
            "Iteration 3581 Loss = 0.06599972841882767\n",
            "Iteration 3582 Loss = 0.0659831976251641\n",
            "Iteration 3583 Loss = 0.06596667610038787\n",
            "Iteration 3584 Loss = 0.0659501638393018\n",
            "Iteration 3585 Loss = 0.06593366083671176\n",
            "Iteration 3586 Loss = 0.06591716708742637\n",
            "Iteration 3587 Loss = 0.06590068258625752\n",
            "Iteration 3588 Loss = 0.06588420732801954\n",
            "Iteration 3589 Loss = 0.06586774130752995\n",
            "Iteration 3590 Loss = 0.06585128451960907\n",
            "Iteration 3591 Loss = 0.06583483695908017\n",
            "Iteration 3592 Loss = 0.06581839862076949\n",
            "Iteration 3593 Loss = 0.06580196949950601\n",
            "Iteration 3594 Loss = 0.06578554959012166\n",
            "Iteration 3595 Loss = 0.06576913888745135\n",
            "Iteration 3596 Loss = 0.0657527373863328\n",
            "Iteration 3597 Loss = 0.06573634508160679\n",
            "Iteration 3598 Loss = 0.0657199619681168\n",
            "Iteration 3599 Loss = 0.06570358804070907\n",
            "Iteration 3600 Loss = 0.06568722329423321\n",
            "Iteration 3601 Loss = 0.06567086772354123\n",
            "Iteration 3602 Loss = 0.06565452132348824\n",
            "Iteration 3603 Loss = 0.06563818408893252\n",
            "Iteration 3604 Loss = 0.06562185601473444\n",
            "Iteration 3605 Loss = 0.06560553709575816\n",
            "Iteration 3606 Loss = 0.06558922732687021\n",
            "Iteration 3607 Loss = 0.06557292670293986\n",
            "Iteration 3608 Loss = 0.06555663521883988\n",
            "Iteration 3609 Loss = 0.0655403528694452\n",
            "Iteration 3610 Loss = 0.06552407964963411\n",
            "Iteration 3611 Loss = 0.0655078155542876\n",
            "Iteration 3612 Loss = 0.06549156057828946\n",
            "Iteration 3613 Loss = 0.06547531471652653\n",
            "Iteration 3614 Loss = 0.06545907796388842\n",
            "Iteration 3615 Loss = 0.06544285031526752\n",
            "Iteration 3616 Loss = 0.06542663176555925\n",
            "Iteration 3617 Loss = 0.06541042230966171\n",
            "Iteration 3618 Loss = 0.06539422194247616\n",
            "Iteration 3619 Loss = 0.06537803065890621\n",
            "Iteration 3620 Loss = 0.06536184845385898\n",
            "Iteration 3621 Loss = 0.0653456753222438\n",
            "Iteration 3622 Loss = 0.06532951125897339\n",
            "Iteration 3623 Loss = 0.06531335625896299\n",
            "Iteration 3624 Loss = 0.06529721031713075\n",
            "Iteration 3625 Loss = 0.06528107342839788\n",
            "Iteration 3626 Loss = 0.06526494558768799\n",
            "Iteration 3627 Loss = 0.06524882678992797\n",
            "Iteration 3628 Loss = 0.06523271703004745\n",
            "Iteration 3629 Loss = 0.06521661630297891\n",
            "Iteration 3630 Loss = 0.0652005246036574\n",
            "Iteration 3631 Loss = 0.0651844419270212\n",
            "Iteration 3632 Loss = 0.06516836826801099\n",
            "Iteration 3633 Loss = 0.06515230362157091\n",
            "Iteration 3634 Loss = 0.06513624798264742\n",
            "Iteration 3635 Loss = 0.06512020134618977\n",
            "Iteration 3636 Loss = 0.06510416370715057\n",
            "Iteration 3637 Loss = 0.06508813506048469\n",
            "Iteration 3638 Loss = 0.06507211540115028\n",
            "Iteration 3639 Loss = 0.06505610472410775\n",
            "Iteration 3640 Loss = 0.06504010302432114\n",
            "Iteration 3641 Loss = 0.0650241102967565\n",
            "Iteration 3642 Loss = 0.06500812653638333\n",
            "Iteration 3643 Loss = 0.0649921517381734\n",
            "Iteration 3644 Loss = 0.06497618589710183\n",
            "Iteration 3645 Loss = 0.06496022900814637\n",
            "Iteration 3646 Loss = 0.06494428106628719\n",
            "Iteration 3647 Loss = 0.06492834206650791\n",
            "Iteration 3648 Loss = 0.06491241200379454\n",
            "Iteration 3649 Loss = 0.06489649087313602\n",
            "Iteration 3650 Loss = 0.06488057866952422\n",
            "Iteration 3651 Loss = 0.06486467538795367\n",
            "Iteration 3652 Loss = 0.06484878102342166\n",
            "Iteration 3653 Loss = 0.06483289557092844\n",
            "Iteration 3654 Loss = 0.06481701902547708\n",
            "Iteration 3655 Loss = 0.06480115138207307\n",
            "Iteration 3656 Loss = 0.06478529263572524\n",
            "Iteration 3657 Loss = 0.06476944278144485\n",
            "Iteration 3658 Loss = 0.06475360181424623\n",
            "Iteration 3659 Loss = 0.06473776972914629\n",
            "Iteration 3660 Loss = 0.0647219465211647\n",
            "Iteration 3661 Loss = 0.06470613218532414\n",
            "Iteration 3662 Loss = 0.0646903267166499\n",
            "Iteration 3663 Loss = 0.06467453011017017\n",
            "Iteration 3664 Loss = 0.06465874236091576\n",
            "Iteration 3665 Loss = 0.0646429634639206\n",
            "Iteration 3666 Loss = 0.06462719341422085\n",
            "Iteration 3667 Loss = 0.06461143220685625\n",
            "Iteration 3668 Loss = 0.0645956798368685\n",
            "Iteration 3669 Loss = 0.06457993629930255\n",
            "Iteration 3670 Loss = 0.06456420158920606\n",
            "Iteration 3671 Loss = 0.06454847570162942\n",
            "Iteration 3672 Loss = 0.06453275863162586\n",
            "Iteration 3673 Loss = 0.06451705037425114\n",
            "Iteration 3674 Loss = 0.06450135092456406\n",
            "Iteration 3675 Loss = 0.06448566027762627\n",
            "Iteration 3676 Loss = 0.06446997842850191\n",
            "Iteration 3677 Loss = 0.064454305372258\n",
            "Iteration 3678 Loss = 0.06443864110396438\n",
            "Iteration 3679 Loss = 0.06442298561869356\n",
            "Iteration 3680 Loss = 0.06440733891152092\n",
            "Iteration 3681 Loss = 0.06439170097752442\n",
            "Iteration 3682 Loss = 0.06437607181178504\n",
            "Iteration 3683 Loss = 0.06436045140938638\n",
            "Iteration 3684 Loss = 0.06434483976541458\n",
            "Iteration 3685 Loss = 0.06432923687495902\n",
            "Iteration 3686 Loss = 0.0643136427331114\n",
            "Iteration 3687 Loss = 0.06429805733496634\n",
            "Iteration 3688 Loss = 0.06428248067562137\n",
            "Iteration 3689 Loss = 0.06426691275017651\n",
            "Iteration 3690 Loss = 0.06425135355373458\n",
            "Iteration 3691 Loss = 0.06423580308140124\n",
            "Iteration 3692 Loss = 0.06422026132828472\n",
            "Iteration 3693 Loss = 0.06420472828949636\n",
            "Iteration 3694 Loss = 0.06418920396014986\n",
            "Iteration 3695 Loss = 0.06417368833536186\n",
            "Iteration 3696 Loss = 0.06415818141025145\n",
            "Iteration 3697 Loss = 0.06414268317994092\n",
            "Iteration 3698 Loss = 0.06412719363955503\n",
            "Iteration 3699 Loss = 0.0641117127842212\n",
            "Iteration 3700 Loss = 0.06409624060906985\n",
            "Iteration 3701 Loss = 0.06408077710923388\n",
            "Iteration 3702 Loss = 0.06406532227984887\n",
            "Iteration 3703 Loss = 0.06404987611605344\n",
            "Iteration 3704 Loss = 0.06403443861298863\n",
            "Iteration 3705 Loss = 0.0640190097657984\n",
            "Iteration 3706 Loss = 0.06400358956962945\n",
            "Iteration 3707 Loss = 0.06398817801963093\n",
            "Iteration 3708 Loss = 0.06397277511095499\n",
            "Iteration 3709 Loss = 0.06395738083875648\n",
            "Iteration 3710 Loss = 0.06394199519819284\n",
            "Iteration 3711 Loss = 0.06392661818442417\n",
            "Iteration 3712 Loss = 0.0639112497926134\n",
            "Iteration 3713 Loss = 0.06389589001792627\n",
            "Iteration 3714 Loss = 0.06388053885553112\n",
            "Iteration 3715 Loss = 0.0638651963005989\n",
            "Iteration 3716 Loss = 0.06384986234830357\n",
            "Iteration 3717 Loss = 0.06383453699382129\n",
            "Iteration 3718 Loss = 0.06381922023233152\n",
            "Iteration 3719 Loss = 0.06380391205901592\n",
            "Iteration 3720 Loss = 0.06378861246905919\n",
            "Iteration 3721 Loss = 0.0637733214576485\n",
            "Iteration 3722 Loss = 0.06375803901997389\n",
            "Iteration 3723 Loss = 0.06374276515122804\n",
            "Iteration 3724 Loss = 0.06372749984660633\n",
            "Iteration 3725 Loss = 0.06371224310130677\n",
            "Iteration 3726 Loss = 0.06369699491053012\n",
            "Iteration 3727 Loss = 0.06368175526947979\n",
            "Iteration 3728 Loss = 0.06366652417336198\n",
            "Iteration 3729 Loss = 0.06365130161738539\n",
            "Iteration 3730 Loss = 0.06363608759676179\n",
            "Iteration 3731 Loss = 0.06362088210670519\n",
            "Iteration 3732 Loss = 0.06360568514243224\n",
            "Iteration 3733 Loss = 0.06359049669916293\n",
            "Iteration 3734 Loss = 0.06357531677211913\n",
            "Iteration 3735 Loss = 0.06356014535652599\n",
            "Iteration 3736 Loss = 0.0635449824476111\n",
            "Iteration 3737 Loss = 0.06352982804060454\n",
            "Iteration 3738 Loss = 0.06351468213073942\n",
            "Iteration 3739 Loss = 0.06349954471325144\n",
            "Iteration 3740 Loss = 0.06348441578337864\n",
            "Iteration 3741 Loss = 0.06346929533636207\n",
            "Iteration 3742 Loss = 0.06345418336744538\n",
            "Iteration 3743 Loss = 0.06343907987187491\n",
            "Iteration 3744 Loss = 0.0634239848448996\n",
            "Iteration 3745 Loss = 0.06340889828177097\n",
            "Iteration 3746 Loss = 0.06339382017774342\n",
            "Iteration 3747 Loss = 0.06337875052807389\n",
            "Iteration 3748 Loss = 0.06336368932802189\n",
            "Iteration 3749 Loss = 0.06334863657284971\n",
            "Iteration 3750 Loss = 0.06333359225782237\n",
            "Iteration 3751 Loss = 0.06331855637820734\n",
            "Iteration 3752 Loss = 0.0633035289292749\n",
            "Iteration 3753 Loss = 0.06328850990629784\n",
            "Iteration 3754 Loss = 0.06327349930455183\n",
            "Iteration 3755 Loss = 0.06325849711931499\n",
            "Iteration 3756 Loss = 0.06324350334586813\n",
            "Iteration 3757 Loss = 0.06322851797949469\n",
            "Iteration 3758 Loss = 0.06321354101548096\n",
            "Iteration 3759 Loss = 0.06319857244911554\n",
            "Iteration 3760 Loss = 0.06318361227568976\n",
            "Iteration 3761 Loss = 0.06316866049049782\n",
            "Iteration 3762 Loss = 0.06315371708883633\n",
            "Iteration 3763 Loss = 0.06313878206600457\n",
            "Iteration 3764 Loss = 0.06312385541730459\n",
            "Iteration 3765 Loss = 0.06310893713804105\n",
            "Iteration 3766 Loss = 0.0630940272235209\n",
            "Iteration 3767 Loss = 0.06307912566905434\n",
            "Iteration 3768 Loss = 0.06306423246995334\n",
            "Iteration 3769 Loss = 0.06304934762153366\n",
            "Iteration 3770 Loss = 0.06303447111911255\n",
            "Iteration 3771 Loss = 0.06301960295801058\n",
            "Iteration 3772 Loss = 0.06300474313355067\n",
            "Iteration 3773 Loss = 0.06298989164105843\n",
            "Iteration 3774 Loss = 0.06297504847586224\n",
            "Iteration 3775 Loss = 0.06296021363329259\n",
            "Iteration 3776 Loss = 0.06294538710868343\n",
            "Iteration 3777 Loss = 0.06293056889737035\n",
            "Iteration 3778 Loss = 0.06291575899469236\n",
            "Iteration 3779 Loss = 0.06290095739599075\n",
            "Iteration 3780 Loss = 0.06288616409660937\n",
            "Iteration 3781 Loss = 0.0628713790918949\n",
            "Iteration 3782 Loss = 0.06285660237719615\n",
            "Iteration 3783 Loss = 0.06284183394786517\n",
            "Iteration 3784 Loss = 0.06282707379925624\n",
            "Iteration 3785 Loss = 0.0628123219267263\n",
            "Iteration 3786 Loss = 0.062797578325635\n",
            "Iteration 3787 Loss = 0.06278284299134443\n",
            "Iteration 3788 Loss = 0.06276811591921935\n",
            "Iteration 3789 Loss = 0.06275339710462717\n",
            "Iteration 3790 Loss = 0.06273868654293782\n",
            "Iteration 3791 Loss = 0.0627239842295238\n",
            "Iteration 3792 Loss = 0.06270929015976041\n",
            "Iteration 3793 Loss = 0.06269460432902535\n",
            "Iteration 3794 Loss = 0.06267992673269898\n",
            "Iteration 3795 Loss = 0.06266525736616409\n",
            "Iteration 3796 Loss = 0.06265059622480638\n",
            "Iteration 3797 Loss = 0.06263594330401392\n",
            "Iteration 3798 Loss = 0.06262129859917726\n",
            "Iteration 3799 Loss = 0.0626066621056899\n",
            "Iteration 3800 Loss = 0.06259203381894743\n",
            "Iteration 3801 Loss = 0.0625774137343486\n",
            "Iteration 3802 Loss = 0.06256280184729424\n",
            "Iteration 3803 Loss = 0.06254819815318796\n",
            "Iteration 3804 Loss = 0.06253360264743607\n",
            "Iteration 3805 Loss = 0.0625190153254472\n",
            "Iteration 3806 Loss = 0.0625044361826326\n",
            "Iteration 3807 Loss = 0.06248986521440635\n",
            "Iteration 3808 Loss = 0.06247530241618482\n",
            "Iteration 3809 Loss = 0.06246074778338712\n",
            "Iteration 3810 Loss = 0.062446201311434836\n",
            "Iteration 3811 Loss = 0.062431662995752195\n",
            "Iteration 3812 Loss = 0.06241713283176578\n",
            "Iteration 3813 Loss = 0.06240261081490507\n",
            "Iteration 3814 Loss = 0.06238809694060169\n",
            "Iteration 3815 Loss = 0.06237359120429046\n",
            "Iteration 3816 Loss = 0.062359093601408136\n",
            "Iteration 3817 Loss = 0.062344604127394204\n",
            "Iteration 3818 Loss = 0.062330122777691\n",
            "Iteration 3819 Loss = 0.062315649547742866\n",
            "Iteration 3820 Loss = 0.062301184432997356\n",
            "Iteration 3821 Loss = 0.062286727428904114\n",
            "Iteration 3822 Loss = 0.062272278530915416\n",
            "Iteration 3823 Loss = 0.06225783773448612\n",
            "Iteration 3824 Loss = 0.062243405035073744\n",
            "Iteration 3825 Loss = 0.06222898042813808\n",
            "Iteration 3826 Loss = 0.06221456390914186\n",
            "Iteration 3827 Loss = 0.06220015547355\n",
            "Iteration 3828 Loss = 0.062185755116830166\n",
            "Iteration 3829 Loss = 0.06217136283445258\n",
            "Iteration 3830 Loss = 0.06215697862188966\n",
            "Iteration 3831 Loss = 0.062142602474616865\n",
            "Iteration 3832 Loss = 0.062128234388111755\n",
            "Iteration 3833 Loss = 0.06211387435785488\n",
            "Iteration 3834 Loss = 0.06209952237932882\n",
            "Iteration 3835 Loss = 0.062085178448019133\n",
            "Iteration 3836 Loss = 0.062070842559413635\n",
            "Iteration 3837 Loss = 0.06205651470900261\n",
            "Iteration 3838 Loss = 0.06204219489227927\n",
            "Iteration 3839 Loss = 0.06202788310473884\n",
            "Iteration 3840 Loss = 0.06201357934187949\n",
            "Iteration 3841 Loss = 0.061999283599201656\n",
            "Iteration 3842 Loss = 0.061984995872208404\n",
            "Iteration 3843 Loss = 0.06197071615640539\n",
            "Iteration 3844 Loss = 0.06195644444730048\n",
            "Iteration 3845 Loss = 0.06194218074040463\n",
            "Iteration 3846 Loss = 0.0619279250312307\n",
            "Iteration 3847 Loss = 0.06191367731529446\n",
            "Iteration 3848 Loss = 0.06189943758811392\n",
            "Iteration 3849 Loss = 0.06188520584520997\n",
            "Iteration 3850 Loss = 0.061870982082105536\n",
            "Iteration 3851 Loss = 0.06185676629432654\n",
            "Iteration 3852 Loss = 0.06184255847740107\n",
            "Iteration 3853 Loss = 0.06182835862685985\n",
            "Iteration 3854 Loss = 0.06181416673823608\n",
            "Iteration 3855 Loss = 0.06179998280706551\n",
            "Iteration 3856 Loss = 0.061785806828886285\n",
            "Iteration 3857 Loss = 0.06177163879923922\n",
            "Iteration 3858 Loss = 0.061757478713667525\n",
            "Iteration 3859 Loss = 0.0617433265677169\n",
            "Iteration 3860 Loss = 0.0617291823569356\n",
            "Iteration 3861 Loss = 0.06171504607687421\n",
            "Iteration 3862 Loss = 0.06170091772308614\n",
            "Iteration 3863 Loss = 0.061686797291126964\n",
            "Iteration 3864 Loss = 0.061672684776555\n",
            "Iteration 3865 Loss = 0.061658580174930644\n",
            "Iteration 3866 Loss = 0.06164448348181743\n",
            "Iteration 3867 Loss = 0.06163039469278081\n",
            "Iteration 3868 Loss = 0.06161631380338895\n",
            "Iteration 3869 Loss = 0.0616022408092126\n",
            "Iteration 3870 Loss = 0.06158817570582477\n",
            "Iteration 3871 Loss = 0.061574118488801066\n",
            "Iteration 3872 Loss = 0.06156006915371969\n",
            "Iteration 3873 Loss = 0.06154602769616097\n",
            "Iteration 3874 Loss = 0.06153199411170832\n",
            "Iteration 3875 Loss = 0.06151796839594676\n",
            "Iteration 3876 Loss = 0.061503950544464725\n",
            "Iteration 3877 Loss = 0.061489940552852305\n",
            "Iteration 3878 Loss = 0.061475938416702826\n",
            "Iteration 3879 Loss = 0.061461944131611365\n",
            "Iteration 3880 Loss = 0.061447957693176035\n",
            "Iteration 3881 Loss = 0.06143397909699708\n",
            "Iteration 3882 Loss = 0.06142000833867719\n",
            "Iteration 3883 Loss = 0.061406045413821816\n",
            "Iteration 3884 Loss = 0.061392090318038765\n",
            "Iteration 3885 Loss = 0.06137814304693808\n",
            "Iteration 3886 Loss = 0.06136420359613234\n",
            "Iteration 3887 Loss = 0.06135027196123696\n",
            "Iteration 3888 Loss = 0.061336348137869284\n",
            "Iteration 3889 Loss = 0.06132243212164957\n",
            "Iteration 3890 Loss = 0.061308523908200095\n",
            "Iteration 3891 Loss = 0.06129462349314597\n",
            "Iteration 3892 Loss = 0.06128073087211448\n",
            "Iteration 3893 Loss = 0.0612668460407355\n",
            "Iteration 3894 Loss = 0.06125296899464152\n",
            "Iteration 3895 Loss = 0.061239099729467006\n",
            "Iteration 3896 Loss = 0.061225238240849356\n",
            "Iteration 3897 Loss = 0.0612113845244283\n",
            "Iteration 3898 Loss = 0.06119753857584562\n",
            "Iteration 3899 Loss = 0.06118370039074626\n",
            "Iteration 3900 Loss = 0.061169869964776795\n",
            "Iteration 3901 Loss = 0.06115604729358687\n",
            "Iteration 3902 Loss = 0.061142232372828495\n",
            "Iteration 3903 Loss = 0.061128425198155624\n",
            "Iteration 3904 Loss = 0.06111462576522522\n",
            "Iteration 3905 Loss = 0.061100834069696394\n",
            "Iteration 3906 Loss = 0.06108705010723071\n",
            "Iteration 3907 Loss = 0.06107327387349234\n",
            "Iteration 3908 Loss = 0.061059505364147566\n",
            "Iteration 3909 Loss = 0.061045744574865474\n",
            "Iteration 3910 Loss = 0.061031991501317266\n",
            "Iteration 3911 Loss = 0.06101824613917667\n",
            "Iteration 3912 Loss = 0.06100450848412007\n",
            "Iteration 3913 Loss = 0.06099077853182577\n",
            "Iteration 3914 Loss = 0.06097705627797514\n",
            "Iteration 3915 Loss = 0.0609633417182513\n",
            "Iteration 3916 Loss = 0.0609496348483404\n",
            "Iteration 3917 Loss = 0.06093593566393055\n",
            "Iteration 3918 Loss = 0.060922244160712395\n",
            "Iteration 3919 Loss = 0.06090856033437938\n",
            "Iteration 3920 Loss = 0.060894884180626586\n",
            "Iteration 3921 Loss = 0.060881215695152466\n",
            "Iteration 3922 Loss = 0.060867554873656846\n",
            "Iteration 3923 Loss = 0.06085390171184295\n",
            "Iteration 3924 Loss = 0.06084025620541577\n",
            "Iteration 3925 Loss = 0.060826618350082885\n",
            "Iteration 3926 Loss = 0.06081298814155433\n",
            "Iteration 3927 Loss = 0.06079936557554261\n",
            "Iteration 3928 Loss = 0.06078575064776231\n",
            "Iteration 3929 Loss = 0.06077214335393072\n",
            "Iteration 3930 Loss = 0.060758543689767575\n",
            "Iteration 3931 Loss = 0.0607449516509948\n",
            "Iteration 3932 Loss = 0.06073136723333675\n",
            "Iteration 3933 Loss = 0.0607177904325205\n",
            "Iteration 3934 Loss = 0.0607042212442748\n",
            "Iteration 3935 Loss = 0.06069065966433158\n",
            "Iteration 3936 Loss = 0.060677105688424604\n",
            "Iteration 3937 Loss = 0.06066355931229057\n",
            "Iteration 3938 Loss = 0.060650020531668146\n",
            "Iteration 3939 Loss = 0.06063648934229825\n",
            "Iteration 3940 Loss = 0.060622965739924636\n",
            "Iteration 3941 Loss = 0.060609449720293355\n",
            "Iteration 3942 Loss = 0.0605959412791525\n",
            "Iteration 3943 Loss = 0.06058244041225284\n",
            "Iteration 3944 Loss = 0.060568947115347556\n",
            "Iteration 3945 Loss = 0.0605554613841921\n",
            "Iteration 3946 Loss = 0.06054198321454428\n",
            "Iteration 3947 Loss = 0.06052851260216426\n",
            "Iteration 3948 Loss = 0.06051504954281474\n",
            "Iteration 3949 Loss = 0.06050159403226074\n",
            "Iteration 3950 Loss = 0.06048814606626941\n",
            "Iteration 3951 Loss = 0.06047470564061081\n",
            "Iteration 3952 Loss = 0.06046127275105686\n",
            "Iteration 3953 Loss = 0.060447847393381965\n",
            "Iteration 3954 Loss = 0.06043442956336307\n",
            "Iteration 3955 Loss = 0.060421019256779326\n",
            "Iteration 3956 Loss = 0.06040761646941246\n",
            "Iteration 3957 Loss = 0.0603942211970461\n",
            "Iteration 3958 Loss = 0.06038083343546693\n",
            "Iteration 3959 Loss = 0.06036745318046333\n",
            "Iteration 3960 Loss = 0.06035408042782643\n",
            "Iteration 3961 Loss = 0.06034071517334975\n",
            "Iteration 3962 Loss = 0.060327357412828804\n",
            "Iteration 3963 Loss = 0.060314007142061886\n",
            "Iteration 3964 Loss = 0.06030066435684936\n",
            "Iteration 3965 Loss = 0.06028732905299397\n",
            "Iteration 3966 Loss = 0.06027400122630117\n",
            "Iteration 3967 Loss = 0.06026068087257828\n",
            "Iteration 3968 Loss = 0.060247367987635095\n",
            "Iteration 3969 Loss = 0.06023406256728397\n",
            "Iteration 3970 Loss = 0.06022076460733947\n",
            "Iteration 3971 Loss = 0.06020747410361845\n",
            "Iteration 3972 Loss = 0.06019419105194029\n",
            "Iteration 3973 Loss = 0.0601809154481263\n",
            "Iteration 3974 Loss = 0.06016764728800084\n",
            "Iteration 3975 Loss = 0.06015438656738988\n",
            "Iteration 3976 Loss = 0.060141133282122373\n",
            "Iteration 3977 Loss = 0.06012788742802889\n",
            "Iteration 3978 Loss = 0.06011464900094313\n",
            "Iteration 3979 Loss = 0.06010141799670043\n",
            "Iteration 3980 Loss = 0.06008819441113896\n",
            "Iteration 3981 Loss = 0.060074978240098935\n",
            "Iteration 3982 Loss = 0.06006176947942304\n",
            "Iteration 3983 Loss = 0.0600485681249563\n",
            "Iteration 3984 Loss = 0.06003537417254584\n",
            "Iteration 3985 Loss = 0.060022187618041435\n",
            "Iteration 3986 Loss = 0.06000900845729513\n",
            "Iteration 3987 Loss = 0.05999583668616105\n",
            "Iteration 3988 Loss = 0.059982672300495914\n",
            "Iteration 3989 Loss = 0.059969515296158654\n",
            "Iteration 3990 Loss = 0.05995636566901038\n",
            "Iteration 3991 Loss = 0.0599432234149149\n",
            "Iteration 3992 Loss = 0.05993008852973792\n",
            "Iteration 3993 Loss = 0.05991696100934769\n",
            "Iteration 3994 Loss = 0.05990384084961497\n",
            "Iteration 3995 Loss = 0.059890728046412156\n",
            "Iteration 3996 Loss = 0.05987762259561484\n",
            "Iteration 3997 Loss = 0.05986452449310034\n",
            "Iteration 3998 Loss = 0.059851433734748334\n",
            "Iteration 3999 Loss = 0.05983835031644115\n",
            "Iteration 4000 Loss = 0.05982527423406294\n",
            "Iteration 4001 Loss = 0.05981220548350071\n",
            "Iteration 4002 Loss = 0.05979914406064305\n",
            "Iteration 4003 Loss = 0.059786089961381766\n",
            "Iteration 4004 Loss = 0.05977304318161009\n",
            "Iteration 4005 Loss = 0.059760003717224223\n",
            "Iteration 4006 Loss = 0.05974697156412225\n",
            "Iteration 4007 Loss = 0.05973394671820471\n",
            "Iteration 4008 Loss = 0.05972092917537463\n",
            "Iteration 4009 Loss = 0.05970791893153687\n",
            "Iteration 4010 Loss = 0.05969491598259895\n",
            "Iteration 4011 Loss = 0.05968192032447059\n",
            "Iteration 4012 Loss = 0.05966893195306387\n",
            "Iteration 4013 Loss = 0.05965595086429308\n",
            "Iteration 4014 Loss = 0.05964297705407464\n",
            "Iteration 4015 Loss = 0.05963001051832762\n",
            "Iteration 4016 Loss = 0.059617051252973084\n",
            "Iteration 4017 Loss = 0.0596040992539346\n",
            "Iteration 4018 Loss = 0.0595911545171378\n",
            "Iteration 4019 Loss = 0.059578217038510836\n",
            "Iteration 4020 Loss = 0.05956528681398405\n",
            "Iteration 4021 Loss = 0.0595523638394898\n",
            "Iteration 4022 Loss = 0.059539448110963226\n",
            "Iteration 4023 Loss = 0.059526539624341325\n",
            "Iteration 4024 Loss = 0.059513638375563435\n",
            "Iteration 4025 Loss = 0.05950074436057149\n",
            "Iteration 4026 Loss = 0.059487857575309486\n",
            "Iteration 4027 Loss = 0.059474978015723524\n",
            "Iteration 4028 Loss = 0.05946210567776212\n",
            "Iteration 4029 Loss = 0.059449240557376166\n",
            "Iteration 4030 Loss = 0.059436382650518815\n",
            "Iteration 4031 Loss = 0.05942353195314523\n",
            "Iteration 4032 Loss = 0.05941068846121309\n",
            "Iteration 4033 Loss = 0.059397852170682296\n",
            "Iteration 4034 Loss = 0.059385023077515024\n",
            "Iteration 4035 Loss = 0.059372201177675596\n",
            "Iteration 4036 Loss = 0.05935938646713074\n",
            "Iteration 4037 Loss = 0.059346578941849394\n",
            "Iteration 4038 Loss = 0.059333778597802665\n",
            "Iteration 4039 Loss = 0.05932098543096422\n",
            "Iteration 4040 Loss = 0.05930819943730953\n",
            "Iteration 4041 Loss = 0.05929542061281666\n",
            "Iteration 4042 Loss = 0.05928264895346584\n",
            "Iteration 4043 Loss = 0.05926988445523945\n",
            "Iteration 4044 Loss = 0.05925712711412228\n",
            "Iteration 4045 Loss = 0.05924437692610131\n",
            "Iteration 4046 Loss = 0.05923163388716584\n",
            "Iteration 4047 Loss = 0.05921889799330731\n",
            "Iteration 4048 Loss = 0.05920616924051939\n",
            "Iteration 4049 Loss = 0.05919344762479799\n",
            "Iteration 4050 Loss = 0.059180733142141506\n",
            "Iteration 4051 Loss = 0.05916802578855032\n",
            "Iteration 4052 Loss = 0.05915532556002704\n",
            "Iteration 4053 Loss = 0.0591426324525768\n",
            "Iteration 4054 Loss = 0.05912994646220667\n",
            "Iteration 4055 Loss = 0.05911726758492599\n",
            "Iteration 4056 Loss = 0.05910459581674648\n",
            "Iteration 4057 Loss = 0.059091931153682116\n",
            "Iteration 4058 Loss = 0.059079273591748936\n",
            "Iteration 4059 Loss = 0.05906662312696537\n",
            "Iteration 4060 Loss = 0.059053979755351996\n",
            "Iteration 4061 Loss = 0.05904134347293163\n",
            "Iteration 4062 Loss = 0.059028714275729297\n",
            "Iteration 4063 Loss = 0.05901609215977237\n",
            "Iteration 4064 Loss = 0.05900347712109028\n",
            "Iteration 4065 Loss = 0.058990869155714866\n",
            "Iteration 4066 Loss = 0.05897826825968006\n",
            "Iteration 4067 Loss = 0.05896567442902192\n",
            "Iteration 4068 Loss = 0.058953087659779024\n",
            "Iteration 4069 Loss = 0.058940507947991945\n",
            "Iteration 4070 Loss = 0.05892793528970349\n",
            "Iteration 4071 Loss = 0.05891536968095892\n",
            "Iteration 4072 Loss = 0.058902811117805244\n",
            "Iteration 4073 Loss = 0.05889025959629219\n",
            "Iteration 4074 Loss = 0.058877715112471425\n",
            "Iteration 4075 Loss = 0.058865177662396774\n",
            "Iteration 4076 Loss = 0.058852647242124544\n",
            "Iteration 4077 Loss = 0.058840123847712984\n",
            "Iteration 4078 Loss = 0.058827607475222796\n",
            "Iteration 4079 Loss = 0.05881509812071652\n",
            "Iteration 4080 Loss = 0.058802595780259315\n",
            "Iteration 4081 Loss = 0.058790100449918496\n",
            "Iteration 4082 Loss = 0.058777612125763046\n",
            "Iteration 4083 Loss = 0.058765130803865015\n",
            "Iteration 4084 Loss = 0.05875265648029807\n",
            "Iteration 4085 Loss = 0.05874018915113803\n",
            "Iteration 4086 Loss = 0.058727728812463326\n",
            "Iteration 4087 Loss = 0.05871527546035428\n",
            "Iteration 4088 Loss = 0.05870282909089343\n",
            "Iteration 4089 Loss = 0.058690389700165595\n",
            "Iteration 4090 Loss = 0.05867795728425795\n",
            "Iteration 4091 Loss = 0.05866553183925948\n",
            "Iteration 4092 Loss = 0.0586531133612617\n",
            "Iteration 4093 Loss = 0.05864070184635793\n",
            "Iteration 4094 Loss = 0.058628297290644275\n",
            "Iteration 4095 Loss = 0.05861589969021855\n",
            "Iteration 4096 Loss = 0.0586035090411808\n",
            "Iteration 4097 Loss = 0.0585911253396333\n",
            "Iteration 4098 Loss = 0.05857874858168089\n",
            "Iteration 4099 Loss = 0.05856637876342997\n",
            "Iteration 4100 Loss = 0.058554015880989546\n",
            "Iteration 4101 Loss = 0.058541659930470676\n",
            "Iteration 4102 Loss = 0.05852931090798644\n",
            "Iteration 4103 Loss = 0.05851696880965259\n",
            "Iteration 4104 Loss = 0.05850463363158634\n",
            "Iteration 4105 Loss = 0.05849230536990785\n",
            "Iteration 4106 Loss = 0.058479984020738625\n",
            "Iteration 4107 Loss = 0.05846766958020326\n",
            "Iteration 4108 Loss = 0.05845536204442769\n",
            "Iteration 4109 Loss = 0.05844306140954067\n",
            "Iteration 4110 Loss = 0.05843076767167257\n",
            "Iteration 4111 Loss = 0.05841848082695653\n",
            "Iteration 4112 Loss = 0.058406200871527196\n",
            "Iteration 4113 Loss = 0.0583939278015218\n",
            "Iteration 4114 Loss = 0.05838166161307972\n",
            "Iteration 4115 Loss = 0.05836940230234253\n",
            "Iteration 4116 Loss = 0.05835714986545369\n",
            "Iteration 4117 Loss = 0.05834490429855898\n",
            "Iteration 4118 Loss = 0.05833266559780668\n",
            "Iteration 4119 Loss = 0.05832043375934661\n",
            "Iteration 4120 Loss = 0.05830820877933104\n",
            "Iteration 4121 Loss = 0.058295990653914546\n",
            "Iteration 4122 Loss = 0.05828377937925374\n",
            "Iteration 4123 Loss = 0.05827157495150724\n",
            "Iteration 4124 Loss = 0.05825937736683622\n",
            "Iteration 4125 Loss = 0.05824718662140337\n",
            "Iteration 4126 Loss = 0.05823500271137422\n",
            "Iteration 4127 Loss = 0.058222825632915906\n",
            "Iteration 4128 Loss = 0.05821065538219795\n",
            "Iteration 4129 Loss = 0.05819849195539218\n",
            "Iteration 4130 Loss = 0.05818633534867229\n",
            "Iteration 4131 Loss = 0.05817418555821413\n",
            "Iteration 4132 Loss = 0.05816204258019605\n",
            "Iteration 4133 Loss = 0.05814990641079799\n",
            "Iteration 4134 Loss = 0.05813777704620253\n",
            "Iteration 4135 Loss = 0.05812565448259424\n",
            "Iteration 4136 Loss = 0.058113538716159624\n",
            "Iteration 4137 Loss = 0.0581014297430875\n",
            "Iteration 4138 Loss = 0.05808932755956883\n",
            "Iteration 4139 Loss = 0.05807723216179663\n",
            "Iteration 4140 Loss = 0.05806514354596607\n",
            "Iteration 4141 Loss = 0.05805306170827474\n",
            "Iteration 4142 Loss = 0.05804098664492188\n",
            "Iteration 4143 Loss = 0.058028918352109125\n",
            "Iteration 4144 Loss = 0.058016856826040124\n",
            "Iteration 4145 Loss = 0.058004802062921045\n",
            "Iteration 4146 Loss = 0.057992754058959456\n",
            "Iteration 4147 Loss = 0.05798071281036579\n",
            "Iteration 4148 Loss = 0.057968678313352145\n",
            "Iteration 4149 Loss = 0.05795665056413277\n",
            "Iteration 4150 Loss = 0.05794462955892447\n",
            "Iteration 4151 Loss = 0.057932615293945555\n",
            "Iteration 4152 Loss = 0.057920607765416875\n",
            "Iteration 4153 Loss = 0.05790860696956114\n",
            "Iteration 4154 Loss = 0.057896612902603506\n",
            "Iteration 4155 Loss = 0.05788462556077101\n",
            "Iteration 4156 Loss = 0.05787264494029273\n",
            "Iteration 4157 Loss = 0.05786067103740003\n",
            "Iteration 4158 Loss = 0.05784870384832641\n",
            "Iteration 4159 Loss = 0.05783674336930724\n",
            "Iteration 4160 Loss = 0.057824789596580474\n",
            "Iteration 4161 Loss = 0.057812842526385445\n",
            "Iteration 4162 Loss = 0.057800902154964244\n",
            "Iteration 4163 Loss = 0.05778896847856086\n",
            "Iteration 4164 Loss = 0.057777041493421236\n",
            "Iteration 4165 Loss = 0.05776512119579375\n",
            "Iteration 4166 Loss = 0.05775320758192861\n",
            "Iteration 4167 Loss = 0.05774130064807813\n",
            "Iteration 4168 Loss = 0.05772940039049684\n",
            "Iteration 4169 Loss = 0.05771750680544117\n",
            "Iteration 4170 Loss = 0.057705619889170154\n",
            "Iteration 4171 Loss = 0.057693739637944205\n",
            "Iteration 4172 Loss = 0.05768186604802654\n",
            "Iteration 4173 Loss = 0.05766999911568191\n",
            "Iteration 4174 Loss = 0.05765813883717746\n",
            "Iteration 4175 Loss = 0.05764628520878223\n",
            "Iteration 4176 Loss = 0.05763443822676764\n",
            "Iteration 4177 Loss = 0.05762259788740704\n",
            "Iteration 4178 Loss = 0.057610764186975756\n",
            "Iteration 4179 Loss = 0.057598937121751466\n",
            "Iteration 4180 Loss = 0.05758711668801345\n",
            "Iteration 4181 Loss = 0.05757530288204371\n",
            "Iteration 4182 Loss = 0.057563495700126054\n",
            "Iteration 4183 Loss = 0.05755169513854613\n",
            "Iteration 4184 Loss = 0.05753990119359208\n",
            "Iteration 4185 Loss = 0.05752811386155392\n",
            "Iteration 4186 Loss = 0.05751633313872348\n",
            "Iteration 4187 Loss = 0.05750455902139534\n",
            "Iteration 4188 Loss = 0.05749279150586548\n",
            "Iteration 4189 Loss = 0.05748103058843241\n",
            "Iteration 4190 Loss = 0.057469276265396475\n",
            "Iteration 4191 Loss = 0.05745752853306017\n",
            "Iteration 4192 Loss = 0.05744578738772812\n",
            "Iteration 4193 Loss = 0.05743405282570678\n",
            "Iteration 4194 Loss = 0.057422324843305145\n",
            "Iteration 4195 Loss = 0.0574106034368337\n",
            "Iteration 4196 Loss = 0.05739888860260552\n",
            "Iteration 4197 Loss = 0.057387180336935545\n",
            "Iteration 4198 Loss = 0.05737547863614044\n",
            "Iteration 4199 Loss = 0.05736378349653959\n",
            "Iteration 4200 Loss = 0.05735209491445393\n",
            "Iteration 4201 Loss = 0.057340412886206664\n",
            "Iteration 4202 Loss = 0.05732873740812309\n",
            "Iteration 4203 Loss = 0.05731706847653037\n",
            "Iteration 4204 Loss = 0.05730540608775805\n",
            "Iteration 4205 Loss = 0.057293750238137375\n",
            "Iteration 4206 Loss = 0.05728210092400187\n",
            "Iteration 4207 Loss = 0.05727045814168699\n",
            "Iteration 4208 Loss = 0.05725882188753059\n",
            "Iteration 4209 Loss = 0.057247192157871926\n",
            "Iteration 4210 Loss = 0.05723556894905287\n",
            "Iteration 4211 Loss = 0.05722395225741729\n",
            "Iteration 4212 Loss = 0.057212342079310634\n",
            "Iteration 4213 Loss = 0.057200738411081094\n",
            "Iteration 4214 Loss = 0.057189141249078285\n",
            "Iteration 4215 Loss = 0.05717755058965432\n",
            "Iteration 4216 Loss = 0.05716596642916295\n",
            "Iteration 4217 Loss = 0.05715438876396054\n",
            "Iteration 4218 Loss = 0.057142817590404925\n",
            "Iteration 4219 Loss = 0.057131252904856204\n",
            "Iteration 4220 Loss = 0.0571196947036765\n",
            "Iteration 4221 Loss = 0.05710814298323019\n",
            "Iteration 4222 Loss = 0.057096597739883286\n",
            "Iteration 4223 Loss = 0.0570850589700042\n",
            "Iteration 4224 Loss = 0.05707352666996305\n",
            "Iteration 4225 Loss = 0.057062000836132484\n",
            "Iteration 4226 Loss = 0.05705048146488648\n",
            "Iteration 4227 Loss = 0.057038968552601675\n",
            "Iteration 4228 Loss = 0.05702746209565663\n",
            "Iteration 4229 Loss = 0.05701596209043146\n",
            "Iteration 4230 Loss = 0.05700446853330891\n",
            "Iteration 4231 Loss = 0.05699298142067344\n",
            "Iteration 4232 Loss = 0.056981500748911555\n",
            "Iteration 4233 Loss = 0.05697002651441195\n",
            "Iteration 4234 Loss = 0.05695855871356509\n",
            "Iteration 4235 Loss = 0.05694709734276355\n",
            "Iteration 4236 Loss = 0.0569356423984022\n",
            "Iteration 4237 Loss = 0.05692419387687747\n",
            "Iteration 4238 Loss = 0.05691275177458814\n",
            "Iteration 4239 Loss = 0.056901316087935035\n",
            "Iteration 4240 Loss = 0.05688988681332069\n",
            "Iteration 4241 Loss = 0.056878463947149946\n",
            "Iteration 4242 Loss = 0.05686704748582958\n",
            "Iteration 4243 Loss = 0.05685563742576836\n",
            "Iteration 4244 Loss = 0.05684423376337697\n",
            "Iteration 4245 Loss = 0.05683283649506833\n",
            "Iteration 4246 Loss = 0.05682144561725713\n",
            "Iteration 4247 Loss = 0.056810061126360396\n",
            "Iteration 4248 Loss = 0.056798683018796675\n",
            "Iteration 4249 Loss = 0.05678731129098707\n",
            "Iteration 4250 Loss = 0.056775945939354346\n",
            "Iteration 4251 Loss = 0.05676458696032314\n",
            "Iteration 4252 Loss = 0.05675323435032064\n",
            "Iteration 4253 Loss = 0.056741888105775545\n",
            "Iteration 4254 Loss = 0.05673054822311872\n",
            "Iteration 4255 Loss = 0.05671921469878293\n",
            "Iteration 4256 Loss = 0.056707887529203235\n",
            "Iteration 4257 Loss = 0.056696566710816215\n",
            "Iteration 4258 Loss = 0.05668525224006115\n",
            "Iteration 4259 Loss = 0.05667394411337851\n",
            "Iteration 4260 Loss = 0.05666264232721129\n",
            "Iteration 4261 Loss = 0.05665134687800436\n",
            "Iteration 4262 Loss = 0.05664005776220473\n",
            "Iteration 4263 Loss = 0.05662877497626071\n",
            "Iteration 4264 Loss = 0.05661749851662367\n",
            "Iteration 4265 Loss = 0.056606228379746076\n",
            "Iteration 4266 Loss = 0.056594964562082926\n",
            "Iteration 4267 Loss = 0.05658370706009096\n",
            "Iteration 4268 Loss = 0.05657245587022883\n",
            "Iteration 4269 Loss = 0.05656121098895749\n",
            "Iteration 4270 Loss = 0.05654997241273955\n",
            "Iteration 4271 Loss = 0.05653874013803978\n",
            "Iteration 4272 Loss = 0.056527514161324906\n",
            "Iteration 4273 Loss = 0.05651629447906364\n",
            "Iteration 4274 Loss = 0.05650508108772665\n",
            "Iteration 4275 Loss = 0.05649387398378648\n",
            "Iteration 4276 Loss = 0.056482673163717904\n",
            "Iteration 4277 Loss = 0.056471478623997456\n",
            "Iteration 4278 Loss = 0.05646029036110378\n",
            "Iteration 4279 Loss = 0.056449108371517204\n",
            "Iteration 4280 Loss = 0.05643793265172057\n",
            "Iteration 4281 Loss = 0.056426763198198275\n",
            "Iteration 4282 Loss = 0.056415600007436735\n",
            "Iteration 4283 Loss = 0.056404443075924404\n",
            "Iteration 4284 Loss = 0.056393292400151625\n",
            "Iteration 4285 Loss = 0.05638214797661089\n",
            "Iteration 4286 Loss = 0.056371009801796663\n",
            "Iteration 4287 Loss = 0.05635987787220494\n",
            "Iteration 4288 Loss = 0.05634875218433425\n",
            "Iteration 4289 Loss = 0.05633763273468475\n",
            "Iteration 4290 Loss = 0.05632651951975867\n",
            "Iteration 4291 Loss = 0.05631541253606002\n",
            "Iteration 4292 Loss = 0.05630431178009519\n",
            "Iteration 4293 Loss = 0.056293217248372086\n",
            "Iteration 4294 Loss = 0.05628212893740076\n",
            "Iteration 4295 Loss = 0.056271046843693315\n",
            "Iteration 4296 Loss = 0.05625997096376346\n",
            "Iteration 4297 Loss = 0.05624890129412744\n",
            "Iteration 4298 Loss = 0.05623783783130292\n",
            "Iteration 4299 Loss = 0.0562267805718096\n",
            "Iteration 4300 Loss = 0.056215729512169466\n",
            "Iteration 4301 Loss = 0.056204684648906204\n",
            "Iteration 4302 Loss = 0.056193645978545384\n",
            "Iteration 4303 Loss = 0.056182613497614664\n",
            "Iteration 4304 Loss = 0.05617158720264356\n",
            "Iteration 4305 Loss = 0.05616056709016365\n",
            "Iteration 4306 Loss = 0.05614955315670845\n",
            "Iteration 4307 Loss = 0.05613854539881312\n",
            "Iteration 4308 Loss = 0.05612754381301525\n",
            "Iteration 4309 Loss = 0.056116548395853956\n",
            "Iteration 4310 Loss = 0.05610555914387051\n",
            "Iteration 4311 Loss = 0.05609457605360806\n",
            "Iteration 4312 Loss = 0.0560835991216116\n",
            "Iteration 4313 Loss = 0.056072628344428464\n",
            "Iteration 4314 Loss = 0.05606166371860732\n",
            "Iteration 4315 Loss = 0.05605070524069914\n",
            "Iteration 4316 Loss = 0.05603975290725687\n",
            "Iteration 4317 Loss = 0.05602880671483512\n",
            "Iteration 4318 Loss = 0.05601786665999083\n",
            "Iteration 4319 Loss = 0.05600693273928248\n",
            "Iteration 4320 Loss = 0.05599600494927061\n",
            "Iteration 4321 Loss = 0.055985083286517745\n",
            "Iteration 4322 Loss = 0.05597416774758821\n",
            "Iteration 4323 Loss = 0.05596325832904858\n",
            "Iteration 4324 Loss = 0.05595235502746695\n",
            "Iteration 4325 Loss = 0.055941457839413525\n",
            "Iteration 4326 Loss = 0.055930566761460465\n",
            "Iteration 4327 Loss = 0.05591968179018187\n",
            "Iteration 4328 Loss = 0.05590880292215355\n",
            "Iteration 4329 Loss = 0.05589793015395359\n",
            "Iteration 4330 Loss = 0.05588706348216157\n",
            "Iteration 4331 Loss = 0.055876202903359366\n",
            "Iteration 4332 Loss = 0.055865348414130496\n",
            "Iteration 4333 Loss = 0.05585450001106059\n",
            "Iteration 4334 Loss = 0.05584365769073704\n",
            "Iteration 4335 Loss = 0.055832821449749416\n",
            "Iteration 4336 Loss = 0.05582199128468877\n",
            "Iteration 4337 Loss = 0.055811167192148466\n",
            "Iteration 4338 Loss = 0.05580034916872342\n",
            "Iteration 4339 Loss = 0.055789537211010966\n",
            "Iteration 4340 Loss = 0.05577873131560968\n",
            "Iteration 4341 Loss = 0.05576793147912062\n",
            "Iteration 4342 Loss = 0.05575713769814647\n",
            "Iteration 4343 Loss = 0.05574634996929197\n",
            "Iteration 4344 Loss = 0.055735568289163546\n",
            "Iteration 4345 Loss = 0.055724792654369575\n",
            "Iteration 4346 Loss = 0.05571402306152057\n",
            "Iteration 4347 Loss = 0.05570325950722879\n",
            "Iteration 4348 Loss = 0.05569250198810838\n",
            "Iteration 4349 Loss = 0.05568175050077525\n",
            "Iteration 4350 Loss = 0.055671005041847584\n",
            "Iteration 4351 Loss = 0.05566026560794502\n",
            "Iteration 4352 Loss = 0.05564953219568942\n",
            "Iteration 4353 Loss = 0.05563880480170424\n",
            "Iteration 4354 Loss = 0.05562808342261531\n",
            "Iteration 4355 Loss = 0.05561736805504992\n",
            "Iteration 4356 Loss = 0.055606658695637246\n",
            "Iteration 4357 Loss = 0.05559595534100858\n",
            "Iteration 4358 Loss = 0.05558525798779716\n",
            "Iteration 4359 Loss = 0.05557456663263774\n",
            "Iteration 4360 Loss = 0.05556388127216722\n",
            "Iteration 4361 Loss = 0.05555320190302454\n",
            "Iteration 4362 Loss = 0.0555425285218502\n",
            "Iteration 4363 Loss = 0.05553186112528675\n",
            "Iteration 4364 Loss = 0.05552119970997852\n",
            "Iteration 4365 Loss = 0.05551054427257191\n",
            "Iteration 4366 Loss = 0.05549989480971495\n",
            "Iteration 4367 Loss = 0.05548925131805785\n",
            "Iteration 4368 Loss = 0.055478613794252375\n",
            "Iteration 4369 Loss = 0.05546798223495251\n",
            "Iteration 4370 Loss = 0.05545735663681367\n",
            "Iteration 4371 Loss = 0.05544673699649354\n",
            "Iteration 4372 Loss = 0.05543612331065164\n",
            "Iteration 4373 Loss = 0.055425515575949186\n",
            "Iteration 4374 Loss = 0.0554149137890494\n",
            "Iteration 4375 Loss = 0.055404317946617036\n",
            "Iteration 4376 Loss = 0.055393728045319515\n",
            "Iteration 4377 Loss = 0.05538314408182527\n",
            "Iteration 4378 Loss = 0.05537256605280488\n",
            "Iteration 4379 Loss = 0.05536199395493106\n",
            "Iteration 4380 Loss = 0.05535142778487829\n",
            "Iteration 4381 Loss = 0.05534086753932247\n",
            "Iteration 4382 Loss = 0.05533031321494201\n",
            "Iteration 4383 Loss = 0.05531976480841663\n",
            "Iteration 4384 Loss = 0.055309222316428416\n",
            "Iteration 4385 Loss = 0.05529868573566101\n",
            "Iteration 4386 Loss = 0.055288155062799764\n",
            "Iteration 4387 Loss = 0.055277630294532185\n",
            "Iteration 4388 Loss = 0.05526711142754772\n",
            "Iteration 4389 Loss = 0.05525659845853729\n",
            "Iteration 4390 Loss = 0.05524609138419391\n",
            "Iteration 4391 Loss = 0.05523559020121255\n",
            "Iteration 4392 Loss = 0.05522509490628969\n",
            "Iteration 4393 Loss = 0.05521460549612406\n",
            "Iteration 4394 Loss = 0.05520412196741595\n",
            "Iteration 4395 Loss = 0.05519364431686761\n",
            "Iteration 4396 Loss = 0.0551831725411831\n",
            "Iteration 4397 Loss = 0.05517270663706849\n",
            "Iteration 4398 Loss = 0.05516224660123137\n",
            "Iteration 4399 Loss = 0.055151792430381534\n",
            "Iteration 4400 Loss = 0.05514134412123029\n",
            "Iteration 4401 Loss = 0.055130901670491214\n",
            "Iteration 4402 Loss = 0.055120465074879346\n",
            "Iteration 4403 Loss = 0.05511003433111159\n",
            "Iteration 4404 Loss = 0.05509960943590679\n",
            "Iteration 4405 Loss = 0.05508919038598581\n",
            "Iteration 4406 Loss = 0.05507877717807102\n",
            "Iteration 4407 Loss = 0.05506836980888685\n",
            "Iteration 4408 Loss = 0.055057968275159526\n",
            "Iteration 4409 Loss = 0.05504757257361701\n",
            "Iteration 4410 Loss = 0.05503718270098918\n",
            "Iteration 4411 Loss = 0.05502679865400777\n",
            "Iteration 4412 Loss = 0.0550164204294063\n",
            "Iteration 4413 Loss = 0.05500604802392016\n",
            "Iteration 4414 Loss = 0.05499568143428647\n",
            "Iteration 4415 Loss = 0.05498532065724446\n",
            "Iteration 4416 Loss = 0.05497496568953475\n",
            "Iteration 4417 Loss = 0.05496461652790007\n",
            "Iteration 4418 Loss = 0.05495427316908505\n",
            "Iteration 4419 Loss = 0.054943935609835896\n",
            "Iteration 4420 Loss = 0.05493360384690086\n",
            "Iteration 4421 Loss = 0.054923277877029754\n",
            "Iteration 4422 Loss = 0.054912957696974636\n",
            "Iteration 4423 Loss = 0.05490264330348896\n",
            "Iteration 4424 Loss = 0.054892334693328264\n",
            "Iteration 4425 Loss = 0.05488203186324963\n",
            "Iteration 4426 Loss = 0.05487173481001241\n",
            "Iteration 4427 Loss = 0.05486144353037724\n",
            "Iteration 4428 Loss = 0.054851158021107155\n",
            "Iteration 4429 Loss = 0.054840878278966304\n",
            "Iteration 4430 Loss = 0.05483060430072132\n",
            "Iteration 4431 Loss = 0.05482033608314018\n",
            "Iteration 4432 Loss = 0.054810073622993026\n",
            "Iteration 4433 Loss = 0.05479981691705157\n",
            "Iteration 4434 Loss = 0.054789565962089296\n",
            "Iteration 4435 Loss = 0.05477932075488183\n",
            "Iteration 4436 Loss = 0.05476908129220619\n",
            "Iteration 4437 Loss = 0.05475884757084138\n",
            "Iteration 4438 Loss = 0.05474861958756826\n",
            "Iteration 4439 Loss = 0.054738397339169624\n",
            "Iteration 4440 Loss = 0.05472818082242973\n",
            "Iteration 4441 Loss = 0.05471797003413484\n",
            "Iteration 4442 Loss = 0.05470776497107299\n",
            "Iteration 4443 Loss = 0.05469756563003408\n",
            "Iteration 4444 Loss = 0.054687372007809584\n",
            "Iteration 4445 Loss = 0.054677184101193134\n",
            "Iteration 4446 Loss = 0.05466700190697991\n",
            "Iteration 4447 Loss = 0.05465682542196691\n",
            "Iteration 4448 Loss = 0.05464665464295286\n",
            "Iteration 4449 Loss = 0.05463648956673864\n",
            "Iteration 4450 Loss = 0.05462633019012642\n",
            "Iteration 4451 Loss = 0.054616176509920435\n",
            "Iteration 4452 Loss = 0.05460602852292685\n",
            "Iteration 4453 Loss = 0.05459588622595329\n",
            "Iteration 4454 Loss = 0.05458574961580947\n",
            "Iteration 4455 Loss = 0.05457561868930663\n",
            "Iteration 4456 Loss = 0.05456549344325799\n",
            "Iteration 4457 Loss = 0.05455537387447862\n",
            "Iteration 4458 Loss = 0.054545259979784905\n",
            "Iteration 4459 Loss = 0.05453515175599565\n",
            "Iteration 4460 Loss = 0.05452504919993125\n",
            "Iteration 4461 Loss = 0.054514952308413495\n",
            "Iteration 4462 Loss = 0.05450486107826643\n",
            "Iteration 4463 Loss = 0.05449477550631559\n",
            "Iteration 4464 Loss = 0.05448469558938837\n",
            "Iteration 4465 Loss = 0.05447462132431416\n",
            "Iteration 4466 Loss = 0.05446455270792389\n",
            "Iteration 4467 Loss = 0.05445448973705031\n",
            "Iteration 4468 Loss = 0.0544444324085278\n",
            "Iteration 4469 Loss = 0.054434380719192814\n",
            "Iteration 4470 Loss = 0.05442433466588335\n",
            "Iteration 4471 Loss = 0.054414294245439525\n",
            "Iteration 4472 Loss = 0.054404259454702626\n",
            "Iteration 4473 Loss = 0.054394230290516064\n",
            "Iteration 4474 Loss = 0.05438420674972528\n",
            "Iteration 4475 Loss = 0.0543741888291772\n",
            "Iteration 4476 Loss = 0.054364176525720134\n",
            "Iteration 4477 Loss = 0.054354169836204926\n",
            "Iteration 4478 Loss = 0.05434416875748372\n",
            "Iteration 4479 Loss = 0.05433417328641051\n",
            "Iteration 4480 Loss = 0.05432418341984102\n",
            "Iteration 4481 Loss = 0.05431419915463282\n",
            "Iteration 4482 Loss = 0.05430422048764517\n",
            "Iteration 4483 Loss = 0.054294247415739176\n",
            "Iteration 4484 Loss = 0.05428427993577759\n",
            "Iteration 4485 Loss = 0.054274318044625035\n",
            "Iteration 4486 Loss = 0.05426436173914786\n",
            "Iteration 4487 Loss = 0.054254411016214116\n",
            "Iteration 4488 Loss = 0.054244465872693735\n",
            "Iteration 4489 Loss = 0.05423452630545806\n",
            "Iteration 4490 Loss = 0.05422459231138078\n",
            "Iteration 4491 Loss = 0.05421466388733682\n",
            "Iteration 4492 Loss = 0.05420474103020301\n",
            "Iteration 4493 Loss = 0.054194823736857994\n",
            "Iteration 4494 Loss = 0.0541849120041822\n",
            "Iteration 4495 Loss = 0.05417500582905765\n",
            "Iteration 4496 Loss = 0.05416510520836813\n",
            "Iteration 4497 Loss = 0.054155210138999585\n",
            "Iteration 4498 Loss = 0.05414532061783895\n",
            "Iteration 4499 Loss = 0.054135436641775496\n",
            "Iteration 4500 Loss = 0.05412555820770013\n",
            "Iteration 4501 Loss = 0.0541156853125054\n",
            "Iteration 4502 Loss = 0.05410581795308551\n",
            "Iteration 4503 Loss = 0.054095956126336664\n",
            "Iteration 4504 Loss = 0.05408609982915655\n",
            "Iteration 4505 Loss = 0.05407624905844498\n",
            "Iteration 4506 Loss = 0.05406640381110283\n",
            "Iteration 4507 Loss = 0.054056564084033525\n",
            "Iteration 4508 Loss = 0.05404672987414162\n",
            "Iteration 4509 Loss = 0.05403690117833358\n",
            "Iteration 4510 Loss = 0.05402707799351767\n",
            "Iteration 4511 Loss = 0.054017260316603845\n",
            "Iteration 4512 Loss = 0.054007448144503895\n",
            "Iteration 4513 Loss = 0.05399764147413112\n",
            "Iteration 4514 Loss = 0.05398784030240067\n",
            "Iteration 4515 Loss = 0.05397804462622958\n",
            "Iteration 4516 Loss = 0.05396825444253643\n",
            "Iteration 4517 Loss = 0.05395846974824157\n",
            "Iteration 4518 Loss = 0.053948690540266866\n",
            "Iteration 4519 Loss = 0.053938916815536295\n",
            "Iteration 4520 Loss = 0.05392914857097544\n",
            "Iteration 4521 Loss = 0.05391938580351134\n",
            "Iteration 4522 Loss = 0.05390962851007318\n",
            "Iteration 4523 Loss = 0.05389987668759149\n",
            "Iteration 4524 Loss = 0.05389013033299877\n",
            "Iteration 4525 Loss = 0.053880389443229065\n",
            "Iteration 4526 Loss = 0.05387065401521829\n",
            "Iteration 4527 Loss = 0.053860924045904125\n",
            "Iteration 4528 Loss = 0.05385119953222556\n",
            "Iteration 4529 Loss = 0.05384148047112382\n",
            "Iteration 4530 Loss = 0.053831766859541684\n",
            "Iteration 4531 Loss = 0.05382205869442331\n",
            "Iteration 4532 Loss = 0.05381235597271508\n",
            "Iteration 4533 Loss = 0.05380265869136488\n",
            "Iteration 4534 Loss = 0.05379296684732202\n",
            "Iteration 4535 Loss = 0.05378328043753796\n",
            "Iteration 4536 Loss = 0.053773599458965676\n",
            "Iteration 4537 Loss = 0.053763923908559795\n",
            "Iteration 4538 Loss = 0.053754253783276834\n",
            "Iteration 4539 Loss = 0.053744589080074866\n",
            "Iteration 4540 Loss = 0.05373492979591371\n",
            "Iteration 4541 Loss = 0.05372527592775484\n",
            "Iteration 4542 Loss = 0.053715627472561436\n",
            "Iteration 4543 Loss = 0.05370598442729852\n",
            "Iteration 4544 Loss = 0.05369634678893263\n",
            "Iteration 4545 Loss = 0.053686714554432274\n",
            "Iteration 4546 Loss = 0.053677087720767264\n",
            "Iteration 4547 Loss = 0.05366746628490941\n",
            "Iteration 4548 Loss = 0.05365785024383221\n",
            "Iteration 4549 Loss = 0.05364823959451068\n",
            "Iteration 4550 Loss = 0.05363863433392176\n",
            "Iteration 4551 Loss = 0.053629034459043816\n",
            "Iteration 4552 Loss = 0.05361943996685726\n",
            "Iteration 4553 Loss = 0.0536098508543438\n",
            "Iteration 4554 Loss = 0.053600267118487116\n",
            "Iteration 4555 Loss = 0.05359068875627229\n",
            "Iteration 4556 Loss = 0.0535811157646867\n",
            "Iteration 4557 Loss = 0.05357154814071877\n",
            "Iteration 4558 Loss = 0.053561985881358846\n",
            "Iteration 4559 Loss = 0.05355242898359905\n",
            "Iteration 4560 Loss = 0.05354287744443301\n",
            "Iteration 4561 Loss = 0.053533331260856167\n",
            "Iteration 4562 Loss = 0.053523790429865645\n",
            "Iteration 4563 Loss = 0.05351425494846026\n",
            "Iteration 4564 Loss = 0.05350472481364041\n",
            "Iteration 4565 Loss = 0.053495200022408214\n",
            "Iteration 4566 Loss = 0.05348568057176766\n",
            "Iteration 4567 Loss = 0.05347616645872414\n",
            "Iteration 4568 Loss = 0.05346665768028478\n",
            "Iteration 4569 Loss = 0.053457154233458544\n",
            "Iteration 4570 Loss = 0.05344765611525608\n",
            "Iteration 4571 Loss = 0.05343816332268931\n",
            "Iteration 4572 Loss = 0.05342867585277241\n",
            "Iteration 4573 Loss = 0.05341919370252075\n",
            "Iteration 4574 Loss = 0.05340971686895162\n",
            "Iteration 4575 Loss = 0.05340024534908402\n",
            "Iteration 4576 Loss = 0.05339077913993842\n",
            "Iteration 4577 Loss = 0.05338131823853722\n",
            "Iteration 4578 Loss = 0.053371862641904196\n",
            "Iteration 4579 Loss = 0.053362412347065\n",
            "Iteration 4580 Loss = 0.05335296735104685\n",
            "Iteration 4581 Loss = 0.05334352765087871\n",
            "Iteration 4582 Loss = 0.05333409324359124\n",
            "Iteration 4583 Loss = 0.05332466412621666\n",
            "Iteration 4584 Loss = 0.05331524029578881\n",
            "Iteration 4585 Loss = 0.053305821749343424\n",
            "Iteration 4586 Loss = 0.05329640848391764\n",
            "Iteration 4587 Loss = 0.05328700049655041\n",
            "Iteration 4588 Loss = 0.05327759778428219\n",
            "Iteration 4589 Loss = 0.05326820034415545\n",
            "Iteration 4590 Loss = 0.05325880817321388\n",
            "Iteration 4591 Loss = 0.05324942126850301\n",
            "Iteration 4592 Loss = 0.0532400396270702\n",
            "Iteration 4593 Loss = 0.05323066324596416\n",
            "Iteration 4594 Loss = 0.05322129212223546\n",
            "Iteration 4595 Loss = 0.05321192625293639\n",
            "Iteration 4596 Loss = 0.05320256563512048\n",
            "Iteration 4597 Loss = 0.05319321026584342\n",
            "Iteration 4598 Loss = 0.053183860142162344\n",
            "Iteration 4599 Loss = 0.053174515261135806\n",
            "Iteration 4600 Loss = 0.053165175619824576\n",
            "Iteration 4601 Loss = 0.05315584121529033\n",
            "Iteration 4602 Loss = 0.05314651204459711\n",
            "Iteration 4603 Loss = 0.05313718810481019\n",
            "Iteration 4604 Loss = 0.053127869392996394\n",
            "Iteration 4605 Loss = 0.05311855590622463\n",
            "Iteration 4606 Loss = 0.053109247641564966\n",
            "Iteration 4607 Loss = 0.053099944596089604\n",
            "Iteration 4608 Loss = 0.05309064676687185\n",
            "Iteration 4609 Loss = 0.05308135415098716\n",
            "Iteration 4610 Loss = 0.05307206674551211\n",
            "Iteration 4611 Loss = 0.053062784547525534\n",
            "Iteration 4612 Loss = 0.053053507554107336\n",
            "Iteration 4613 Loss = 0.05304423576233953\n",
            "Iteration 4614 Loss = 0.053034969169305266\n",
            "Iteration 4615 Loss = 0.053025707772089696\n",
            "Iteration 4616 Loss = 0.05301645156777952\n",
            "Iteration 4617 Loss = 0.05300720055346309\n",
            "Iteration 4618 Loss = 0.05299795472623012\n",
            "Iteration 4619 Loss = 0.052988714083172433\n",
            "Iteration 4620 Loss = 0.052979478621383316\n",
            "Iteration 4621 Loss = 0.05297024833795738\n",
            "Iteration 4622 Loss = 0.05296102322999116\n",
            "Iteration 4623 Loss = 0.052951803294582905\n",
            "Iteration 4624 Loss = 0.052942588528832046\n",
            "Iteration 4625 Loss = 0.05293337892984023\n",
            "Iteration 4626 Loss = 0.05292417449471021\n",
            "Iteration 4627 Loss = 0.052914975220546744\n",
            "Iteration 4628 Loss = 0.05290578110445587\n",
            "Iteration 4629 Loss = 0.05289659214354575\n",
            "Iteration 4630 Loss = 0.05288740833492557\n",
            "Iteration 4631 Loss = 0.052878229675706406\n",
            "Iteration 4632 Loss = 0.05286905616300114\n",
            "Iteration 4633 Loss = 0.05285988779392416\n",
            "Iteration 4634 Loss = 0.05285072456559108\n",
            "Iteration 4635 Loss = 0.05284156647511977\n",
            "Iteration 4636 Loss = 0.05283241351962927\n",
            "Iteration 4637 Loss = 0.05282326569624037\n",
            "Iteration 4638 Loss = 0.05281412300207559\n",
            "Iteration 4639 Loss = 0.052804985434258836\n",
            "Iteration 4640 Loss = 0.05279585298991576\n",
            "Iteration 4641 Loss = 0.0527867256661737\n",
            "Iteration 4642 Loss = 0.052777603460161346\n",
            "Iteration 4643 Loss = 0.05276848636900934\n",
            "Iteration 4644 Loss = 0.05275937438984966\n",
            "Iteration 4645 Loss = 0.052750267519816006\n",
            "Iteration 4646 Loss = 0.052741165756043595\n",
            "Iteration 4647 Loss = 0.05273206909566943\n",
            "Iteration 4648 Loss = 0.05272297753583201\n",
            "Iteration 4649 Loss = 0.05271389107367148\n",
            "Iteration 4650 Loss = 0.05270480970632937\n",
            "Iteration 4651 Loss = 0.05269573343094921\n",
            "Iteration 4652 Loss = 0.052686662244675796\n",
            "Iteration 4653 Loss = 0.05267759614465561\n",
            "Iteration 4654 Loss = 0.052668535128036924\n",
            "Iteration 4655 Loss = 0.05265947919196928\n",
            "Iteration 4656 Loss = 0.05265042833360403\n",
            "Iteration 4657 Loss = 0.05264138255009416\n",
            "Iteration 4658 Loss = 0.052632341838594165\n",
            "Iteration 4659 Loss = 0.05262330619626007\n",
            "Iteration 4660 Loss = 0.052614275620249694\n",
            "Iteration 4661 Loss = 0.05260525010772207\n",
            "Iteration 4662 Loss = 0.05259622965583843\n",
            "Iteration 4663 Loss = 0.052587214261760995\n",
            "Iteration 4664 Loss = 0.05257820392265398\n",
            "Iteration 4665 Loss = 0.052569198635682844\n",
            "Iteration 4666 Loss = 0.05256019839801507\n",
            "Iteration 4667 Loss = 0.05255120320681928\n",
            "Iteration 4668 Loss = 0.05254221305926619\n",
            "Iteration 4669 Loss = 0.052533227952527545\n",
            "Iteration 4670 Loss = 0.052524247883777\n",
            "Iteration 4671 Loss = 0.05251527285018973\n",
            "Iteration 4672 Loss = 0.05250630284894248\n",
            "Iteration 4673 Loss = 0.052497337877213675\n",
            "Iteration 4674 Loss = 0.05248837793218326\n",
            "Iteration 4675 Loss = 0.052479423011032665\n",
            "Iteration 4676 Loss = 0.05247047311094496\n",
            "Iteration 4677 Loss = 0.05246152822910479\n",
            "Iteration 4678 Loss = 0.05245258836269856\n",
            "Iteration 4679 Loss = 0.052443653508913904\n",
            "Iteration 4680 Loss = 0.05243472366494052\n",
            "Iteration 4681 Loss = 0.052425798827968925\n",
            "Iteration 4682 Loss = 0.0524168789951921\n",
            "Iteration 4683 Loss = 0.05240796416380391\n",
            "Iteration 4684 Loss = 0.052399054331000165\n",
            "Iteration 4685 Loss = 0.052390149493978126\n",
            "Iteration 4686 Loss = 0.05238124964993659\n",
            "Iteration 4687 Loss = 0.05237235479607604\n",
            "Iteration 4688 Loss = 0.052363464929598355\n",
            "Iteration 4689 Loss = 0.052354580047707225\n",
            "Iteration 4690 Loss = 0.05234570014760773\n",
            "Iteration 4691 Loss = 0.052336825226506456\n",
            "Iteration 4692 Loss = 0.05232795528161176\n",
            "Iteration 4693 Loss = 0.05231909031013347\n",
            "Iteration 4694 Loss = 0.0523102303092829\n",
            "Iteration 4695 Loss = 0.052301375276272974\n",
            "Iteration 4696 Loss = 0.0522925252083184\n",
            "Iteration 4697 Loss = 0.05228368010263508\n",
            "Iteration 4698 Loss = 0.05227483995644068\n",
            "Iteration 4699 Loss = 0.05226600476695428\n",
            "Iteration 4700 Loss = 0.05225717453139688\n",
            "Iteration 4701 Loss = 0.052248349246990566\n",
            "Iteration 4702 Loss = 0.05223952891095928\n",
            "Iteration 4703 Loss = 0.052230713520528535\n",
            "Iteration 4704 Loss = 0.05222190307292534\n",
            "Iteration 4705 Loss = 0.052213097565377994\n",
            "Iteration 4706 Loss = 0.0522042969951168\n",
            "Iteration 4707 Loss = 0.05219550135937315\n",
            "Iteration 4708 Loss = 0.05218671065538054\n",
            "Iteration 4709 Loss = 0.05217792488037359\n",
            "Iteration 4710 Loss = 0.05216914403158863\n",
            "Iteration 4711 Loss = 0.05216036810626332\n",
            "Iteration 4712 Loss = 0.05215159710163728\n",
            "Iteration 4713 Loss = 0.05214283101495141\n",
            "Iteration 4714 Loss = 0.0521340698434481\n",
            "Iteration 4715 Loss = 0.05212531358437154\n",
            "Iteration 4716 Loss = 0.052116562234967106\n",
            "Iteration 4717 Loss = 0.05210781579248227\n",
            "Iteration 4718 Loss = 0.052099074254165274\n",
            "Iteration 4719 Loss = 0.052090337617266544\n",
            "Iteration 4720 Loss = 0.05208160587903787\n",
            "Iteration 4721 Loss = 0.05207287903673242\n",
            "Iteration 4722 Loss = 0.0520641570876052\n",
            "Iteration 4723 Loss = 0.05205544002891247\n",
            "Iteration 4724 Loss = 0.05204672785791202\n",
            "Iteration 4725 Loss = 0.05203802057186358\n",
            "Iteration 4726 Loss = 0.052029318168028076\n",
            "Iteration 4727 Loss = 0.052020620643667755\n",
            "Iteration 4728 Loss = 0.05201192799604692\n",
            "Iteration 4729 Loss = 0.05200324022243123\n",
            "Iteration 4730 Loss = 0.05199455732008763\n",
            "Iteration 4731 Loss = 0.05198587928628476\n",
            "Iteration 4732 Loss = 0.05197720611829291\n",
            "Iteration 4733 Loss = 0.05196853781338379\n",
            "Iteration 4734 Loss = 0.05195987436883071\n",
            "Iteration 4735 Loss = 0.051951215781908255\n",
            "Iteration 4736 Loss = 0.05194256204989292\n",
            "Iteration 4737 Loss = 0.05193391317006243\n",
            "Iteration 4738 Loss = 0.05192526913969617\n",
            "Iteration 4739 Loss = 0.05191662995607503\n",
            "Iteration 4740 Loss = 0.051907995616481455\n",
            "Iteration 4741 Loss = 0.05189936611819931\n",
            "Iteration 4742 Loss = 0.051890741458514085\n",
            "Iteration 4743 Loss = 0.051882121634712744\n",
            "Iteration 4744 Loss = 0.051873506644083855\n",
            "Iteration 4745 Loss = 0.051864896483917275\n",
            "Iteration 4746 Loss = 0.05185629115150476\n",
            "Iteration 4747 Loss = 0.05184769064413925\n",
            "Iteration 4748 Loss = 0.05183909495911526\n",
            "Iteration 4749 Loss = 0.05183050409372903\n",
            "Iteration 4750 Loss = 0.05182191804527793\n",
            "Iteration 4751 Loss = 0.051813336811061476\n",
            "Iteration 4752 Loss = 0.051804760388379875\n",
            "Iteration 4753 Loss = 0.051796188774535575\n",
            "Iteration 4754 Loss = 0.051787621966832\n",
            "Iteration 4755 Loss = 0.05177905996257463\n",
            "Iteration 4756 Loss = 0.05177050275906996\n",
            "Iteration 4757 Loss = 0.05176195035362611\n",
            "Iteration 4758 Loss = 0.051753402743553036\n",
            "Iteration 4759 Loss = 0.05174485992616175\n",
            "Iteration 4760 Loss = 0.05173632189876501\n",
            "Iteration 4761 Loss = 0.05172778865867714\n",
            "Iteration 4762 Loss = 0.051719260203213664\n",
            "Iteration 4763 Loss = 0.05171073652969204\n",
            "Iteration 4764 Loss = 0.05170221763543104\n",
            "Iteration 4765 Loss = 0.051693703517750704\n",
            "Iteration 4766 Loss = 0.05168519417397291\n",
            "Iteration 4767 Loss = 0.0516766896014208\n",
            "Iteration 4768 Loss = 0.05166818979741928\n",
            "Iteration 4769 Loss = 0.05165969475929459\n",
            "Iteration 4770 Loss = 0.05165120448437427\n",
            "Iteration 4771 Loss = 0.05164271896998795\n",
            "Iteration 4772 Loss = 0.05163423821346595\n",
            "Iteration 4773 Loss = 0.05162576221214092\n",
            "Iteration 4774 Loss = 0.051617290963346225\n",
            "Iteration 4775 Loss = 0.051608824464417315\n",
            "Iteration 4776 Loss = 0.051600362712690966\n",
            "Iteration 4777 Loss = 0.05159190570550528\n",
            "Iteration 4778 Loss = 0.051583453440199954\n",
            "Iteration 4779 Loss = 0.05157500591411645\n",
            "Iteration 4780 Loss = 0.051566563124596995\n",
            "Iteration 4781 Loss = 0.05155812506898624\n",
            "Iteration 4782 Loss = 0.051549691744629506\n",
            "Iteration 4783 Loss = 0.0515412631488743\n",
            "Iteration 4784 Loss = 0.051532839279069036\n",
            "Iteration 4785 Loss = 0.05152442013256388\n",
            "Iteration 4786 Loss = 0.051516005706710576\n",
            "Iteration 4787 Loss = 0.05150759599886199\n",
            "Iteration 4788 Loss = 0.051499191006373055\n",
            "Iteration 4789 Loss = 0.05149079072659951\n",
            "Iteration 4790 Loss = 0.05148239515689918\n",
            "Iteration 4791 Loss = 0.05147400429463098\n",
            "Iteration 4792 Loss = 0.05146561813715555\n",
            "Iteration 4793 Loss = 0.05145723668183476\n",
            "Iteration 4794 Loss = 0.051448859926032035\n",
            "Iteration 4795 Loss = 0.05144048786711248\n",
            "Iteration 4796 Loss = 0.051432120502442405\n",
            "Iteration 4797 Loss = 0.05142375782938999\n",
            "Iteration 4798 Loss = 0.05141539984532428\n",
            "Iteration 4799 Loss = 0.05140704654761635\n",
            "Iteration 4800 Loss = 0.051398697933638576\n",
            "Iteration 4801 Loss = 0.05139035400076451\n",
            "Iteration 4802 Loss = 0.05138201474636962\n",
            "Iteration 4803 Loss = 0.05137368016783078\n",
            "Iteration 4804 Loss = 0.05136535026252604\n",
            "Iteration 4805 Loss = 0.05135702502783518\n",
            "Iteration 4806 Loss = 0.051348704461139226\n",
            "Iteration 4807 Loss = 0.051340388559820954\n",
            "Iteration 4808 Loss = 0.05133207732126446\n",
            "Iteration 4809 Loss = 0.05132377074285529\n",
            "Iteration 4810 Loss = 0.051315468821980396\n",
            "Iteration 4811 Loss = 0.05130717155602844\n",
            "Iteration 4812 Loss = 0.05129887894238924\n",
            "Iteration 4813 Loss = 0.05129059097845435\n",
            "Iteration 4814 Loss = 0.05128230766161658\n",
            "Iteration 4815 Loss = 0.051274028989270276\n",
            "Iteration 4816 Loss = 0.05126575495881127\n",
            "Iteration 4817 Loss = 0.0512574855676369\n",
            "Iteration 4818 Loss = 0.05124922081314571\n",
            "Iteration 4819 Loss = 0.051240960692738154\n",
            "Iteration 4820 Loss = 0.05123270520381571\n",
            "Iteration 4821 Loss = 0.051224454343781485\n",
            "Iteration 4822 Loss = 0.05121620811004008\n",
            "Iteration 4823 Loss = 0.051207966499997586\n",
            "Iteration 4824 Loss = 0.051199729511061456\n",
            "Iteration 4825 Loss = 0.051191497140640446\n",
            "Iteration 4826 Loss = 0.051183269386145015\n",
            "Iteration 4827 Loss = 0.051175046244987116\n",
            "Iteration 4828 Loss = 0.05116682771457991\n",
            "Iteration 4829 Loss = 0.051158613792338194\n",
            "Iteration 4830 Loss = 0.05115040447567802\n",
            "Iteration 4831 Loss = 0.05114219976201719\n",
            "Iteration 4832 Loss = 0.05113399964877469\n",
            "Iteration 4833 Loss = 0.05112580413337117\n",
            "Iteration 4834 Loss = 0.05111761321322829\n",
            "Iteration 4835 Loss = 0.05110942688576981\n",
            "Iteration 4836 Loss = 0.0511012451484204\n",
            "Iteration 4837 Loss = 0.05109306799860647\n",
            "Iteration 4838 Loss = 0.05108489543375571\n",
            "Iteration 4839 Loss = 0.0510767274512973\n",
            "Iteration 4840 Loss = 0.05106856404866191\n",
            "Iteration 4841 Loss = 0.05106040522328154\n",
            "Iteration 4842 Loss = 0.05105225097258997\n",
            "Iteration 4843 Loss = 0.05104410129402171\n",
            "Iteration 4844 Loss = 0.05103595618501348\n",
            "Iteration 4845 Loss = 0.051027815643002995\n",
            "Iteration 4846 Loss = 0.051019679665429564\n",
            "Iteration 4847 Loss = 0.051011548249733835\n",
            "Iteration 4848 Loss = 0.051003421393358034\n",
            "Iteration 4849 Loss = 0.05099529909374558\n",
            "Iteration 4850 Loss = 0.0509871813483416\n",
            "Iteration 4851 Loss = 0.05097906815459254\n",
            "Iteration 4852 Loss = 0.050970959509946095\n",
            "Iteration 4853 Loss = 0.05096285541185183\n",
            "Iteration 4854 Loss = 0.05095475585776013\n",
            "Iteration 4855 Loss = 0.05094666084512356\n",
            "Iteration 4856 Loss = 0.05093857037139542\n",
            "Iteration 4857 Loss = 0.050930484434030796\n",
            "Iteration 4858 Loss = 0.05092240303048624\n",
            "Iteration 4859 Loss = 0.05091432615821934\n",
            "Iteration 4860 Loss = 0.050906253814689766\n",
            "Iteration 4861 Loss = 0.050898185997357875\n",
            "Iteration 4862 Loss = 0.05089012270368604\n",
            "Iteration 4863 Loss = 0.050882063931137735\n",
            "Iteration 4864 Loss = 0.050874009677178006\n",
            "Iteration 4865 Loss = 0.05086595993927333\n",
            "Iteration 4866 Loss = 0.05085791471489114\n",
            "Iteration 4867 Loss = 0.0508498740015011\n",
            "Iteration 4868 Loss = 0.05084183779657386\n",
            "Iteration 4869 Loss = 0.05083380609758137\n",
            "Iteration 4870 Loss = 0.05082577890199706\n",
            "Iteration 4871 Loss = 0.05081775620729595\n",
            "Iteration 4872 Loss = 0.05080973801095447\n",
            "Iteration 4873 Loss = 0.05080172431045015\n",
            "Iteration 4874 Loss = 0.05079371510326248\n",
            "Iteration 4875 Loss = 0.05078571038687162\n",
            "Iteration 4876 Loss = 0.05077771015875991\n",
            "Iteration 4877 Loss = 0.05076971441641057\n",
            "Iteration 4878 Loss = 0.050761723157308467\n",
            "Iteration 4879 Loss = 0.05075373637893977\n",
            "Iteration 4880 Loss = 0.0507457540787922\n",
            "Iteration 4881 Loss = 0.05073777625435479\n",
            "Iteration 4882 Loss = 0.050729802903117904\n",
            "Iteration 4883 Loss = 0.05072183402257355\n",
            "Iteration 4884 Loss = 0.05071386961021482\n",
            "Iteration 4885 Loss = 0.050705909663536476\n",
            "Iteration 4886 Loss = 0.05069795418003464\n",
            "Iteration 4887 Loss = 0.05069000315720662\n",
            "Iteration 4888 Loss = 0.05068205659255145\n",
            "Iteration 4889 Loss = 0.05067411448356942\n",
            "Iteration 4890 Loss = 0.05066617682776206\n",
            "Iteration 4891 Loss = 0.050658243622632645\n",
            "Iteration 4892 Loss = 0.05065031486568556\n",
            "Iteration 4893 Loss = 0.05064239055442671\n",
            "Iteration 4894 Loss = 0.050634470686363425\n",
            "Iteration 4895 Loss = 0.050626555259004134\n",
            "Iteration 4896 Loss = 0.05061864426985932\n",
            "Iteration 4897 Loss = 0.050610737716440195\n",
            "Iteration 4898 Loss = 0.050602835596259596\n",
            "Iteration 4899 Loss = 0.050594937906832026\n",
            "Iteration 4900 Loss = 0.05058704464567286\n",
            "Iteration 4901 Loss = 0.05057915581029935\n",
            "Iteration 4902 Loss = 0.05057127139822987\n",
            "Iteration 4903 Loss = 0.050563391406984355\n",
            "Iteration 4904 Loss = 0.050555515834083777\n",
            "Iteration 4905 Loss = 0.050547644677050856\n",
            "Iteration 4906 Loss = 0.05053977793340975\n",
            "Iteration 4907 Loss = 0.05053191560068573\n",
            "Iteration 4908 Loss = 0.05052405767640557\n",
            "Iteration 4909 Loss = 0.05051620415809758\n",
            "Iteration 4910 Loss = 0.05050835504329107\n",
            "Iteration 4911 Loss = 0.050500510329517065\n",
            "Iteration 4912 Loss = 0.050492670014308016\n",
            "Iteration 4913 Loss = 0.05048483409519737\n",
            "Iteration 4914 Loss = 0.050477002569720514\n",
            "Iteration 4915 Loss = 0.050469175435413814\n",
            "Iteration 4916 Loss = 0.05046135268981501\n",
            "Iteration 4917 Loss = 0.05045353433046339\n",
            "Iteration 4918 Loss = 0.05044572035489962\n",
            "Iteration 4919 Loss = 0.050437910760665644\n",
            "Iteration 4920 Loss = 0.05043010554530483\n",
            "Iteration 4921 Loss = 0.050422304706362046\n",
            "Iteration 4922 Loss = 0.05041450824138342\n",
            "Iteration 4923 Loss = 0.05040671614791608\n",
            "Iteration 4924 Loss = 0.05039892842350926\n",
            "Iteration 4925 Loss = 0.050391145065713176\n",
            "Iteration 4926 Loss = 0.05038336607207935\n",
            "Iteration 4927 Loss = 0.05037559144016081\n",
            "Iteration 4928 Loss = 0.050367821167511956\n",
            "Iteration 4929 Loss = 0.050360055251688594\n",
            "Iteration 4930 Loss = 0.05035229369024756\n",
            "Iteration 4931 Loss = 0.05034453648074745\n",
            "Iteration 4932 Loss = 0.0503367836207484\n",
            "Iteration 4933 Loss = 0.050329035107811275\n",
            "Iteration 4934 Loss = 0.05032129093949888\n",
            "Iteration 4935 Loss = 0.05031355111337494\n",
            "Iteration 4936 Loss = 0.05030581562700499\n",
            "Iteration 4937 Loss = 0.050298084477955605\n",
            "Iteration 4938 Loss = 0.050290357663794846\n",
            "Iteration 4939 Loss = 0.050282635182092174\n",
            "Iteration 4940 Loss = 0.05027491703041822\n",
            "Iteration 4941 Loss = 0.05026720320634531\n",
            "Iteration 4942 Loss = 0.05025949370744679\n",
            "Iteration 4943 Loss = 0.05025178853129755\n",
            "Iteration 4944 Loss = 0.05024408767547382\n",
            "Iteration 4945 Loss = 0.050236391137553196\n",
            "Iteration 4946 Loss = 0.05022869891511459\n",
            "Iteration 4947 Loss = 0.050221011005738274\n",
            "Iteration 4948 Loss = 0.050213327407005987\n",
            "Iteration 4949 Loss = 0.050205648116500474\n",
            "Iteration 4950 Loss = 0.05019797313180643\n",
            "Iteration 4951 Loss = 0.05019030245050936\n",
            "Iteration 4952 Loss = 0.05018263607019643\n",
            "Iteration 4953 Loss = 0.05017497398845599\n",
            "Iteration 4954 Loss = 0.050167316202877864\n",
            "Iteration 4955 Loss = 0.05015966271105322\n",
            "Iteration 4956 Loss = 0.05015201351057441\n",
            "Iteration 4957 Loss = 0.050144368599035274\n",
            "Iteration 4958 Loss = 0.05013672797403105\n",
            "Iteration 4959 Loss = 0.0501290916331583\n",
            "Iteration 4960 Loss = 0.05012145957401461\n",
            "Iteration 4961 Loss = 0.05011383179419961\n",
            "Iteration 4962 Loss = 0.05010620829131357\n",
            "Iteration 4963 Loss = 0.05009858906295856\n",
            "Iteration 4964 Loss = 0.050090974106737726\n",
            "Iteration 4965 Loss = 0.050083363420255704\n",
            "Iteration 4966 Loss = 0.05007575700111834\n",
            "Iteration 4967 Loss = 0.05006815484693307\n",
            "Iteration 4968 Loss = 0.05006055695530847\n",
            "Iteration 4969 Loss = 0.05005296332385454\n",
            "Iteration 4970 Loss = 0.05004537395018244\n",
            "Iteration 4971 Loss = 0.05003778883190485\n",
            "Iteration 4972 Loss = 0.050030207966635945\n",
            "Iteration 4973 Loss = 0.050022631351990775\n",
            "Iteration 4974 Loss = 0.05001505898558626\n",
            "Iteration 4975 Loss = 0.0500074908650401\n",
            "Iteration 4976 Loss = 0.04999992698797196\n",
            "Iteration 4977 Loss = 0.04999236735200227\n",
            "Iteration 4978 Loss = 0.04998481195475309\n",
            "Iteration 4979 Loss = 0.04997726079384777\n",
            "Iteration 4980 Loss = 0.049969713866910764\n",
            "Iteration 4981 Loss = 0.04996217117156845\n",
            "Iteration 4982 Loss = 0.04995463270544785\n",
            "Iteration 4983 Loss = 0.049947098466177814\n",
            "Iteration 4984 Loss = 0.049939568451388215\n",
            "Iteration 4985 Loss = 0.04993204265871036\n",
            "Iteration 4986 Loss = 0.04992452108577703\n",
            "Iteration 4987 Loss = 0.049917003730221926\n",
            "Iteration 4988 Loss = 0.04990949058968071\n",
            "Iteration 4989 Loss = 0.049901981661789656\n",
            "Iteration 4990 Loss = 0.04989447694418683\n",
            "Iteration 4991 Loss = 0.04988697643451162\n",
            "Iteration 4992 Loss = 0.04987948013040463\n",
            "Iteration 4993 Loss = 0.04987198802950755\n",
            "Iteration 4994 Loss = 0.049864500129463855\n",
            "Iteration 4995 Loss = 0.04985701642791809\n",
            "Iteration 4996 Loss = 0.049849536922516\n",
            "Iteration 4997 Loss = 0.04984206161090484\n",
            "Iteration 4998 Loss = 0.049834590490733216\n",
            "Iteration 4999 Loss = 0.049827123559650834\n",
            "Iteration 5000 Loss = 0.04981966081530893\n",
            "Iteration 5001 Loss = 0.04981220225535994\n",
            "Iteration 5002 Loss = 0.04980474787745775\n",
            "Iteration 5003 Loss = 0.04979729767925745\n",
            "Iteration 5004 Loss = 0.04978985165841533\n",
            "Iteration 5005 Loss = 0.049782409812589246\n",
            "Iteration 5006 Loss = 0.0497749721394383\n",
            "Iteration 5007 Loss = 0.0497675386366227\n",
            "Iteration 5008 Loss = 0.04976010930180416\n",
            "Iteration 5009 Loss = 0.04975268413264586\n",
            "Iteration 5010 Loss = 0.04974526312681185\n",
            "Iteration 5011 Loss = 0.04973784628196773\n",
            "Iteration 5012 Loss = 0.04973043359578069\n",
            "Iteration 5013 Loss = 0.049723025065918726\n",
            "Iteration 5014 Loss = 0.04971562069005146\n",
            "Iteration 5015 Loss = 0.04970822046584958\n",
            "Iteration 5016 Loss = 0.049700824390985346\n",
            "Iteration 5017 Loss = 0.0496934324631323\n",
            "Iteration 5018 Loss = 0.04968604467996506\n",
            "Iteration 5019 Loss = 0.049678661039159654\n",
            "Iteration 5020 Loss = 0.049671281538393626\n",
            "Iteration 5021 Loss = 0.04966390617534544\n",
            "Iteration 5022 Loss = 0.04965653494769508\n",
            "Iteration 5023 Loss = 0.04964916785312388\n",
            "Iteration 5024 Loss = 0.049641804889314474\n",
            "Iteration 5025 Loss = 0.049634446053950655\n",
            "Iteration 5026 Loss = 0.04962709134471745\n",
            "Iteration 5027 Loss = 0.049619740759301555\n",
            "Iteration 5028 Loss = 0.04961239429539055\n",
            "Iteration 5029 Loss = 0.049605051950673676\n",
            "Iteration 5030 Loss = 0.04959771372284104\n",
            "Iteration 5031 Loss = 0.04959037960958444\n",
            "Iteration 5032 Loss = 0.049583049608596795\n",
            "Iteration 5033 Loss = 0.04957572371757236\n",
            "Iteration 5034 Loss = 0.04956840193420647\n",
            "Iteration 5035 Loss = 0.04956108425619629\n",
            "Iteration 5036 Loss = 0.04955377068123971\n",
            "Iteration 5037 Loss = 0.0495464612070362\n",
            "Iteration 5038 Loss = 0.04953915583128635\n",
            "Iteration 5039 Loss = 0.04953185455169211\n",
            "Iteration 5040 Loss = 0.04952455736595698\n",
            "Iteration 5041 Loss = 0.04951726427178523\n",
            "Iteration 5042 Loss = 0.049509975266882965\n",
            "Iteration 5043 Loss = 0.04950269034895718\n",
            "Iteration 5044 Loss = 0.049495409515716335\n",
            "Iteration 5045 Loss = 0.04948813276487002\n",
            "Iteration 5046 Loss = 0.04948086009412934\n",
            "Iteration 5047 Loss = 0.04947359150120655\n",
            "Iteration 5048 Loss = 0.04946632698381513\n",
            "Iteration 5049 Loss = 0.04945906653967001\n",
            "Iteration 5050 Loss = 0.04945181016648717\n",
            "Iteration 5051 Loss = 0.04944455786198418\n",
            "Iteration 5052 Loss = 0.04943730962387958\n",
            "Iteration 5053 Loss = 0.04943006544989336\n",
            "Iteration 5054 Loss = 0.049422825337746744\n",
            "Iteration 5055 Loss = 0.04941558928516232\n",
            "Iteration 5056 Loss = 0.049408357289863605\n",
            "Iteration 5057 Loss = 0.049401129349576016\n",
            "Iteration 5058 Loss = 0.04939390546202559\n",
            "Iteration 5059 Loss = 0.049386685624940224\n",
            "Iteration 5060 Loss = 0.049379469836048566\n",
            "Iteration 5061 Loss = 0.04937225809308089\n",
            "Iteration 5062 Loss = 0.04936505039376867\n",
            "Iteration 5063 Loss = 0.04935784673584453\n",
            "Iteration 5064 Loss = 0.049350647117042436\n",
            "Iteration 5065 Loss = 0.049343451535097727\n",
            "Iteration 5066 Loss = 0.04933625998774688\n",
            "Iteration 5067 Loss = 0.049329072472727595\n",
            "Iteration 5068 Loss = 0.04932188898777888\n",
            "Iteration 5069 Loss = 0.04931470953064137\n",
            "Iteration 5070 Loss = 0.04930753409905633\n",
            "Iteration 5071 Loss = 0.0493003626907668\n",
            "Iteration 5072 Loss = 0.049293195303516835\n",
            "Iteration 5073 Loss = 0.04928603193505178\n",
            "Iteration 5074 Loss = 0.04927887258311829\n",
            "Iteration 5075 Loss = 0.04927171724546453\n",
            "Iteration 5076 Loss = 0.049264565919839305\n",
            "Iteration 5077 Loss = 0.0492574186039932\n",
            "Iteration 5078 Loss = 0.049250275295678006\n",
            "Iteration 5079 Loss = 0.04924313599264649\n",
            "Iteration 5080 Loss = 0.049236000692653124\n",
            "Iteration 5081 Loss = 0.04922886939345309\n",
            "Iteration 5082 Loss = 0.049221742092803465\n",
            "Iteration 5083 Loss = 0.049214618788462\n",
            "Iteration 5084 Loss = 0.04920749947818792\n",
            "Iteration 5085 Loss = 0.04920038415974193\n",
            "Iteration 5086 Loss = 0.04919327283088568\n",
            "Iteration 5087 Loss = 0.04918616548938215\n",
            "Iteration 5088 Loss = 0.049179062132995756\n",
            "Iteration 5089 Loss = 0.04917196275949186\n",
            "Iteration 5090 Loss = 0.0491648673666373\n",
            "Iteration 5091 Loss = 0.0491577759522002\n",
            "Iteration 5092 Loss = 0.04915068851394965\n",
            "Iteration 5093 Loss = 0.049143605049656404\n",
            "Iteration 5094 Loss = 0.04913652555709217\n",
            "Iteration 5095 Loss = 0.04912945003402994\n",
            "Iteration 5096 Loss = 0.049122378478244\n",
            "Iteration 5097 Loss = 0.049115310887509915\n",
            "Iteration 5098 Loss = 0.0491082472596044\n",
            "Iteration 5099 Loss = 0.04910118759230555\n",
            "Iteration 5100 Loss = 0.04909413188339257\n",
            "Iteration 5101 Loss = 0.04908708013064615\n",
            "Iteration 5102 Loss = 0.04908003233184782\n",
            "Iteration 5103 Loss = 0.04907298848478075\n",
            "Iteration 5104 Loss = 0.04906594858722901\n",
            "Iteration 5105 Loss = 0.04905891263697838\n",
            "Iteration 5106 Loss = 0.049051880631815245\n",
            "Iteration 5107 Loss = 0.04904485256952782\n",
            "Iteration 5108 Loss = 0.04903782844790519\n",
            "Iteration 5109 Loss = 0.04903080826473789\n",
            "Iteration 5110 Loss = 0.04902379201781765\n",
            "Iteration 5111 Loss = 0.0490167797049372\n",
            "Iteration 5112 Loss = 0.049009771323890915\n",
            "Iteration 5113 Loss = 0.04900276687247412\n",
            "Iteration 5114 Loss = 0.04899576634848344\n",
            "Iteration 5115 Loss = 0.048988769749716755\n",
            "Iteration 5116 Loss = 0.04898177707397325\n",
            "Iteration 5117 Loss = 0.04897478831905311\n",
            "Iteration 5118 Loss = 0.048967803482758115\n",
            "Iteration 5119 Loss = 0.04896082256289086\n",
            "Iteration 5120 Loss = 0.04895384555725554\n",
            "Iteration 5121 Loss = 0.04894687246365739\n",
            "Iteration 5122 Loss = 0.04893990327990288\n",
            "Iteration 5123 Loss = 0.04893293800379973\n",
            "Iteration 5124 Loss = 0.04892597663315712\n",
            "Iteration 5125 Loss = 0.04891901916578492\n",
            "Iteration 5126 Loss = 0.0489120655994947\n",
            "Iteration 5127 Loss = 0.048905115932099126\n",
            "Iteration 5128 Loss = 0.048898170161412034\n",
            "Iteration 5129 Loss = 0.04889122828524851\n",
            "Iteration 5130 Loss = 0.048884290301424854\n",
            "Iteration 5131 Loss = 0.048877356207758814\n",
            "Iteration 5132 Loss = 0.04887042600206892\n",
            "Iteration 5133 Loss = 0.04886349968217529\n",
            "Iteration 5134 Loss = 0.04885657724589913\n",
            "Iteration 5135 Loss = 0.04884965869106282\n",
            "Iteration 5136 Loss = 0.04884274401549002\n",
            "Iteration 5137 Loss = 0.04883583321700565\n",
            "Iteration 5138 Loss = 0.04882892629343582\n",
            "Iteration 5139 Loss = 0.04882202324260789\n",
            "Iteration 5140 Loss = 0.04881512406235031\n",
            "Iteration 5141 Loss = 0.04880822875049283\n",
            "Iteration 5142 Loss = 0.04880133730486655\n",
            "Iteration 5143 Loss = 0.04879444972330354\n",
            "Iteration 5144 Loss = 0.04878756600363708\n",
            "Iteration 5145 Loss = 0.04878068614370202\n",
            "Iteration 5146 Loss = 0.048773810141334265\n",
            "Iteration 5147 Loss = 0.04876693799437056\n",
            "Iteration 5148 Loss = 0.048760069700649285\n",
            "Iteration 5149 Loss = 0.04875320525801002\n",
            "Iteration 5150 Loss = 0.048746344664293305\n",
            "Iteration 5151 Loss = 0.0487394879173411\n",
            "Iteration 5152 Loss = 0.04873263501499654\n",
            "Iteration 5153 Loss = 0.048725785955103865\n",
            "Iteration 5154 Loss = 0.04871894073550858\n",
            "Iteration 5155 Loss = 0.0487120993540574\n",
            "Iteration 5156 Loss = 0.04870526180859841\n",
            "Iteration 5157 Loss = 0.04869842809698068\n",
            "Iteration 5158 Loss = 0.048691598217054544\n",
            "Iteration 5159 Loss = 0.048684772166671525\n",
            "Iteration 5160 Loss = 0.048677949943684386\n",
            "Iteration 5161 Loss = 0.048671131545947086\n",
            "Iteration 5162 Loss = 0.04866431697131485\n",
            "Iteration 5163 Loss = 0.04865750621764397\n",
            "Iteration 5164 Loss = 0.04865069928279219\n",
            "Iteration 5165 Loss = 0.04864389616461797\n",
            "Iteration 5166 Loss = 0.048637096860981534\n",
            "Iteration 5167 Loss = 0.04863030136974398\n",
            "Iteration 5168 Loss = 0.04862350968876776\n",
            "Iteration 5169 Loss = 0.04861672181591624\n",
            "Iteration 5170 Loss = 0.04860993774905443\n",
            "Iteration 5171 Loss = 0.048603157486048085\n",
            "Iteration 5172 Loss = 0.048596381024764525\n",
            "Iteration 5173 Loss = 0.048589608363072065\n",
            "Iteration 5174 Loss = 0.048582839498840236\n",
            "Iteration 5175 Loss = 0.04857607442993975\n",
            "Iteration 5176 Loss = 0.04856931315424252\n",
            "Iteration 5177 Loss = 0.0485625556696218\n",
            "Iteration 5178 Loss = 0.04855580197395182\n",
            "Iteration 5179 Loss = 0.04854905206510823\n",
            "Iteration 5180 Loss = 0.04854230594096769\n",
            "Iteration 5181 Loss = 0.048535563599407916\n",
            "Iteration 5182 Loss = 0.0485288250383081\n",
            "Iteration 5183 Loss = 0.04852209025554878\n",
            "Iteration 5184 Loss = 0.04851535924901101\n",
            "Iteration 5185 Loss = 0.048508632016577805\n",
            "Iteration 5186 Loss = 0.04850190855613286\n",
            "Iteration 5187 Loss = 0.048495188865561104\n",
            "Iteration 5188 Loss = 0.04848847294274884\n",
            "Iteration 5189 Loss = 0.04848176078558346\n",
            "Iteration 5190 Loss = 0.04847505239195369\n",
            "Iteration 5191 Loss = 0.04846834775974907\n",
            "Iteration 5192 Loss = 0.048461646886860596\n",
            "Iteration 5193 Loss = 0.04845494977118058\n",
            "Iteration 5194 Loss = 0.048448256410602214\n",
            "Iteration 5195 Loss = 0.048441566803019954\n",
            "Iteration 5196 Loss = 0.048434880946329494\n",
            "Iteration 5197 Loss = 0.04842819883842787\n",
            "Iteration 5198 Loss = 0.04842152047721282\n",
            "Iteration 5199 Loss = 0.0484148458605837\n",
            "Iteration 5200 Loss = 0.04840817498644101\n",
            "Iteration 5201 Loss = 0.04840150785268631\n",
            "Iteration 5202 Loss = 0.048394844457222114\n",
            "Iteration 5203 Loss = 0.04838818479795263\n",
            "Iteration 5204 Loss = 0.04838152887278278\n",
            "Iteration 5205 Loss = 0.048374876679618775\n",
            "Iteration 5206 Loss = 0.048368228216368346\n",
            "Iteration 5207 Loss = 0.04836158348093981\n",
            "Iteration 5208 Loss = 0.04835494247124322\n",
            "Iteration 5209 Loss = 0.04834830518518933\n",
            "Iteration 5210 Loss = 0.04834167162069045\n",
            "Iteration 5211 Loss = 0.04833504177565977\n",
            "Iteration 5212 Loss = 0.04832841564801174\n",
            "Iteration 5213 Loss = 0.04832179323566218\n",
            "Iteration 5214 Loss = 0.04831517453652773\n",
            "Iteration 5215 Loss = 0.048308559548526496\n",
            "Iteration 5216 Loss = 0.04830194826957749\n",
            "Iteration 5217 Loss = 0.048295340697601136\n",
            "Iteration 5218 Loss = 0.04828873683051895\n",
            "Iteration 5219 Loss = 0.04828213666625357\n",
            "Iteration 5220 Loss = 0.0482755402027286\n",
            "Iteration 5221 Loss = 0.04826894743786935\n",
            "Iteration 5222 Loss = 0.048262358369601764\n",
            "Iteration 5223 Loss = 0.04825577299585316\n",
            "Iteration 5224 Loss = 0.048249191314552096\n",
            "Iteration 5225 Loss = 0.04824261332362806\n",
            "Iteration 5226 Loss = 0.04823603902101186\n",
            "Iteration 5227 Loss = 0.048229468404635586\n",
            "Iteration 5228 Loss = 0.04822290147243219\n",
            "Iteration 5229 Loss = 0.04821633822233615\n",
            "Iteration 5230 Loss = 0.048209778652282695\n",
            "Iteration 5231 Loss = 0.048203222760208386\n",
            "Iteration 5232 Loss = 0.04819667054405113\n",
            "Iteration 5233 Loss = 0.048190122001749755\n",
            "Iteration 5234 Loss = 0.048183577131244305\n",
            "Iteration 5235 Loss = 0.04817703593047595\n",
            "Iteration 5236 Loss = 0.04817049839738705\n",
            "Iteration 5237 Loss = 0.048163964529921166\n",
            "Iteration 5238 Loss = 0.04815743432602309\n",
            "Iteration 5239 Loss = 0.04815090778363832\n",
            "Iteration 5240 Loss = 0.0481443849007142\n",
            "Iteration 5241 Loss = 0.04813786567519864\n",
            "Iteration 5242 Loss = 0.048131350105040965\n",
            "Iteration 5243 Loss = 0.04812483818819145\n",
            "Iteration 5244 Loss = 0.0481183299226019\n",
            "Iteration 5245 Loss = 0.04811182530622495\n",
            "Iteration 5246 Loss = 0.048105324337014504\n",
            "Iteration 5247 Loss = 0.048098827012925394\n",
            "Iteration 5248 Loss = 0.04809233333191415\n",
            "Iteration 5249 Loss = 0.04808584329193769\n",
            "Iteration 5250 Loss = 0.048079356890954765\n",
            "Iteration 5251 Loss = 0.04807287412692474\n",
            "Iteration 5252 Loss = 0.04806639499780864\n",
            "Iteration 5253 Loss = 0.048059919501568044\n",
            "Iteration 5254 Loss = 0.04805344763616616\n",
            "Iteration 5255 Loss = 0.04804697939956712\n",
            "Iteration 5256 Loss = 0.048040514789736276\n",
            "Iteration 5257 Loss = 0.0480340538046401\n",
            "Iteration 5258 Loss = 0.048027596442246044\n",
            "Iteration 5259 Loss = 0.048021142700523066\n",
            "Iteration 5260 Loss = 0.04801469257744083\n",
            "Iteration 5261 Loss = 0.0480082460709705\n",
            "Iteration 5262 Loss = 0.04800180317908411\n",
            "Iteration 5263 Loss = 0.04799536389975506\n",
            "Iteration 5264 Loss = 0.04798892823095758\n",
            "Iteration 5265 Loss = 0.047982496170667396\n",
            "Iteration 5266 Loss = 0.047976067716861304\n",
            "Iteration 5267 Loss = 0.047969642867516815\n",
            "Iteration 5268 Loss = 0.04796322162061319\n",
            "Iteration 5269 Loss = 0.04795680397413041\n",
            "Iteration 5270 Loss = 0.04795038992604966\n",
            "Iteration 5271 Loss = 0.0479439794743533\n",
            "Iteration 5272 Loss = 0.047937572617024984\n",
            "Iteration 5273 Loss = 0.04793116935204914\n",
            "Iteration 5274 Loss = 0.04792476967741159\n",
            "Iteration 5275 Loss = 0.04791837359109931\n",
            "Iteration 5276 Loss = 0.047911981091100043\n",
            "Iteration 5277 Loss = 0.04790559217540334\n",
            "Iteration 5278 Loss = 0.047899206841999116\n",
            "Iteration 5279 Loss = 0.047892825088878786\n",
            "Iteration 5280 Loss = 0.04788644691403507\n",
            "Iteration 5281 Loss = 0.04788007231546146\n",
            "Iteration 5282 Loss = 0.04787370129115281\n",
            "Iteration 5283 Loss = 0.04786733383910506\n",
            "Iteration 5284 Loss = 0.04786096995731498\n",
            "Iteration 5285 Loss = 0.04785460964378096\n",
            "Iteration 5286 Loss = 0.04784825289650226\n",
            "Iteration 5287 Loss = 0.047841899713479005\n",
            "Iteration 5288 Loss = 0.04783555009271303\n",
            "Iteration 5289 Loss = 0.04782920403220673\n",
            "Iteration 5290 Loss = 0.047822861529963984\n",
            "Iteration 5291 Loss = 0.047816522583989704\n",
            "Iteration 5292 Loss = 0.04781018719228956\n",
            "Iteration 5293 Loss = 0.047803855352870954\n",
            "Iteration 5294 Loss = 0.04779752706374211\n",
            "Iteration 5295 Loss = 0.04779120232291219\n",
            "Iteration 5296 Loss = 0.047784881128391896\n",
            "Iteration 5297 Loss = 0.047778563478192385\n",
            "Iteration 5298 Loss = 0.04777224937032688\n",
            "Iteration 5299 Loss = 0.047765938802808695\n",
            "Iteration 5300 Loss = 0.047759631773653034\n",
            "Iteration 5301 Loss = 0.04775332828087583\n",
            "Iteration 5302 Loss = 0.0477470283224942\n",
            "Iteration 5303 Loss = 0.04774073189652641\n",
            "Iteration 5304 Loss = 0.04773443900099184\n",
            "Iteration 5305 Loss = 0.04772814963391092\n",
            "Iteration 5306 Loss = 0.04772186379330534\n",
            "Iteration 5307 Loss = 0.04771558147719772\n",
            "Iteration 5308 Loss = 0.04770930268361173\n",
            "Iteration 5309 Loss = 0.0477030274105724\n",
            "Iteration 5310 Loss = 0.04769675565610578\n",
            "Iteration 5311 Loss = 0.04769048741823901\n",
            "Iteration 5312 Loss = 0.04768422269500008\n",
            "Iteration 5313 Loss = 0.04767796148441871\n",
            "Iteration 5314 Loss = 0.047671703784524955\n",
            "Iteration 5315 Loss = 0.04766544959335061\n",
            "Iteration 5316 Loss = 0.04765919890892824\n",
            "Iteration 5317 Loss = 0.04765295172929157\n",
            "Iteration 5318 Loss = 0.04764670805247555\n",
            "Iteration 5319 Loss = 0.04764046787651595\n",
            "Iteration 5320 Loss = 0.04763423119945001\n",
            "Iteration 5321 Loss = 0.04762799801931579\n",
            "Iteration 5322 Loss = 0.047621768334152645\n",
            "Iteration 5323 Loss = 0.04761554214200069\n",
            "Iteration 5324 Loss = 0.04760931944090168\n",
            "Iteration 5325 Loss = 0.04760310022889784\n",
            "Iteration 5326 Loss = 0.04759688450403328\n",
            "Iteration 5327 Loss = 0.04759067226435225\n",
            "Iteration 5328 Loss = 0.04758446350790084\n",
            "Iteration 5329 Loss = 0.04757825823272601\n",
            "Iteration 5330 Loss = 0.047572056436875704\n",
            "Iteration 5331 Loss = 0.047565858118399175\n",
            "Iteration 5332 Loss = 0.047559663275346484\n",
            "Iteration 5333 Loss = 0.04755347190576906\n",
            "Iteration 5334 Loss = 0.04754728400771917\n",
            "Iteration 5335 Loss = 0.04754109957925047\n",
            "Iteration 5336 Loss = 0.04753491861841753\n",
            "Iteration 5337 Loss = 0.047528741123276035\n",
            "Iteration 5338 Loss = 0.04752256709188264\n",
            "Iteration 5339 Loss = 0.04751639652229531\n",
            "Iteration 5340 Loss = 0.04751022941257291\n",
            "Iteration 5341 Loss = 0.047504065760775586\n",
            "Iteration 5342 Loss = 0.0474979055649645\n",
            "Iteration 5343 Loss = 0.04749174882320163\n",
            "Iteration 5344 Loss = 0.04748559553355056\n",
            "Iteration 5345 Loss = 0.047479445694075466\n",
            "Iteration 5346 Loss = 0.04747329930284195\n",
            "Iteration 5347 Loss = 0.04746715635791656\n",
            "Iteration 5348 Loss = 0.04746101685736682\n",
            "Iteration 5349 Loss = 0.04745488079926148\n",
            "Iteration 5350 Loss = 0.04744874818167051\n",
            "Iteration 5351 Loss = 0.04744261900266461\n",
            "Iteration 5352 Loss = 0.04743649326031594\n",
            "Iteration 5353 Loss = 0.047430370952697205\n",
            "Iteration 5354 Loss = 0.04742425207788294\n",
            "Iteration 5355 Loss = 0.047418136633948185\n",
            "Iteration 5356 Loss = 0.047412024618969116\n",
            "Iteration 5357 Loss = 0.04740591603102337\n",
            "Iteration 5358 Loss = 0.0473998108681891\n",
            "Iteration 5359 Loss = 0.04739370912854609\n",
            "Iteration 5360 Loss = 0.047387610810174806\n",
            "Iteration 5361 Loss = 0.04738151591115684\n",
            "Iteration 5362 Loss = 0.04737542442957509\n",
            "Iteration 5363 Loss = 0.047369336363513356\n",
            "Iteration 5364 Loss = 0.04736325171105663\n",
            "Iteration 5365 Loss = 0.0473571704702907\n",
            "Iteration 5366 Loss = 0.047351092639302664\n",
            "Iteration 5367 Loss = 0.04734501821618068\n",
            "Iteration 5368 Loss = 0.04733894719901405\n",
            "Iteration 5369 Loss = 0.04733287958589295\n",
            "Iteration 5370 Loss = 0.04732681537490862\n",
            "Iteration 5371 Loss = 0.04732075456415353\n",
            "Iteration 5372 Loss = 0.047314697151721215\n",
            "Iteration 5373 Loss = 0.04730864313570623\n",
            "Iteration 5374 Loss = 0.04730259251420404\n",
            "Iteration 5375 Loss = 0.047296545285311534\n",
            "Iteration 5376 Loss = 0.04729050144712635\n",
            "Iteration 5377 Loss = 0.047284460997747337\n",
            "Iteration 5378 Loss = 0.047278423935274226\n",
            "Iteration 5379 Loss = 0.04727239025780819\n",
            "Iteration 5380 Loss = 0.04726635996345115\n",
            "Iteration 5381 Loss = 0.04726033305030612\n",
            "Iteration 5382 Loss = 0.04725430951647737\n",
            "Iteration 5383 Loss = 0.0472482893600699\n",
            "Iteration 5384 Loss = 0.0472422725791902\n",
            "Iteration 5385 Loss = 0.04723625917194545\n",
            "Iteration 5386 Loss = 0.047230249136444094\n",
            "Iteration 5387 Loss = 0.04722424247079565\n",
            "Iteration 5388 Loss = 0.047218239173110514\n",
            "Iteration 5389 Loss = 0.04721223924150029\n",
            "Iteration 5390 Loss = 0.04720624267407756\n",
            "Iteration 5391 Loss = 0.04720024946895607\n",
            "Iteration 5392 Loss = 0.047194259624250584\n",
            "Iteration 5393 Loss = 0.047188273138076844\n",
            "Iteration 5394 Loss = 0.04718229000855169\n",
            "Iteration 5395 Loss = 0.04717631023379315\n",
            "Iteration 5396 Loss = 0.04717033381192004\n",
            "Iteration 5397 Loss = 0.047164360741052555\n",
            "Iteration 5398 Loss = 0.04715839101931163\n",
            "Iteration 5399 Loss = 0.0471524246448194\n",
            "Iteration 5400 Loss = 0.04714646161569921\n",
            "Iteration 5401 Loss = 0.047140501930075115\n",
            "Iteration 5402 Loss = 0.047134545586072385\n",
            "Iteration 5403 Loss = 0.047128592581817556\n",
            "Iteration 5404 Loss = 0.04712264291543776\n",
            "Iteration 5405 Loss = 0.04711669658506173\n",
            "Iteration 5406 Loss = 0.04711075358881871\n",
            "Iteration 5407 Loss = 0.04710481392483931\n",
            "Iteration 5408 Loss = 0.04709887759125512\n",
            "Iteration 5409 Loss = 0.047092944586198886\n",
            "Iteration 5410 Loss = 0.04708701490780401\n",
            "Iteration 5411 Loss = 0.04708108855420553\n",
            "Iteration 5412 Loss = 0.04707516552353906\n",
            "Iteration 5413 Loss = 0.04706924581394137\n",
            "Iteration 5414 Loss = 0.04706332942355045\n",
            "Iteration 5415 Loss = 0.0470574163505052\n",
            "Iteration 5416 Loss = 0.047051506592945436\n",
            "Iteration 5417 Loss = 0.04704560014901225\n",
            "Iteration 5418 Loss = 0.047039697016847716\n",
            "Iteration 5419 Loss = 0.04703379719459484\n",
            "Iteration 5420 Loss = 0.04702790068039772\n",
            "Iteration 5421 Loss = 0.047022007472401574\n",
            "Iteration 5422 Loss = 0.04701611756875249\n",
            "Iteration 5423 Loss = 0.04701023096759787\n",
            "Iteration 5424 Loss = 0.04700434766708588\n",
            "Iteration 5425 Loss = 0.046998467665365966\n",
            "Iteration 5426 Loss = 0.046992590960588325\n",
            "Iteration 5427 Loss = 0.046986717550904424\n",
            "Iteration 5428 Loss = 0.04698084743446669\n",
            "Iteration 5429 Loss = 0.04697498060942855\n",
            "Iteration 5430 Loss = 0.04696911707394449\n",
            "Iteration 5431 Loss = 0.04696325682617014\n",
            "Iteration 5432 Loss = 0.046957399864261846\n",
            "Iteration 5433 Loss = 0.046951546186377514\n",
            "Iteration 5434 Loss = 0.04694569579067555\n",
            "Iteration 5435 Loss = 0.04693984867531576\n",
            "Iteration 5436 Loss = 0.046934004838458804\n",
            "Iteration 5437 Loss = 0.04692816427826633\n",
            "Iteration 5438 Loss = 0.04692232699290115\n",
            "Iteration 5439 Loss = 0.046916492980527094\n",
            "Iteration 5440 Loss = 0.046910662239308946\n",
            "Iteration 5441 Loss = 0.0469048347674125\n",
            "Iteration 5442 Loss = 0.04689901056300468\n",
            "Iteration 5443 Loss = 0.0468931896242535\n",
            "Iteration 5444 Loss = 0.046887371949327766\n",
            "Iteration 5445 Loss = 0.046881557536397424\n",
            "Iteration 5446 Loss = 0.046875746383633565\n",
            "Iteration 5447 Loss = 0.04686993848920809\n",
            "Iteration 5448 Loss = 0.04686413385129403\n",
            "Iteration 5449 Loss = 0.046858332468065485\n",
            "Iteration 5450 Loss = 0.04685253433769763\n",
            "Iteration 5451 Loss = 0.04684673945836644\n",
            "Iteration 5452 Loss = 0.046840947828249185\n",
            "Iteration 5453 Loss = 0.04683515944552369\n",
            "Iteration 5454 Loss = 0.0468293743083696\n",
            "Iteration 5455 Loss = 0.04682359241496678\n",
            "Iteration 5456 Loss = 0.04681781376349653\n",
            "Iteration 5457 Loss = 0.04681203835214108\n",
            "Iteration 5458 Loss = 0.04680626617908379\n",
            "Iteration 5459 Loss = 0.04680049724250868\n",
            "Iteration 5460 Loss = 0.04679473154060124\n",
            "Iteration 5461 Loss = 0.04678896907154781\n",
            "Iteration 5462 Loss = 0.046783209833535656\n",
            "Iteration 5463 Loss = 0.046777453824753054\n",
            "Iteration 5464 Loss = 0.04677170104338943\n",
            "Iteration 5465 Loss = 0.046765951487635175\n",
            "Iteration 5466 Loss = 0.04676020515568147\n",
            "Iteration 5467 Loss = 0.04675446204572103\n",
            "Iteration 5468 Loss = 0.046748722155947144\n",
            "Iteration 5469 Loss = 0.046742985484554106\n",
            "Iteration 5470 Loss = 0.04673725202973763\n",
            "Iteration 5471 Loss = 0.046731521789693906\n",
            "Iteration 5472 Loss = 0.04672579476262047\n",
            "Iteration 5473 Loss = 0.046720070946715805\n",
            "Iteration 5474 Loss = 0.046714350340179456\n",
            "Iteration 5475 Loss = 0.046708632941211886\n",
            "Iteration 5476 Loss = 0.04670291874801457\n",
            "Iteration 5477 Loss = 0.046697207758790056\n",
            "Iteration 5478 Loss = 0.046691499971741816\n",
            "Iteration 5479 Loss = 0.046685795385074516\n",
            "Iteration 5480 Loss = 0.046680093996993396\n",
            "Iteration 5481 Loss = 0.046674395805705315\n",
            "Iteration 5482 Loss = 0.04666870080941758\n",
            "Iteration 5483 Loss = 0.046663009006338844\n",
            "Iteration 5484 Loss = 0.04665732039467868\n",
            "Iteration 5485 Loss = 0.046651634972647585\n",
            "Iteration 5486 Loss = 0.04664595273845723\n",
            "Iteration 5487 Loss = 0.04664027369031999\n",
            "Iteration 5488 Loss = 0.046634597826449624\n",
            "Iteration 5489 Loss = 0.04662892514506048\n",
            "Iteration 5490 Loss = 0.04662325564436854\n",
            "Iteration 5491 Loss = 0.04661758932258978\n",
            "Iteration 5492 Loss = 0.04661192617794233\n",
            "Iteration 5493 Loss = 0.04660626620864448\n",
            "Iteration 5494 Loss = 0.04660060941291582\n",
            "Iteration 5495 Loss = 0.04659495578897678\n",
            "Iteration 5496 Loss = 0.04658930533504928\n",
            "Iteration 5497 Loss = 0.04658365804935553\n",
            "Iteration 5498 Loss = 0.046578013930119275\n",
            "Iteration 5499 Loss = 0.0465723729755651\n",
            "Iteration 5500 Loss = 0.046566735183918476\n",
            "Iteration 5501 Loss = 0.046561100553405836\n",
            "Iteration 5502 Loss = 0.0465554690822549\n",
            "Iteration 5503 Loss = 0.04654984076869416\n",
            "Iteration 5504 Loss = 0.04654421561095316\n",
            "Iteration 5505 Loss = 0.04653859360726224\n",
            "Iteration 5506 Loss = 0.04653297475585309\n",
            "Iteration 5507 Loss = 0.0465273590549582\n",
            "Iteration 5508 Loss = 0.046521746502811115\n",
            "Iteration 5509 Loss = 0.046516137097646126\n",
            "Iteration 5510 Loss = 0.046510530837698766\n",
            "Iteration 5511 Loss = 0.04650492772120562\n",
            "Iteration 5512 Loss = 0.04649932774640409\n",
            "Iteration 5513 Loss = 0.04649373091153258\n",
            "Iteration 5514 Loss = 0.04648813721483052\n",
            "Iteration 5515 Loss = 0.0464825466545384\n",
            "Iteration 5516 Loss = 0.04647695922889752\n",
            "Iteration 5517 Loss = 0.04647137493615017\n",
            "Iteration 5518 Loss = 0.04646579377453996\n",
            "Iteration 5519 Loss = 0.04646021574231116\n",
            "Iteration 5520 Loss = 0.04645464083770911\n",
            "Iteration 5521 Loss = 0.046449069058980136\n",
            "Iteration 5522 Loss = 0.0464435004043715\n",
            "Iteration 5523 Loss = 0.04643793487213147\n",
            "Iteration 5524 Loss = 0.04643237246050942\n",
            "Iteration 5525 Loss = 0.04642681316775565\n",
            "Iteration 5526 Loss = 0.04642125699212117\n",
            "Iteration 5527 Loss = 0.0464157039318584\n",
            "Iteration 5528 Loss = 0.04641015398522044\n",
            "Iteration 5529 Loss = 0.046404607150461574\n",
            "Iteration 5530 Loss = 0.04639906342583686\n",
            "Iteration 5531 Loss = 0.04639352280960249\n",
            "Iteration 5532 Loss = 0.046387985300015616\n",
            "Iteration 5533 Loss = 0.0463824508953341\n",
            "Iteration 5534 Loss = 0.046376919593817255\n",
            "Iteration 5535 Loss = 0.04637139139372502\n",
            "Iteration 5536 Loss = 0.04636586629331847\n",
            "Iteration 5537 Loss = 0.046360344290859536\n",
            "Iteration 5538 Loss = 0.04635482538461113\n",
            "Iteration 5539 Loss = 0.04634930957283726\n",
            "Iteration 5540 Loss = 0.046343796853802965\n",
            "Iteration 5541 Loss = 0.046338287225773926\n",
            "Iteration 5542 Loss = 0.046332780687017225\n",
            "Iteration 5543 Loss = 0.04632727723580043\n",
            "Iteration 5544 Loss = 0.046321776870392496\n",
            "Iteration 5545 Loss = 0.04631627958906323\n",
            "Iteration 5546 Loss = 0.04631078539008335\n",
            "Iteration 5547 Loss = 0.04630529427172449\n",
            "Iteration 5548 Loss = 0.046299806232259266\n",
            "Iteration 5549 Loss = 0.04629432126996166\n",
            "Iteration 5550 Loss = 0.04628883938310594\n",
            "Iteration 5551 Loss = 0.046283360569967846\n",
            "Iteration 5552 Loss = 0.04627788482882399\n",
            "Iteration 5553 Loss = 0.04627241215795182\n",
            "Iteration 5554 Loss = 0.04626694255562981\n",
            "Iteration 5555 Loss = 0.04626147602013739\n",
            "Iteration 5556 Loss = 0.046256012549754914\n",
            "Iteration 5557 Loss = 0.046250552142763975\n",
            "Iteration 5558 Loss = 0.046245094797446686\n",
            "Iteration 5559 Loss = 0.046239640512086615\n",
            "Iteration 5560 Loss = 0.04623418928496775\n",
            "Iteration 5561 Loss = 0.04622874111437559\n",
            "Iteration 5562 Loss = 0.04622329599859607\n",
            "Iteration 5563 Loss = 0.046217853935916486\n",
            "Iteration 5564 Loss = 0.04621241492462494\n",
            "Iteration 5565 Loss = 0.04620697896301052\n",
            "Iteration 5566 Loss = 0.04620154604936313\n",
            "Iteration 5567 Loss = 0.046196116181973924\n",
            "Iteration 5568 Loss = 0.046190689359134854\n",
            "Iteration 5569 Loss = 0.04618526557913867\n",
            "Iteration 5570 Loss = 0.04617984484027936\n",
            "Iteration 5571 Loss = 0.04617442714085171\n",
            "Iteration 5572 Loss = 0.04616901247915158\n",
            "Iteration 5573 Loss = 0.046163600853475574\n",
            "Iteration 5574 Loss = 0.04615819226212138\n",
            "Iteration 5575 Loss = 0.04615278670338774\n",
            "Iteration 5576 Loss = 0.0461473841755741\n",
            "Iteration 5577 Loss = 0.04614198467698124\n",
            "Iteration 5578 Loss = 0.04613658820591048\n",
            "Iteration 5579 Loss = 0.0461311947606644\n",
            "Iteration 5580 Loss = 0.04612580433954632\n",
            "Iteration 5581 Loss = 0.04612041694086065\n",
            "Iteration 5582 Loss = 0.04611503256291257\n",
            "Iteration 5583 Loss = 0.04610965120400849\n",
            "Iteration 5584 Loss = 0.04610427286245555\n",
            "Iteration 5585 Loss = 0.04609889753656195\n",
            "Iteration 5586 Loss = 0.04609352522463678\n",
            "Iteration 5587 Loss = 0.046088155924990055\n",
            "Iteration 5588 Loss = 0.046082789635932736\n",
            "Iteration 5589 Loss = 0.04607742635577696\n",
            "Iteration 5590 Loss = 0.046072066082835525\n",
            "Iteration 5591 Loss = 0.046066708815422214\n",
            "Iteration 5592 Loss = 0.046061354551851744\n",
            "Iteration 5593 Loss = 0.04605600329044012\n",
            "Iteration 5594 Loss = 0.046050655029503826\n",
            "Iteration 5595 Loss = 0.046045309767360466\n",
            "Iteration 5596 Loss = 0.04603996750232868\n",
            "Iteration 5597 Loss = 0.04603462823272785\n",
            "Iteration 5598 Loss = 0.04602929195687866\n",
            "Iteration 5599 Loss = 0.046023958673102244\n",
            "Iteration 5600 Loss = 0.046018628379721156\n",
            "Iteration 5601 Loss = 0.0460133010750585\n",
            "Iteration 5602 Loss = 0.04600797675743863\n",
            "Iteration 5603 Loss = 0.0460026554251866\n",
            "Iteration 5604 Loss = 0.04599733707662849\n",
            "Iteration 5605 Loss = 0.04599202171009135\n",
            "Iteration 5606 Loss = 0.04598670932390317\n",
            "Iteration 5607 Loss = 0.04598139991639291\n",
            "Iteration 5608 Loss = 0.04597609348589019\n",
            "Iteration 5609 Loss = 0.04597079003072609\n",
            "Iteration 5610 Loss = 0.04596548954923212\n",
            "Iteration 5611 Loss = 0.045960192039741\n",
            "Iteration 5612 Loss = 0.04595489750058631\n",
            "Iteration 5613 Loss = 0.04594960593010259\n",
            "Iteration 5614 Loss = 0.045944317326625315\n",
            "Iteration 5615 Loss = 0.04593903168849073\n",
            "Iteration 5616 Loss = 0.045933749014036254\n",
            "Iteration 5617 Loss = 0.04592846930160012\n",
            "Iteration 5618 Loss = 0.045923192549521694\n",
            "Iteration 5619 Loss = 0.045917918756140784\n",
            "Iteration 5620 Loss = 0.045912647919798655\n",
            "Iteration 5621 Loss = 0.04590738003883717\n",
            "Iteration 5622 Loss = 0.04590211511159922\n",
            "Iteration 5623 Loss = 0.045896853136428745\n",
            "Iteration 5624 Loss = 0.045891594111670424\n",
            "Iteration 5625 Loss = 0.04588633803566993\n",
            "Iteration 5626 Loss = 0.04588108490677401\n",
            "Iteration 5627 Loss = 0.04587583472333008\n",
            "Iteration 5628 Loss = 0.04587058748368664\n",
            "Iteration 5629 Loss = 0.04586534318619329\n",
            "Iteration 5630 Loss = 0.04586010182919995\n",
            "Iteration 5631 Loss = 0.045854863411058176\n",
            "Iteration 5632 Loss = 0.045849627930120034\n",
            "Iteration 5633 Loss = 0.04584439538473867\n",
            "Iteration 5634 Loss = 0.045839165773268055\n",
            "Iteration 5635 Loss = 0.045833939094063185\n",
            "Iteration 5636 Loss = 0.0458287153454799\n",
            "Iteration 5637 Loss = 0.04582349452587485\n",
            "Iteration 5638 Loss = 0.04581827663360605\n",
            "Iteration 5639 Loss = 0.045813061667031814\n",
            "Iteration 5640 Loss = 0.04580784962451193\n",
            "Iteration 5641 Loss = 0.045802640504406744\n",
            "Iteration 5642 Loss = 0.04579743430507769\n",
            "Iteration 5643 Loss = 0.045792231024887074\n",
            "Iteration 5644 Loss = 0.04578703066219816\n",
            "Iteration 5645 Loss = 0.04578183321537499\n",
            "Iteration 5646 Loss = 0.04577663868278262\n",
            "Iteration 5647 Loss = 0.045771447062787136\n",
            "Iteration 5648 Loss = 0.04576625835375542\n",
            "Iteration 5649 Loss = 0.04576107255405519\n",
            "Iteration 5650 Loss = 0.045755889662055155\n",
            "Iteration 5651 Loss = 0.045750709676125184\n",
            "Iteration 5652 Loss = 0.04574553259463559\n",
            "Iteration 5653 Loss = 0.045740358415957874\n",
            "Iteration 5654 Loss = 0.045735187138464556\n",
            "Iteration 5655 Loss = 0.04573001876052883\n",
            "Iteration 5656 Loss = 0.045724853280524905\n",
            "Iteration 5657 Loss = 0.04571969069682791\n",
            "Iteration 5658 Loss = 0.045714531007813856\n",
            "Iteration 5659 Loss = 0.045709374211859645\n",
            "Iteration 5660 Loss = 0.045704220307343266\n",
            "Iteration 5661 Loss = 0.04569906929264337\n",
            "Iteration 5662 Loss = 0.04569392116613956\n",
            "Iteration 5663 Loss = 0.045688775926212545\n",
            "Iteration 5664 Loss = 0.04568363357124375\n",
            "Iteration 5665 Loss = 0.045678494099615545\n",
            "Iteration 5666 Loss = 0.045673357509711326\n",
            "Iteration 5667 Loss = 0.04566822379991516\n",
            "Iteration 5668 Loss = 0.04566309296861224\n",
            "Iteration 5669 Loss = 0.04565796501418865\n",
            "Iteration 5670 Loss = 0.04565283993503105\n",
            "Iteration 5671 Loss = 0.04564771772952761\n",
            "Iteration 5672 Loss = 0.04564259839606682\n",
            "Iteration 5673 Loss = 0.045637481933038376\n",
            "Iteration 5674 Loss = 0.04563236833883288\n",
            "Iteration 5675 Loss = 0.045627257611841776\n",
            "Iteration 5676 Loss = 0.04562214975045727\n",
            "Iteration 5677 Loss = 0.04561704475307271\n",
            "Iteration 5678 Loss = 0.04561194261808218\n",
            "Iteration 5679 Loss = 0.04560684334388075\n",
            "Iteration 5680 Loss = 0.04560174692886439\n",
            "Iteration 5681 Loss = 0.045596653371429846\n",
            "Iteration 5682 Loss = 0.04559156266997502\n",
            "Iteration 5683 Loss = 0.04558647482289845\n",
            "Iteration 5684 Loss = 0.045581389828599654\n",
            "Iteration 5685 Loss = 0.04557630768547906\n",
            "Iteration 5686 Loss = 0.04557122839193811\n",
            "Iteration 5687 Loss = 0.045566151946378995\n",
            "Iteration 5688 Loss = 0.04556107834720472\n",
            "Iteration 5689 Loss = 0.04555600759281949\n",
            "Iteration 5690 Loss = 0.045550939681628086\n",
            "Iteration 5691 Loss = 0.04554587461203632\n",
            "Iteration 5692 Loss = 0.04554081238245091\n",
            "Iteration 5693 Loss = 0.04553575299127957\n",
            "Iteration 5694 Loss = 0.04553069643693052\n",
            "Iteration 5695 Loss = 0.04552564271781342\n",
            "Iteration 5696 Loss = 0.04552059183233839\n",
            "Iteration 5697 Loss = 0.045515543778916735\n",
            "Iteration 5698 Loss = 0.04551049855596033\n",
            "Iteration 5699 Loss = 0.04550545616188228\n",
            "Iteration 5700 Loss = 0.04550041659509623\n",
            "Iteration 5701 Loss = 0.045495379854017116\n",
            "Iteration 5702 Loss = 0.045490345937060465\n",
            "Iteration 5703 Loss = 0.045485314842642635\n",
            "Iteration 5704 Loss = 0.04548028656918131\n",
            "Iteration 5705 Loss = 0.04547526111509448\n",
            "Iteration 5706 Loss = 0.045470238478801564\n",
            "Iteration 5707 Loss = 0.04546521865872242\n",
            "Iteration 5708 Loss = 0.045460201653278\n",
            "Iteration 5709 Loss = 0.045455187460890265\n",
            "Iteration 5710 Loss = 0.04545017607998172\n",
            "Iteration 5711 Loss = 0.04544516750897623\n",
            "Iteration 5712 Loss = 0.045440161746297975\n",
            "Iteration 5713 Loss = 0.0454351587903726\n",
            "Iteration 5714 Loss = 0.04543015863962612\n",
            "Iteration 5715 Loss = 0.045425161292485715\n",
            "Iteration 5716 Loss = 0.04542016674737947\n",
            "Iteration 5717 Loss = 0.04541517500273608\n",
            "Iteration 5718 Loss = 0.04541018605698559\n",
            "Iteration 5719 Loss = 0.04540519990855848\n",
            "Iteration 5720 Loss = 0.04540021655588627\n",
            "Iteration 5721 Loss = 0.04539523599740141\n",
            "Iteration 5722 Loss = 0.04539025823153719\n",
            "Iteration 5723 Loss = 0.0453852832567278\n",
            "Iteration 5724 Loss = 0.045380311071408184\n",
            "Iteration 5725 Loss = 0.045375341674014295\n",
            "Iteration 5726 Loss = 0.04537037506298314\n",
            "Iteration 5727 Loss = 0.045365411236752085\n",
            "Iteration 5728 Loss = 0.045360450193759894\n",
            "Iteration 5729 Loss = 0.04535549193244585\n",
            "Iteration 5730 Loss = 0.045350536451250256\n",
            "Iteration 5731 Loss = 0.045345583748614464\n",
            "Iteration 5732 Loss = 0.045340633822980356\n",
            "Iteration 5733 Loss = 0.04533568667279096\n",
            "Iteration 5734 Loss = 0.04533074229648992\n",
            "Iteration 5735 Loss = 0.04532580069252212\n",
            "Iteration 5736 Loss = 0.04532086185933289\n",
            "Iteration 5737 Loss = 0.04531592579536874\n",
            "Iteration 5738 Loss = 0.045310992499076934\n",
            "Iteration 5739 Loss = 0.04530606196890581\n",
            "Iteration 5740 Loss = 0.0453011342033041\n",
            "Iteration 5741 Loss = 0.0452962092007218\n",
            "Iteration 5742 Loss = 0.04529128695960976\n",
            "Iteration 5743 Loss = 0.04528636747841959\n",
            "Iteration 5744 Loss = 0.04528145075560358\n",
            "Iteration 5745 Loss = 0.045276536789615365\n",
            "Iteration 5746 Loss = 0.04527162557890914\n",
            "Iteration 5747 Loss = 0.045266717121939824\n",
            "Iteration 5748 Loss = 0.045261811417163636\n",
            "Iteration 5749 Loss = 0.04525690846303716\n",
            "Iteration 5750 Loss = 0.04525200825801826\n",
            "Iteration 5751 Loss = 0.045247110800565545\n",
            "Iteration 5752 Loss = 0.04524221608913828\n",
            "Iteration 5753 Loss = 0.04523732412219675\n",
            "Iteration 5754 Loss = 0.04523243489820236\n",
            "Iteration 5755 Loss = 0.04522754841561694\n",
            "Iteration 5756 Loss = 0.045222664672903236\n",
            "Iteration 5757 Loss = 0.045217783668525253\n",
            "Iteration 5758 Loss = 0.045212905400947405\n",
            "Iteration 5759 Loss = 0.045208029868635334\n",
            "Iteration 5760 Loss = 0.045203157070055164\n",
            "Iteration 5761 Loss = 0.04519828700367429\n",
            "Iteration 5762 Loss = 0.04519341966796058\n",
            "Iteration 5763 Loss = 0.04518855506138305\n",
            "Iteration 5764 Loss = 0.045183693182411416\n",
            "Iteration 5765 Loss = 0.04517883402951613\n",
            "Iteration 5766 Loss = 0.04517397760116903\n",
            "Iteration 5767 Loss = 0.04516912389584226\n",
            "Iteration 5768 Loss = 0.045164272912008856\n",
            "Iteration 5769 Loss = 0.04515942464814316\n",
            "Iteration 5770 Loss = 0.04515457910271991\n",
            "Iteration 5771 Loss = 0.04514973627421489\n",
            "Iteration 5772 Loss = 0.045144896161104696\n",
            "Iteration 5773 Loss = 0.04514005876186674\n",
            "Iteration 5774 Loss = 0.04513522407497937\n",
            "Iteration 5775 Loss = 0.045130392098921994\n",
            "Iteration 5776 Loss = 0.04512556283217426\n",
            "Iteration 5777 Loss = 0.045120736273217256\n",
            "Iteration 5778 Loss = 0.045115912420532825\n",
            "Iteration 5779 Loss = 0.045111091272603224\n",
            "Iteration 5780 Loss = 0.04510627282791216\n",
            "Iteration 5781 Loss = 0.04510145708494378\n",
            "Iteration 5782 Loss = 0.04509664404218336\n",
            "Iteration 5783 Loss = 0.045091833698116715\n",
            "Iteration 5784 Loss = 0.045087026051230716\n",
            "Iteration 5785 Loss = 0.04508222110001318\n",
            "Iteration 5786 Loss = 0.04507741884295239\n",
            "Iteration 5787 Loss = 0.04507261927853788\n",
            "Iteration 5788 Loss = 0.04506782240525984\n",
            "Iteration 5789 Loss = 0.0450630282216094\n",
            "Iteration 5790 Loss = 0.045058236726078366\n",
            "Iteration 5791 Loss = 0.045053447917159505\n",
            "Iteration 5792 Loss = 0.04504866179334655\n",
            "Iteration 5793 Loss = 0.045043878353133844\n",
            "Iteration 5794 Loss = 0.04503909759501671\n",
            "Iteration 5795 Loss = 0.045034319517491275\n",
            "Iteration 5796 Loss = 0.04502954411905451\n",
            "Iteration 5797 Loss = 0.045024771398204225\n",
            "Iteration 5798 Loss = 0.045020001353439226\n",
            "Iteration 5799 Loss = 0.04501523398325881\n",
            "Iteration 5800 Loss = 0.045010469286163445\n",
            "Iteration 5801 Loss = 0.04500570726065435\n",
            "Iteration 5802 Loss = 0.04500094790523347\n",
            "Iteration 5803 Loss = 0.04499619121840372\n",
            "Iteration 5804 Loss = 0.04499143719866868\n",
            "Iteration 5805 Loss = 0.04498668584453314\n",
            "Iteration 5806 Loss = 0.044981937154502404\n",
            "Iteration 5807 Loss = 0.04497719112708248\n",
            "Iteration 5808 Loss = 0.044972447760780784\n",
            "Iteration 5809 Loss = 0.044967707054104966\n",
            "Iteration 5810 Loss = 0.04496296900556378\n",
            "Iteration 5811 Loss = 0.04495823361366694\n",
            "Iteration 5812 Loss = 0.04495350087692472\n",
            "Iteration 5813 Loss = 0.044948770793848486\n",
            "Iteration 5814 Loss = 0.04494404336295016\n",
            "Iteration 5815 Loss = 0.04493931858274278\n",
            "Iteration 5816 Loss = 0.044934596451740014\n",
            "Iteration 5817 Loss = 0.04492987696845654\n",
            "Iteration 5818 Loss = 0.04492516013140762\n",
            "Iteration 5819 Loss = 0.044920445939109585\n",
            "Iteration 5820 Loss = 0.04491573439007961\n",
            "Iteration 5821 Loss = 0.04491102548283542\n",
            "Iteration 5822 Loss = 0.04490631921589584\n",
            "Iteration 5823 Loss = 0.04490161558778044\n",
            "Iteration 5824 Loss = 0.04489691459700967\n",
            "Iteration 5825 Loss = 0.04489221624210464\n",
            "Iteration 5826 Loss = 0.0448875205215875\n",
            "Iteration 5827 Loss = 0.04488282743398116\n",
            "Iteration 5828 Loss = 0.04487813697780937\n",
            "Iteration 5829 Loss = 0.044873449151596426\n",
            "Iteration 5830 Loss = 0.04486876395386793\n",
            "Iteration 5831 Loss = 0.044864081383149984\n",
            "Iteration 5832 Loss = 0.04485940143796959\n",
            "Iteration 5833 Loss = 0.044854724116854844\n",
            "Iteration 5834 Loss = 0.04485004941833395\n",
            "Iteration 5835 Loss = 0.04484537734093698\n",
            "Iteration 5836 Loss = 0.04484070788319381\n",
            "Iteration 5837 Loss = 0.04483604104363569\n",
            "Iteration 5838 Loss = 0.044831376820794744\n",
            "Iteration 5839 Loss = 0.044826715213203576\n",
            "Iteration 5840 Loss = 0.04482205621939599\n",
            "Iteration 5841 Loss = 0.04481739983790628\n",
            "Iteration 5842 Loss = 0.044812746067269836\n",
            "Iteration 5843 Loss = 0.0448080949060226\n",
            "Iteration 5844 Loss = 0.04480344635270157\n",
            "Iteration 5845 Loss = 0.04479880040584439\n",
            "Iteration 5846 Loss = 0.044794157063989724\n",
            "Iteration 5847 Loss = 0.044789516325676765\n",
            "Iteration 5848 Loss = 0.04478487818944592\n",
            "Iteration 5849 Loss = 0.04478024265383804\n",
            "Iteration 5850 Loss = 0.044775609717394985\n",
            "Iteration 5851 Loss = 0.044770979378659384\n",
            "Iteration 5852 Loss = 0.044766351636174644\n",
            "Iteration 5853 Loss = 0.04476172648848506\n",
            "Iteration 5854 Loss = 0.04475710393413582\n",
            "Iteration 5855 Loss = 0.04475248397167272\n",
            "Iteration 5856 Loss = 0.044747866599642507\n",
            "Iteration 5857 Loss = 0.044743251816592666\n",
            "Iteration 5858 Loss = 0.04473863962107168\n",
            "Iteration 5859 Loss = 0.0447340300116285\n",
            "Iteration 5860 Loss = 0.044729422986813275\n",
            "Iteration 5861 Loss = 0.04472481854517683\n",
            "Iteration 5862 Loss = 0.04472021668527054\n",
            "Iteration 5863 Loss = 0.044715617405647036\n",
            "Iteration 5864 Loss = 0.04471102070485945\n",
            "Iteration 5865 Loss = 0.04470642658146186\n",
            "Iteration 5866 Loss = 0.04470183503400905\n",
            "Iteration 5867 Loss = 0.04469724606105663\n",
            "Iteration 5868 Loss = 0.04469265966116132\n",
            "Iteration 5869 Loss = 0.044688075832880154\n",
            "Iteration 5870 Loss = 0.04468349457477128\n",
            "Iteration 5871 Loss = 0.04467891588539343\n",
            "Iteration 5872 Loss = 0.04467433976330666\n",
            "Iteration 5873 Loss = 0.04466976620707123\n",
            "Iteration 5874 Loss = 0.04466519521524851\n",
            "Iteration 5875 Loss = 0.04466062678640064\n",
            "Iteration 5876 Loss = 0.04465606091909054\n",
            "Iteration 5877 Loss = 0.04465149761188191\n",
            "Iteration 5878 Loss = 0.0446469368633393\n",
            "Iteration 5879 Loss = 0.044642378672028166\n",
            "Iteration 5880 Loss = 0.04463782303651443\n",
            "Iteration 5881 Loss = 0.04463326995536524\n",
            "Iteration 5882 Loss = 0.04462871942714838\n",
            "Iteration 5883 Loss = 0.04462417145043226\n",
            "Iteration 5884 Loss = 0.0446196260237863\n",
            "Iteration 5885 Loss = 0.044615083145780644\n",
            "Iteration 5886 Loss = 0.04461054281498637\n",
            "Iteration 5887 Loss = 0.04460600502997508\n",
            "Iteration 5888 Loss = 0.04460146978931949\n",
            "Iteration 5889 Loss = 0.04459693709159287\n",
            "Iteration 5890 Loss = 0.04459240693536949\n",
            "Iteration 5891 Loss = 0.04458787931922409\n",
            "Iteration 5892 Loss = 0.044583354241732626\n",
            "Iteration 5893 Loss = 0.044578831701471804\n",
            "Iteration 5894 Loss = 0.04457431169701865\n",
            "Iteration 5895 Loss = 0.044569794226951544\n",
            "Iteration 5896 Loss = 0.04456527928984943\n",
            "Iteration 5897 Loss = 0.04456076688429204\n",
            "Iteration 5898 Loss = 0.04455625700885983\n",
            "Iteration 5899 Loss = 0.04455174966213427\n",
            "Iteration 5900 Loss = 0.04454724484269756\n",
            "Iteration 5901 Loss = 0.04454274254913246\n",
            "Iteration 5902 Loss = 0.04453824278002273\n",
            "Iteration 5903 Loss = 0.044533745533953166\n",
            "Iteration 5904 Loss = 0.04452925080950867\n",
            "Iteration 5905 Loss = 0.04452475860527555\n",
            "Iteration 5906 Loss = 0.04452026891984081\n",
            "Iteration 5907 Loss = 0.04451578175179207\n",
            "Iteration 5908 Loss = 0.044511297099717864\n",
            "Iteration 5909 Loss = 0.04450681496220734\n",
            "Iteration 5910 Loss = 0.04450233533785062\n",
            "Iteration 5911 Loss = 0.044497858225238676\n",
            "Iteration 5912 Loss = 0.04449338362296307\n",
            "Iteration 5913 Loss = 0.04448891152961631\n",
            "Iteration 5914 Loss = 0.044484441943791586\n",
            "Iteration 5915 Loss = 0.04447997486408283\n",
            "Iteration 5916 Loss = 0.04447551028908492\n",
            "Iteration 5917 Loss = 0.04447104821739357\n",
            "Iteration 5918 Loss = 0.044466588647605096\n",
            "Iteration 5919 Loss = 0.04446213157831658\n",
            "Iteration 5920 Loss = 0.04445767700812607\n",
            "Iteration 5921 Loss = 0.04445322493563225\n",
            "Iteration 5922 Loss = 0.04444877535943464\n",
            "Iteration 5923 Loss = 0.044444328278133695\n",
            "Iteration 5924 Loss = 0.04443988369033041\n",
            "Iteration 5925 Loss = 0.04443544159462664\n",
            "Iteration 5926 Loss = 0.04443100198962506\n",
            "Iteration 5927 Loss = 0.04442656487392926\n",
            "Iteration 5928 Loss = 0.04442213024614332\n",
            "Iteration 5929 Loss = 0.044417698104872366\n",
            "Iteration 5930 Loss = 0.04441326844872203\n",
            "Iteration 5931 Loss = 0.044408841276299106\n",
            "Iteration 5932 Loss = 0.04440441658621085\n",
            "Iteration 5933 Loss = 0.04439999437706542\n",
            "Iteration 5934 Loss = 0.044395574647471736\n",
            "Iteration 5935 Loss = 0.04439115739603949\n",
            "Iteration 5936 Loss = 0.04438674262137912\n",
            "Iteration 5937 Loss = 0.044382330322101975\n",
            "Iteration 5938 Loss = 0.04437792049682025\n",
            "Iteration 5939 Loss = 0.04437351314414633\n",
            "Iteration 5940 Loss = 0.044369108262694246\n",
            "Iteration 5941 Loss = 0.04436470585107816\n",
            "Iteration 5942 Loss = 0.044360305907913185\n",
            "Iteration 5943 Loss = 0.0443559084318154\n",
            "Iteration 5944 Loss = 0.044351513421401416\n",
            "Iteration 5945 Loss = 0.04434712087528875\n",
            "Iteration 5946 Loss = 0.04434273079209557\n",
            "Iteration 5947 Loss = 0.04433834317044109\n",
            "Iteration 5948 Loss = 0.04433395800894494\n",
            "Iteration 5949 Loss = 0.044329575306227754\n",
            "Iteration 5950 Loss = 0.04432519506091089\n",
            "Iteration 5951 Loss = 0.044320817271616436\n",
            "Iteration 5952 Loss = 0.0443164419369674\n",
            "Iteration 5953 Loss = 0.04431206905558745\n",
            "Iteration 5954 Loss = 0.04430769862610077\n",
            "Iteration 5955 Loss = 0.04430333064713293\n",
            "Iteration 5956 Loss = 0.04429896511730963\n",
            "Iteration 5957 Loss = 0.04429460203525781\n",
            "Iteration 5958 Loss = 0.04429024139960487\n",
            "Iteration 5959 Loss = 0.04428588320897913\n",
            "Iteration 5960 Loss = 0.044281527462009744\n",
            "Iteration 5961 Loss = 0.044277174157326446\n",
            "Iteration 5962 Loss = 0.044272823293559846\n",
            "Iteration 5963 Loss = 0.04426847486934135\n",
            "Iteration 5964 Loss = 0.044264128883303104\n",
            "Iteration 5965 Loss = 0.04425978533407787\n",
            "Iteration 5966 Loss = 0.04425544422029957\n",
            "Iteration 5967 Loss = 0.044251105540602464\n",
            "Iteration 5968 Loss = 0.04424676929362176\n",
            "Iteration 5969 Loss = 0.04424243547799339\n",
            "Iteration 5970 Loss = 0.044238104092354215\n",
            "Iteration 5971 Loss = 0.044233775135341664\n",
            "Iteration 5972 Loss = 0.044229448605593975\n",
            "Iteration 5973 Loss = 0.04422512450175017\n",
            "Iteration 5974 Loss = 0.044220802822450014\n",
            "Iteration 5975 Loss = 0.0442164835663341\n",
            "Iteration 5976 Loss = 0.044212166732043735\n",
            "Iteration 5977 Loss = 0.044207852318221025\n",
            "Iteration 5978 Loss = 0.04420354032350879\n",
            "Iteration 5979 Loss = 0.044199230746550425\n",
            "Iteration 5980 Loss = 0.044194923585990524\n",
            "Iteration 5981 Loss = 0.04419061884047423\n",
            "Iteration 5982 Loss = 0.04418631650864725\n",
            "Iteration 5983 Loss = 0.04418201658915623\n",
            "Iteration 5984 Loss = 0.04417771908064877\n",
            "Iteration 5985 Loss = 0.0441734239817728\n",
            "Iteration 5986 Loss = 0.044169131291177256\n",
            "Iteration 5987 Loss = 0.04416484100751192\n",
            "Iteration 5988 Loss = 0.04416055312942709\n",
            "Iteration 5989 Loss = 0.04415626765557399\n",
            "Iteration 5990 Loss = 0.04415198458460454\n",
            "Iteration 5991 Loss = 0.044147703915171574\n",
            "Iteration 5992 Loss = 0.04414342564592837\n",
            "Iteration 5993 Loss = 0.04413914977552922\n",
            "Iteration 5994 Loss = 0.04413487630262903\n",
            "Iteration 5995 Loss = 0.0441306052258836\n",
            "Iteration 5996 Loss = 0.04412633654394924\n",
            "Iteration 5997 Loss = 0.044122070255483224\n",
            "Iteration 5998 Loss = 0.04411780635914366\n",
            "Iteration 5999 Loss = 0.04411354485358909\n",
            "Iteration 6000 Loss = 0.044109285737479104\n",
            "Iteration 6001 Loss = 0.04410502900947385\n",
            "Iteration 6002 Loss = 0.044100774668234435\n",
            "Iteration 6003 Loss = 0.04409652271242239\n",
            "Iteration 6004 Loss = 0.04409227314070052\n",
            "Iteration 6005 Loss = 0.044088025951731676\n",
            "Iteration 6006 Loss = 0.04408378114418003\n",
            "Iteration 6007 Loss = 0.04407953871671039\n",
            "Iteration 6008 Loss = 0.04407529866798804\n",
            "Iteration 6009 Loss = 0.04407106099667926\n",
            "Iteration 6010 Loss = 0.04406682570145116\n",
            "Iteration 6011 Loss = 0.04406259278097138\n",
            "Iteration 6012 Loss = 0.04405836223390836\n",
            "Iteration 6013 Loss = 0.04405413405893133\n",
            "Iteration 6014 Loss = 0.04404990825471021\n",
            "Iteration 6015 Loss = 0.0440456848199158\n",
            "Iteration 6016 Loss = 0.04404146375321945\n",
            "Iteration 6017 Loss = 0.04403724505329346\n",
            "Iteration 6018 Loss = 0.04403302871881064\n",
            "Iteration 6019 Loss = 0.04402881474844488\n",
            "Iteration 6020 Loss = 0.044024603140870325\n",
            "Iteration 6021 Loss = 0.04402039389476241\n",
            "Iteration 6022 Loss = 0.044016187008796955\n",
            "Iteration 6023 Loss = 0.04401198248165057\n",
            "Iteration 6024 Loss = 0.044007780312000734\n",
            "Iteration 6025 Loss = 0.04400358049852553\n",
            "Iteration 6026 Loss = 0.043999383039903854\n",
            "Iteration 6027 Loss = 0.043995187934815425\n",
            "Iteration 6028 Loss = 0.04399099518194043\n",
            "Iteration 6029 Loss = 0.0439868047799601\n",
            "Iteration 6030 Loss = 0.043982616727556226\n",
            "Iteration 6031 Loss = 0.04397843102341137\n",
            "Iteration 6032 Loss = 0.043974247666208985\n",
            "Iteration 6033 Loss = 0.04397006665463289\n",
            "Iteration 6034 Loss = 0.04396588798736811\n",
            "Iteration 6035 Loss = 0.04396171166310004\n",
            "Iteration 6036 Loss = 0.04395753768051512\n",
            "Iteration 6037 Loss = 0.04395336603830012\n",
            "Iteration 6038 Loss = 0.043949196735142984\n",
            "Iteration 6039 Loss = 0.04394502976973205\n",
            "Iteration 6040 Loss = 0.04394086514075668\n",
            "Iteration 6041 Loss = 0.043936702846906726\n",
            "Iteration 6042 Loss = 0.04393254288687287\n",
            "Iteration 6043 Loss = 0.04392838525934655\n",
            "Iteration 6044 Loss = 0.043924229963019965\n",
            "Iteration 6045 Loss = 0.043920076996585865\n",
            "Iteration 6046 Loss = 0.043915926358738\n",
            "Iteration 6047 Loss = 0.04391177804817075\n",
            "Iteration 6048 Loss = 0.04390763206357903\n",
            "Iteration 6049 Loss = 0.043903488403658875\n",
            "Iteration 6050 Loss = 0.04389934706710669\n",
            "Iteration 6051 Loss = 0.043895208052619664\n",
            "Iteration 6052 Loss = 0.0438910713588961\n",
            "Iteration 6053 Loss = 0.04388693698463439\n",
            "Iteration 6054 Loss = 0.04388280492853419\n",
            "Iteration 6055 Loss = 0.043878675189295724\n",
            "Iteration 6056 Loss = 0.04387454776561992\n",
            "Iteration 6057 Loss = 0.04387042265620826\n",
            "Iteration 6058 Loss = 0.043866299859763355\n",
            "Iteration 6059 Loss = 0.043862179374988174\n",
            "Iteration 6060 Loss = 0.043858061200586516\n",
            "Iteration 6061 Loss = 0.043853945335263086\n",
            "Iteration 6062 Loss = 0.04384983177772306\n",
            "Iteration 6063 Loss = 0.04384572052667245\n",
            "Iteration 6064 Loss = 0.043841611580818146\n",
            "Iteration 6065 Loss = 0.04383750493886751\n",
            "Iteration 6066 Loss = 0.04383340059952865\n",
            "Iteration 6067 Loss = 0.043829298561510625\n",
            "Iteration 6068 Loss = 0.04382519882352294\n",
            "Iteration 6069 Loss = 0.04382110138427613\n",
            "Iteration 6070 Loss = 0.04381700624248118\n",
            "Iteration 6071 Loss = 0.043812913396849895\n",
            "Iteration 6072 Loss = 0.043808822846094796\n",
            "Iteration 6073 Loss = 0.04380473458892911\n",
            "Iteration 6074 Loss = 0.04380064862406683\n",
            "Iteration 6075 Loss = 0.043796564950222765\n",
            "Iteration 6076 Loss = 0.043792483566112296\n",
            "Iteration 6077 Loss = 0.04378840447045135\n",
            "Iteration 6078 Loss = 0.04378432766195703\n",
            "Iteration 6079 Loss = 0.04378025313934679\n",
            "Iteration 6080 Loss = 0.043776180901338946\n",
            "Iteration 6081 Loss = 0.04377211094665253\n",
            "Iteration 6082 Loss = 0.043768043274007344\n",
            "Iteration 6083 Loss = 0.04376397788212366\n",
            "Iteration 6084 Loss = 0.043759914769722734\n",
            "Iteration 6085 Loss = 0.04375585393552653\n",
            "Iteration 6086 Loss = 0.04375179537825762\n",
            "Iteration 6087 Loss = 0.04374773909663928\n",
            "Iteration 6088 Loss = 0.04374368508939551\n",
            "Iteration 6089 Loss = 0.04373963335525105\n",
            "Iteration 6090 Loss = 0.04373558389293141\n",
            "Iteration 6091 Loss = 0.04373153670116286\n",
            "Iteration 6092 Loss = 0.04372749177867206\n",
            "Iteration 6093 Loss = 0.04372344912418688\n",
            "Iteration 6094 Loss = 0.043719408736435365\n",
            "Iteration 6095 Loss = 0.043715370614146806\n",
            "Iteration 6096 Loss = 0.04371133475605075\n",
            "Iteration 6097 Loss = 0.04370730116087795\n",
            "Iteration 6098 Loss = 0.0437032698273591\n",
            "Iteration 6099 Loss = 0.04369924075422647\n",
            "Iteration 6100 Loss = 0.04369521394021251\n",
            "Iteration 6101 Loss = 0.043691189384050554\n",
            "Iteration 6102 Loss = 0.04368716708447456\n",
            "Iteration 6103 Loss = 0.043683147040219396\n",
            "Iteration 6104 Loss = 0.0436791292500203\n",
            "Iteration 6105 Loss = 0.0436751137126136\n",
            "Iteration 6106 Loss = 0.043671100426735915\n",
            "Iteration 6107 Loss = 0.04366708939112503\n",
            "Iteration 6108 Loss = 0.04366308060451907\n",
            "Iteration 6109 Loss = 0.043659074065657145\n",
            "Iteration 6110 Loss = 0.043655069773278724\n",
            "Iteration 6111 Loss = 0.04365106772612433\n",
            "Iteration 6112 Loss = 0.0436470679229351\n",
            "Iteration 6113 Loss = 0.043643070362452845\n",
            "Iteration 6114 Loss = 0.04363907504341984\n",
            "Iteration 6115 Loss = 0.043635081964579575\n",
            "Iteration 6116 Loss = 0.043631091124675725\n",
            "Iteration 6117 Loss = 0.04362710252245313\n",
            "Iteration 6118 Loss = 0.04362311615665705\n",
            "Iteration 6119 Loss = 0.04361913202603329\n",
            "Iteration 6120 Loss = 0.043615150129328886\n",
            "Iteration 6121 Loss = 0.043611170465291206\n",
            "Iteration 6122 Loss = 0.04360719303266819\n",
            "Iteration 6123 Loss = 0.04360321783020885\n",
            "Iteration 6124 Loss = 0.04359924485666278\n",
            "Iteration 6125 Loss = 0.04359527411077996\n",
            "Iteration 6126 Loss = 0.04359130559131161\n",
            "Iteration 6127 Loss = 0.04358733929700916\n",
            "Iteration 6128 Loss = 0.04358337522662508\n",
            "Iteration 6129 Loss = 0.043579413378912364\n",
            "Iteration 6130 Loss = 0.04357545375262475\n",
            "Iteration 6131 Loss = 0.04357149634651675\n",
            "Iteration 6132 Loss = 0.043567541159343325\n",
            "Iteration 6133 Loss = 0.043563588189860385\n",
            "Iteration 6134 Loss = 0.043559637436824554\n",
            "Iteration 6135 Loss = 0.04355568889899305\n",
            "Iteration 6136 Loss = 0.04355174257512369\n",
            "Iteration 6137 Loss = 0.0435477984639751\n",
            "Iteration 6138 Loss = 0.04354385656430675\n",
            "Iteration 6139 Loss = 0.04353991687487849\n",
            "Iteration 6140 Loss = 0.043535979394451085\n",
            "Iteration 6141 Loss = 0.043532044121785955\n",
            "Iteration 6142 Loss = 0.04352811105564516\n",
            "Iteration 6143 Loss = 0.04352418019479158\n",
            "Iteration 6144 Loss = 0.04352025153798854\n",
            "Iteration 6145 Loss = 0.04351632508400044\n",
            "Iteration 6146 Loss = 0.04351240083159202\n",
            "Iteration 6147 Loss = 0.04350847877952884\n",
            "Iteration 6148 Loss = 0.043504558926577105\n",
            "Iteration 6149 Loss = 0.04350064127150382\n",
            "Iteration 6150 Loss = 0.04349672581307672\n",
            "Iteration 6151 Loss = 0.04349281255006402\n",
            "Iteration 6152 Loss = 0.04348890148123481\n",
            "Iteration 6153 Loss = 0.04348499260535878\n",
            "Iteration 6154 Loss = 0.043481085921206274\n",
            "Iteration 6155 Loss = 0.04347718142754834\n",
            "Iteration 6156 Loss = 0.04347327912315695\n",
            "Iteration 6157 Loss = 0.04346937900680451\n",
            "Iteration 6158 Loss = 0.04346548107726411\n",
            "Iteration 6159 Loss = 0.043461585333309544\n",
            "Iteration 6160 Loss = 0.04345769177371548\n",
            "Iteration 6161 Loss = 0.043453800397257124\n",
            "Iteration 6162 Loss = 0.04344991120271027\n",
            "Iteration 6163 Loss = 0.043446024188851594\n",
            "Iteration 6164 Loss = 0.04344213935445838\n",
            "Iteration 6165 Loss = 0.043438256698308586\n",
            "Iteration 6166 Loss = 0.043434376219180754\n",
            "Iteration 6167 Loss = 0.043430497915854335\n",
            "Iteration 6168 Loss = 0.043426621787109376\n",
            "Iteration 6169 Loss = 0.04342274783172652\n",
            "Iteration 6170 Loss = 0.04341887604848716\n",
            "Iteration 6171 Loss = 0.04341500643617332\n",
            "Iteration 6172 Loss = 0.04341113899356789\n",
            "Iteration 6173 Loss = 0.043407273719454166\n",
            "Iteration 6174 Loss = 0.04340341061261632\n",
            "Iteration 6175 Loss = 0.04339954967183923\n",
            "Iteration 6176 Loss = 0.04339569089590816\n",
            "Iteration 6177 Loss = 0.0433918342836094\n",
            "Iteration 6178 Loss = 0.043387979833729905\n",
            "Iteration 6179 Loss = 0.04338412754505706\n",
            "Iteration 6180 Loss = 0.043380277416378954\n",
            "Iteration 6181 Loss = 0.04337642944648477\n",
            "Iteration 6182 Loss = 0.04337258363416376\n",
            "Iteration 6183 Loss = 0.04336873997820647\n",
            "Iteration 6184 Loss = 0.043364898477403546\n",
            "Iteration 6185 Loss = 0.04336105913054679\n",
            "Iteration 6186 Loss = 0.043357221936428256\n",
            "Iteration 6187 Loss = 0.04335338689384121\n",
            "Iteration 6188 Loss = 0.043349554001578854\n",
            "Iteration 6189 Loss = 0.04334572325843594\n",
            "Iteration 6190 Loss = 0.04334189466320713\n",
            "Iteration 6191 Loss = 0.04333806821468818\n",
            "Iteration 6192 Loss = 0.04333424391167551\n",
            "Iteration 6193 Loss = 0.043330421752965906\n",
            "Iteration 6194 Loss = 0.043326601737357286\n",
            "Iteration 6195 Loss = 0.04332278386364787\n",
            "Iteration 6196 Loss = 0.04331896813063665\n",
            "Iteration 6197 Loss = 0.043315154537123444\n",
            "Iteration 6198 Loss = 0.04331134308190858\n",
            "Iteration 6199 Loss = 0.043307533763793135\n",
            "Iteration 6200 Loss = 0.043303726581578696\n",
            "Iteration 6201 Loss = 0.043299921534067795\n",
            "Iteration 6202 Loss = 0.043296118620063516\n",
            "Iteration 6203 Loss = 0.043292317838369516\n",
            "Iteration 6204 Loss = 0.04328851918779017\n",
            "Iteration 6205 Loss = 0.043284722667130494\n",
            "Iteration 6206 Loss = 0.043280928275196515\n",
            "Iteration 6207 Loss = 0.04327713601079438\n",
            "Iteration 6208 Loss = 0.043273345872731336\n",
            "Iteration 6209 Loss = 0.043269557859814914\n",
            "Iteration 6210 Loss = 0.04326577197085372\n",
            "Iteration 6211 Loss = 0.043261988204656876\n",
            "Iteration 6212 Loss = 0.04325820656003397\n",
            "Iteration 6213 Loss = 0.04325442703579555\n",
            "Iteration 6214 Loss = 0.0432506496307527\n",
            "Iteration 6215 Loss = 0.04324687434371707\n",
            "Iteration 6216 Loss = 0.04324310117350137\n",
            "Iteration 6217 Loss = 0.043239330118918305\n",
            "Iteration 6218 Loss = 0.04323556117878179\n",
            "Iteration 6219 Loss = 0.04323179435190641\n",
            "Iteration 6220 Loss = 0.04322802963710715\n",
            "Iteration 6221 Loss = 0.04322426703319962\n",
            "Iteration 6222 Loss = 0.04322050653900026\n",
            "Iteration 6223 Loss = 0.04321674815332634\n",
            "Iteration 6224 Loss = 0.043212991874995395\n",
            "Iteration 6225 Loss = 0.04320923770282603\n",
            "Iteration 6226 Loss = 0.043205485635637045\n",
            "Iteration 6227 Loss = 0.043201735672248436\n",
            "Iteration 6228 Loss = 0.04319798781148057\n",
            "Iteration 6229 Loss = 0.04319424205215422\n",
            "Iteration 6230 Loss = 0.04319049839309132\n",
            "Iteration 6231 Loss = 0.04318675683311426\n",
            "Iteration 6232 Loss = 0.04318301737104604\n",
            "Iteration 6233 Loss = 0.043179280005710224\n",
            "Iteration 6234 Loss = 0.04317554473593136\n",
            "Iteration 6235 Loss = 0.04317181156053443\n",
            "Iteration 6236 Loss = 0.04316808047834504\n",
            "Iteration 6237 Loss = 0.04316435148818943\n",
            "Iteration 6238 Loss = 0.04316062458889471\n",
            "Iteration 6239 Loss = 0.043156899779288625\n",
            "Iteration 6240 Loss = 0.04315317705819917\n",
            "Iteration 6241 Loss = 0.04314945642445565\n",
            "Iteration 6242 Loss = 0.043145737876887455\n",
            "Iteration 6243 Loss = 0.04314202141432492\n",
            "Iteration 6244 Loss = 0.04313830703559899\n",
            "Iteration 6245 Loss = 0.043134594739541274\n",
            "Iteration 6246 Loss = 0.04313088452498384\n",
            "Iteration 6247 Loss = 0.04312717639075981\n",
            "Iteration 6248 Loss = 0.04312347033570265\n",
            "Iteration 6249 Loss = 0.04311976635864655\n",
            "Iteration 6250 Loss = 0.043116064458426424\n",
            "Iteration 6251 Loss = 0.04311236463387768\n",
            "Iteration 6252 Loss = 0.04310866688383644\n",
            "Iteration 6253 Loss = 0.04310497120713974\n",
            "Iteration 6254 Loss = 0.04310127760262492\n",
            "Iteration 6255 Loss = 0.04309758606912991\n",
            "Iteration 6256 Loss = 0.04309389660549368\n",
            "Iteration 6257 Loss = 0.04309020921055574\n",
            "Iteration 6258 Loss = 0.04308652388315609\n",
            "Iteration 6259 Loss = 0.043082840622135236\n",
            "Iteration 6260 Loss = 0.04307915942633486\n",
            "Iteration 6261 Loss = 0.04307548029459677\n",
            "Iteration 6262 Loss = 0.04307180322576385\n",
            "Iteration 6263 Loss = 0.04306812821867921\n",
            "Iteration 6264 Loss = 0.04306445527218687\n",
            "Iteration 6265 Loss = 0.043060784385131536\n",
            "Iteration 6266 Loss = 0.04305711555635834\n",
            "Iteration 6267 Loss = 0.043053448784713295\n",
            "Iteration 6268 Loss = 0.04304978406904295\n",
            "Iteration 6269 Loss = 0.043046121408194526\n",
            "Iteration 6270 Loss = 0.043042460801015724\n",
            "Iteration 6271 Loss = 0.04303880224635531\n",
            "Iteration 6272 Loss = 0.04303514574306221\n",
            "Iteration 6273 Loss = 0.04303149128998634\n",
            "Iteration 6274 Loss = 0.04302783888597804\n",
            "Iteration 6275 Loss = 0.04302418852988829\n",
            "Iteration 6276 Loss = 0.043020540220569135\n",
            "Iteration 6277 Loss = 0.04301689395687266\n",
            "Iteration 6278 Loss = 0.04301324973765191\n",
            "Iteration 6279 Loss = 0.04300960756176074\n",
            "Iteration 6280 Loss = 0.04300596742805326\n",
            "Iteration 6281 Loss = 0.04300232933538439\n",
            "Iteration 6282 Loss = 0.04299869328260995\n",
            "Iteration 6283 Loss = 0.0429950592685858\n",
            "Iteration 6284 Loss = 0.04299142729216909\n",
            "Iteration 6285 Loss = 0.04298779735221723\n",
            "Iteration 6286 Loss = 0.042984169447588386\n",
            "Iteration 6287 Loss = 0.04298054357714118\n",
            "Iteration 6288 Loss = 0.04297691973973517\n",
            "Iteration 6289 Loss = 0.04297329793423065\n",
            "Iteration 6290 Loss = 0.04296967815948793\n",
            "Iteration 6291 Loss = 0.04296606041436856\n",
            "Iteration 6292 Loss = 0.042962444697734546\n",
            "Iteration 6293 Loss = 0.04295883100844842\n",
            "Iteration 6294 Loss = 0.04295521934537342\n",
            "Iteration 6295 Loss = 0.04295160970737352\n",
            "Iteration 6296 Loss = 0.04294800209331321\n",
            "Iteration 6297 Loss = 0.0429443965020577\n",
            "Iteration 6298 Loss = 0.04294079293247266\n",
            "Iteration 6299 Loss = 0.042937191383424785\n",
            "Iteration 6300 Loss = 0.042933591853780956\n",
            "Iteration 6301 Loss = 0.04292999434240887\n",
            "Iteration 6302 Loss = 0.042926398848176985\n",
            "Iteration 6303 Loss = 0.042922805369954284\n",
            "Iteration 6304 Loss = 0.0429192139066102\n",
            "Iteration 6305 Loss = 0.04291562445701519\n",
            "Iteration 6306 Loss = 0.04291203702003995\n",
            "Iteration 6307 Loss = 0.04290845159455628\n",
            "Iteration 6308 Loss = 0.04290486817943597\n",
            "Iteration 6309 Loss = 0.04290128677355207\n",
            "Iteration 6310 Loss = 0.0428977073757779\n",
            "Iteration 6311 Loss = 0.042894129984987456\n",
            "Iteration 6312 Loss = 0.04289055460005551\n",
            "Iteration 6313 Loss = 0.04288698121985738\n",
            "Iteration 6314 Loss = 0.04288340984326881\n",
            "Iteration 6315 Loss = 0.04287984046916656\n",
            "Iteration 6316 Loss = 0.04287627309642776\n",
            "Iteration 6317 Loss = 0.04287270772393028\n",
            "Iteration 6318 Loss = 0.042869144350552515\n",
            "Iteration 6319 Loss = 0.04286558297517355\n",
            "Iteration 6320 Loss = 0.04286202359667321\n",
            "Iteration 6321 Loss = 0.04285846621393168\n",
            "Iteration 6322 Loss = 0.042854910825830074\n",
            "Iteration 6323 Loss = 0.04285135743124983\n",
            "Iteration 6324 Loss = 0.042847806029073246\n",
            "Iteration 6325 Loss = 0.042844256618183245\n",
            "Iteration 6326 Loss = 0.04284070919746323\n",
            "Iteration 6327 Loss = 0.042837163765797305\n",
            "Iteration 6328 Loss = 0.0428336203220702\n",
            "Iteration 6329 Loss = 0.04283007886516741\n",
            "Iteration 6330 Loss = 0.0428265393939746\n",
            "Iteration 6331 Loss = 0.04282300190737863\n",
            "Iteration 6332 Loss = 0.04281946640426671\n",
            "Iteration 6333 Loss = 0.042815932883526545\n",
            "Iteration 6334 Loss = 0.04281240134404679\n",
            "Iteration 6335 Loss = 0.04280887178471647\n",
            "Iteration 6336 Loss = 0.04280534420442528\n",
            "Iteration 6337 Loss = 0.042801818602063495\n",
            "Iteration 6338 Loss = 0.042798294976522176\n",
            "Iteration 6339 Loss = 0.04279477332669293\n",
            "Iteration 6340 Loss = 0.042791253651467964\n",
            "Iteration 6341 Loss = 0.04278773594974016\n",
            "Iteration 6342 Loss = 0.042784220220402756\n",
            "Iteration 6343 Loss = 0.04278070646235003\n",
            "Iteration 6344 Loss = 0.04277719467447661\n",
            "Iteration 6345 Loss = 0.04277368485567774\n",
            "Iteration 6346 Loss = 0.042770177004849405\n",
            "Iteration 6347 Loss = 0.04276667112088825\n",
            "Iteration 6348 Loss = 0.04276316720269129\n",
            "Iteration 6349 Loss = 0.042759665249156385\n",
            "Iteration 6350 Loss = 0.04275616525918204\n",
            "Iteration 6351 Loss = 0.04275266723166706\n",
            "Iteration 6352 Loss = 0.04274917116551121\n",
            "Iteration 6353 Loss = 0.042745677059614844\n",
            "Iteration 6354 Loss = 0.04274218491287858\n",
            "Iteration 6355 Loss = 0.04273869472420414\n",
            "Iteration 6356 Loss = 0.04273520649249355\n",
            "Iteration 6357 Loss = 0.04273172021664954\n",
            "Iteration 6358 Loss = 0.04272823589557538\n",
            "Iteration 6359 Loss = 0.04272475352817504\n",
            "Iteration 6360 Loss = 0.04272127311335327\n",
            "Iteration 6361 Loss = 0.04271779465001492\n",
            "Iteration 6362 Loss = 0.04271431813706617\n",
            "Iteration 6363 Loss = 0.04271084357341306\n",
            "Iteration 6364 Loss = 0.04270737095796296\n",
            "Iteration 6365 Loss = 0.0427039002896232\n",
            "Iteration 6366 Loss = 0.04270043156730215\n",
            "Iteration 6367 Loss = 0.04269696478990877\n",
            "Iteration 6368 Loss = 0.04269349995635233\n",
            "Iteration 6369 Loss = 0.04269003706554302\n",
            "Iteration 6370 Loss = 0.042686576116391664\n",
            "Iteration 6371 Loss = 0.04268311710780923\n",
            "Iteration 6372 Loss = 0.042679660038707955\n",
            "Iteration 6373 Loss = 0.04267620490800018\n",
            "Iteration 6374 Loss = 0.04267275171459914\n",
            "Iteration 6375 Loss = 0.04266930045741855\n",
            "Iteration 6376 Loss = 0.042665851135372704\n",
            "Iteration 6377 Loss = 0.04266240374737656\n",
            "Iteration 6378 Loss = 0.04265895829234586\n",
            "Iteration 6379 Loss = 0.04265551476919665\n",
            "Iteration 6380 Loss = 0.042652073176845676\n",
            "Iteration 6381 Loss = 0.042648633514210485\n",
            "Iteration 6382 Loss = 0.042645195780208874\n",
            "Iteration 6383 Loss = 0.042641759973759505\n",
            "Iteration 6384 Loss = 0.042638326093781716\n",
            "Iteration 6385 Loss = 0.0426348941391952\n",
            "Iteration 6386 Loss = 0.04263146410892046\n",
            "Iteration 6387 Loss = 0.04262803600187849\n",
            "Iteration 6388 Loss = 0.042624609816991024\n",
            "Iteration 6389 Loss = 0.0426211855531801\n",
            "Iteration 6390 Loss = 0.042617763209368685\n",
            "Iteration 6391 Loss = 0.04261434278448011\n",
            "Iteration 6392 Loss = 0.042610924277438594\n",
            "Iteration 6393 Loss = 0.04260750768716888\n",
            "Iteration 6394 Loss = 0.0426040930125959\n",
            "Iteration 6395 Loss = 0.04260068025264574\n",
            "Iteration 6396 Loss = 0.04259726940624474\n",
            "Iteration 6397 Loss = 0.0425938604723201\n",
            "Iteration 6398 Loss = 0.04259045344979941\n",
            "Iteration 6399 Loss = 0.04258704833761096\n",
            "Iteration 6400 Loss = 0.042583645134683615\n",
            "Iteration 6401 Loss = 0.04258024383994685\n",
            "Iteration 6402 Loss = 0.04257684445233067\n",
            "Iteration 6403 Loss = 0.042573446970765885\n",
            "Iteration 6404 Loss = 0.042570051394183434\n",
            "Iteration 6405 Loss = 0.04256665772151561\n",
            "Iteration 6406 Loss = 0.04256326595169468\n",
            "Iteration 6407 Loss = 0.042559876083653685\n",
            "Iteration 6408 Loss = 0.04255648811632634\n",
            "Iteration 6409 Loss = 0.04255310204864698\n",
            "Iteration 6410 Loss = 0.04254971787955028\n",
            "Iteration 6411 Loss = 0.04254633560797177\n",
            "Iteration 6412 Loss = 0.042542955232847644\n",
            "Iteration 6413 Loss = 0.04253957675311429\n",
            "Iteration 6414 Loss = 0.04253620016770921\n",
            "Iteration 6415 Loss = 0.04253282547557016\n",
            "Iteration 6416 Loss = 0.04252945267563545\n",
            "Iteration 6417 Loss = 0.04252608176684432\n",
            "Iteration 6418 Loss = 0.04252271274813609\n",
            "Iteration 6419 Loss = 0.04251934561845137\n",
            "Iteration 6420 Loss = 0.04251598037673077\n",
            "Iteration 6421 Loss = 0.042512617021915645\n",
            "Iteration 6422 Loss = 0.04250925555294816\n",
            "Iteration 6423 Loss = 0.04250589596877079\n",
            "Iteration 6424 Loss = 0.04250253826832681\n",
            "Iteration 6425 Loss = 0.04249918245055997\n",
            "Iteration 6426 Loss = 0.042495828514414524\n",
            "Iteration 6427 Loss = 0.04249247645883557\n",
            "Iteration 6428 Loss = 0.0424891262827687\n",
            "Iteration 6429 Loss = 0.04248577798515997\n",
            "Iteration 6430 Loss = 0.04248243156495622\n",
            "Iteration 6431 Loss = 0.042479087021104596\n",
            "Iteration 6432 Loss = 0.0424757443525532\n",
            "Iteration 6433 Loss = 0.04247240355825048\n",
            "Iteration 6434 Loss = 0.04246906463714555\n",
            "Iteration 6435 Loss = 0.04246572758818802\n",
            "Iteration 6436 Loss = 0.042462392410328315\n",
            "Iteration 6437 Loss = 0.04245905910251724\n",
            "Iteration 6438 Loss = 0.04245572766370618\n",
            "Iteration 6439 Loss = 0.04245239809284725\n",
            "Iteration 6440 Loss = 0.042449070388893076\n",
            "Iteration 6441 Loss = 0.04244574455079694\n",
            "Iteration 6442 Loss = 0.042442420577512505\n",
            "Iteration 6443 Loss = 0.04243909846799418\n",
            "Iteration 6444 Loss = 0.04243577822119705\n",
            "Iteration 6445 Loss = 0.04243245983607661\n",
            "Iteration 6446 Loss = 0.04242914331158907\n",
            "Iteration 6447 Loss = 0.042425828646691116\n",
            "Iteration 6448 Loss = 0.04242251584034007\n",
            "Iteration 6449 Loss = 0.04241920489149386\n",
            "Iteration 6450 Loss = 0.04241589579911088\n",
            "Iteration 6451 Loss = 0.04241258856215023\n",
            "Iteration 6452 Loss = 0.04240928317957167\n",
            "Iteration 6453 Loss = 0.04240597965033539\n",
            "Iteration 6454 Loss = 0.042402677973402206\n",
            "Iteration 6455 Loss = 0.042399378147733514\n",
            "Iteration 6456 Loss = 0.04239608017229135\n",
            "Iteration 6457 Loss = 0.04239278404603819\n",
            "Iteration 6458 Loss = 0.04238948976793725\n",
            "Iteration 6459 Loss = 0.04238619733695236\n",
            "Iteration 6460 Loss = 0.042382906752047624\n",
            "Iteration 6461 Loss = 0.04237961801218801\n",
            "Iteration 6462 Loss = 0.04237633111633912\n",
            "Iteration 6463 Loss = 0.042373046063466854\n",
            "Iteration 6464 Loss = 0.04236976285253792\n",
            "Iteration 6465 Loss = 0.04236648148251953\n",
            "Iteration 6466 Loss = 0.042363201952379456\n",
            "Iteration 6467 Loss = 0.04235992426108607\n",
            "Iteration 6468 Loss = 0.04235664840760844\n",
            "Iteration 6469 Loss = 0.04235337439091581\n",
            "Iteration 6470 Loss = 0.04235010220997859\n",
            "Iteration 6471 Loss = 0.04234683186376727\n",
            "Iteration 6472 Loss = 0.042343563351253194\n",
            "Iteration 6473 Loss = 0.04234029667140817\n",
            "Iteration 6474 Loss = 0.04233703182320464\n",
            "Iteration 6475 Loss = 0.04233376880561571\n",
            "Iteration 6476 Loss = 0.042330507617614685\n",
            "Iteration 6477 Loss = 0.042327248258175854\n",
            "Iteration 6478 Loss = 0.04232399072627398\n",
            "Iteration 6479 Loss = 0.042320735020884175\n",
            "Iteration 6480 Loss = 0.042317481140982556\n",
            "Iteration 6481 Loss = 0.04231422908554539\n",
            "Iteration 6482 Loss = 0.042310978853549765\n",
            "Iteration 6483 Loss = 0.04230773044397325\n",
            "Iteration 6484 Loss = 0.04230448385579394\n",
            "Iteration 6485 Loss = 0.042301239087990654\n",
            "Iteration 6486 Loss = 0.04229799613954271\n",
            "Iteration 6487 Loss = 0.042294755009429925\n",
            "Iteration 6488 Loss = 0.04229151569663278\n",
            "Iteration 6489 Loss = 0.04228827820013229\n",
            "Iteration 6490 Loss = 0.04228504251891007\n",
            "Iteration 6491 Loss = 0.04228180865194829\n",
            "Iteration 6492 Loss = 0.042278576598229714\n",
            "Iteration 6493 Loss = 0.04227534635673745\n",
            "Iteration 6494 Loss = 0.042272117926455625\n",
            "Iteration 6495 Loss = 0.04226889130636852\n",
            "Iteration 6496 Loss = 0.04226566649546138\n",
            "Iteration 6497 Loss = 0.04226244349271946\n",
            "Iteration 6498 Loss = 0.0422592222971292\n",
            "Iteration 6499 Loss = 0.04225600290767705\n",
            "Iteration 6500 Loss = 0.042252785323350625\n",
            "Iteration 6501 Loss = 0.042249569543137536\n",
            "Iteration 6502 Loss = 0.04224635556602634\n",
            "Iteration 6503 Loss = 0.042243143391005905\n",
            "Iteration 6504 Loss = 0.042239933017065884\n",
            "Iteration 6505 Loss = 0.04223672444319635\n",
            "Iteration 6506 Loss = 0.04223351766838808\n",
            "Iteration 6507 Loss = 0.042230312691632244\n",
            "Iteration 6508 Loss = 0.04222710951192076\n",
            "Iteration 6509 Loss = 0.04222390812824592\n",
            "Iteration 6510 Loss = 0.04222070853960064\n",
            "Iteration 6511 Loss = 0.0422175107449787\n",
            "Iteration 6512 Loss = 0.04221431474337393\n",
            "Iteration 6513 Loss = 0.04221112053378106\n",
            "Iteration 6514 Loss = 0.04220792811519527\n",
            "Iteration 6515 Loss = 0.04220473748661234\n",
            "Iteration 6516 Loss = 0.04220154864702849\n",
            "Iteration 6517 Loss = 0.04219836159544096\n",
            "Iteration 6518 Loss = 0.04219517633084687\n",
            "Iteration 6519 Loss = 0.042191992852244495\n",
            "Iteration 6520 Loss = 0.042188811158632294\n",
            "Iteration 6521 Loss = 0.04218563124900936\n",
            "Iteration 6522 Loss = 0.04218245312237552\n",
            "Iteration 6523 Loss = 0.04217927677773095\n",
            "Iteration 6524 Loss = 0.04217610221407653\n",
            "Iteration 6525 Loss = 0.04217292943041367\n",
            "Iteration 6526 Loss = 0.04216975842574429\n",
            "Iteration 6527 Loss = 0.04216658919907098\n",
            "Iteration 6528 Loss = 0.04216342174939667\n",
            "Iteration 6529 Loss = 0.04216025607572506\n",
            "Iteration 6530 Loss = 0.04215709217706042\n",
            "Iteration 6531 Loss = 0.04215393005240724\n",
            "Iteration 6532 Loss = 0.042150769700771126\n",
            "Iteration 6533 Loss = 0.04214761112115775\n",
            "Iteration 6534 Loss = 0.04214445431257364\n",
            "Iteration 6535 Loss = 0.04214129927402569\n",
            "Iteration 6536 Loss = 0.04213814600452152\n",
            "Iteration 6537 Loss = 0.04213499450306905\n",
            "Iteration 6538 Loss = 0.042131844768677186\n",
            "Iteration 6539 Loss = 0.04212869680035487\n",
            "Iteration 6540 Loss = 0.04212555059711198\n",
            "Iteration 6541 Loss = 0.042122406157958904\n",
            "Iteration 6542 Loss = 0.04211926348190639\n",
            "Iteration 6543 Loss = 0.04211612256796582\n",
            "Iteration 6544 Loss = 0.04211298341514937\n",
            "Iteration 6545 Loss = 0.04210984602246935\n",
            "Iteration 6546 Loss = 0.04210671038893902\n",
            "Iteration 6547 Loss = 0.042103576513571876\n",
            "Iteration 6548 Loss = 0.04210044439538228\n",
            "Iteration 6549 Loss = 0.04209731403338472\n",
            "Iteration 6550 Loss = 0.04209418542659476\n",
            "Iteration 6551 Loss = 0.04209105857402804\n",
            "Iteration 6552 Loss = 0.04208793347470109\n",
            "Iteration 6553 Loss = 0.04208481012763077\n",
            "Iteration 6554 Loss = 0.04208168853183465\n",
            "Iteration 6555 Loss = 0.04207856868633084\n",
            "Iteration 6556 Loss = 0.042075450590137814\n",
            "Iteration 6557 Loss = 0.0420723342422747\n",
            "Iteration 6558 Loss = 0.04206921964176139\n",
            "Iteration 6559 Loss = 0.04206610678761799\n",
            "Iteration 6560 Loss = 0.042062995678865386\n",
            "Iteration 6561 Loss = 0.04205988631452482\n",
            "Iteration 6562 Loss = 0.04205677869361824\n",
            "Iteration 6563 Loss = 0.042053672815168086\n",
            "Iteration 6564 Loss = 0.0420505686781974\n",
            "Iteration 6565 Loss = 0.04204746628172973\n",
            "Iteration 6566 Loss = 0.04204436562478911\n",
            "Iteration 6567 Loss = 0.04204126670640023\n",
            "Iteration 6568 Loss = 0.042038169525588344\n",
            "Iteration 6569 Loss = 0.042035074081379076\n",
            "Iteration 6570 Loss = 0.042031980372798664\n",
            "Iteration 6571 Loss = 0.04202888839887405\n",
            "Iteration 6572 Loss = 0.042025798158632444\n",
            "Iteration 6573 Loss = 0.04202270965110198\n",
            "Iteration 6574 Loss = 0.04201962287531104\n",
            "Iteration 6575 Loss = 0.04201653783028859\n",
            "Iteration 6576 Loss = 0.042013454515064226\n",
            "Iteration 6577 Loss = 0.04201037292866786\n",
            "Iteration 6578 Loss = 0.04200729307013039\n",
            "Iteration 6579 Loss = 0.04200421493848292\n",
            "Iteration 6580 Loss = 0.04200113853275705\n",
            "Iteration 6581 Loss = 0.04199806385198525\n",
            "Iteration 6582 Loss = 0.04199499089520013\n",
            "Iteration 6583 Loss = 0.04199191966143519\n",
            "Iteration 6584 Loss = 0.041988850149724206\n",
            "Iteration 6585 Loss = 0.04198578235910179\n",
            "Iteration 6586 Loss = 0.04198271628860273\n",
            "Iteration 6587 Loss = 0.04197965193726266\n",
            "Iteration 6588 Loss = 0.041976589304117726\n",
            "Iteration 6589 Loss = 0.04197352838820419\n",
            "Iteration 6590 Loss = 0.041970469188559525\n",
            "Iteration 6591 Loss = 0.04196741170422125\n",
            "Iteration 6592 Loss = 0.041964355934227704\n",
            "Iteration 6593 Loss = 0.04196130187761751\n",
            "Iteration 6594 Loss = 0.0419582495334301\n",
            "Iteration 6595 Loss = 0.041955198900705204\n",
            "Iteration 6596 Loss = 0.041952149978483184\n",
            "Iteration 6597 Loss = 0.04194910276580504\n",
            "Iteration 6598 Loss = 0.04194605726171231\n",
            "Iteration 6599 Loss = 0.041943013465246815\n",
            "Iteration 6600 Loss = 0.04193997137545104\n",
            "Iteration 6601 Loss = 0.0419369309913682\n",
            "Iteration 6602 Loss = 0.0419338923120419\n",
            "Iteration 6603 Loss = 0.041930855336516006\n",
            "Iteration 6604 Loss = 0.04192782006383565\n",
            "Iteration 6605 Loss = 0.041924786493045645\n",
            "Iteration 6606 Loss = 0.041921754623191816\n",
            "Iteration 6607 Loss = 0.041918724453320545\n",
            "Iteration 6608 Loss = 0.04191569598247867\n",
            "Iteration 6609 Loss = 0.04191266920971337\n",
            "Iteration 6610 Loss = 0.041909644134072666\n",
            "Iteration 6611 Loss = 0.0419066207546049\n",
            "Iteration 6612 Loss = 0.04190359907035903\n",
            "Iteration 6613 Loss = 0.04190057908038455\n",
            "Iteration 6614 Loss = 0.04189756078373143\n",
            "Iteration 6615 Loss = 0.041894544179450446\n",
            "Iteration 6616 Loss = 0.04189152926659238\n",
            "Iteration 6617 Loss = 0.04188851604420898\n",
            "Iteration 6618 Loss = 0.04188550451135238\n",
            "Iteration 6619 Loss = 0.041882494667075144\n",
            "Iteration 6620 Loss = 0.04187948651043073\n",
            "Iteration 6621 Loss = 0.041876480040472654\n",
            "Iteration 6622 Loss = 0.041873475256255226\n",
            "Iteration 6623 Loss = 0.04187047215683334\n",
            "Iteration 6624 Loss = 0.04186747074126218\n",
            "Iteration 6625 Loss = 0.041864471008597776\n",
            "Iteration 6626 Loss = 0.0418614729578964\n",
            "Iteration 6627 Loss = 0.041858476588214975\n",
            "Iteration 6628 Loss = 0.04185548189861092\n",
            "Iteration 6629 Loss = 0.041852488888142296\n",
            "Iteration 6630 Loss = 0.04184949755586743\n",
            "Iteration 6631 Loss = 0.04184650790084564\n",
            "Iteration 6632 Loss = 0.041843519922136266\n",
            "Iteration 6633 Loss = 0.041840533618799365\n",
            "Iteration 6634 Loss = 0.041837548989895744\n",
            "Iteration 6635 Loss = 0.04183456603448634\n",
            "Iteration 6636 Loss = 0.04183158475163298\n",
            "Iteration 6637 Loss = 0.04182860514039778\n",
            "Iteration 6638 Loss = 0.041825627199843354\n",
            "Iteration 6639 Loss = 0.04182265092903311\n",
            "Iteration 6640 Loss = 0.041819676327030805\n",
            "Iteration 6641 Loss = 0.04181670339290061\n",
            "Iteration 6642 Loss = 0.041813732125707395\n",
            "Iteration 6643 Loss = 0.041810762524516557\n",
            "Iteration 6644 Loss = 0.04180779458839391\n",
            "Iteration 6645 Loss = 0.041804828316405876\n",
            "Iteration 6646 Loss = 0.04180186370761935\n",
            "Iteration 6647 Loss = 0.041798900761101675\n",
            "Iteration 6648 Loss = 0.04179593947592097\n",
            "Iteration 6649 Loss = 0.04179297985114555\n",
            "Iteration 6650 Loss = 0.04179002188584456\n",
            "Iteration 6651 Loss = 0.041787065579087554\n",
            "Iteration 6652 Loss = 0.041784110929944454\n",
            "Iteration 6653 Loss = 0.04178115793748592\n",
            "Iteration 6654 Loss = 0.04177820660078288\n",
            "Iteration 6655 Loss = 0.04177525691890721\n",
            "Iteration 6656 Loss = 0.04177230889093079\n",
            "Iteration 6657 Loss = 0.04176936251592636\n",
            "Iteration 6658 Loss = 0.0417664177929671\n",
            "Iteration 6659 Loss = 0.04176347472112674\n",
            "Iteration 6660 Loss = 0.04176053329947952\n",
            "Iteration 6661 Loss = 0.041757593527100074\n",
            "Iteration 6662 Loss = 0.04175465540306367\n",
            "Iteration 6663 Loss = 0.04175171892644611\n",
            "Iteration 6664 Loss = 0.041748784096323564\n",
            "Iteration 6665 Loss = 0.04174585091177309\n",
            "Iteration 6666 Loss = 0.04174291937187173\n",
            "Iteration 6667 Loss = 0.04173998947569751\n",
            "Iteration 6668 Loss = 0.041737061222328675\n",
            "Iteration 6669 Loss = 0.04173413461084409\n",
            "Iteration 6670 Loss = 0.04173120964032332\n",
            "Iteration 6671 Loss = 0.041728286309846006\n",
            "Iteration 6672 Loss = 0.04172536461849276\n",
            "Iteration 6673 Loss = 0.04172244456534445\n",
            "Iteration 6674 Loss = 0.041719526149482634\n",
            "Iteration 6675 Loss = 0.04171660936998899\n",
            "Iteration 6676 Loss = 0.04171369422594637\n",
            "Iteration 6677 Loss = 0.04171078071643756\n",
            "Iteration 6678 Loss = 0.04170786884054605\n",
            "Iteration 6679 Loss = 0.04170495859735598\n",
            "Iteration 6680 Loss = 0.04170204998595178\n",
            "Iteration 6681 Loss = 0.04169914300541856\n",
            "Iteration 6682 Loss = 0.04169623765484181\n",
            "Iteration 6683 Loss = 0.041693333933307666\n",
            "Iteration 6684 Loss = 0.04169043183990274\n",
            "Iteration 6685 Loss = 0.04168753137371409\n",
            "Iteration 6686 Loss = 0.04168463253382947\n",
            "Iteration 6687 Loss = 0.04168173531933679\n",
            "Iteration 6688 Loss = 0.04167883972932473\n",
            "Iteration 6689 Loss = 0.04167594576288255\n",
            "Iteration 6690 Loss = 0.041673053419099826\n",
            "Iteration 6691 Loss = 0.041670162697066734\n",
            "Iteration 6692 Loss = 0.04166727359587394\n",
            "Iteration 6693 Loss = 0.04166438611461264\n",
            "Iteration 6694 Loss = 0.041661500252374604\n",
            "Iteration 6695 Loss = 0.041658616008251874\n",
            "Iteration 6696 Loss = 0.041655733381337455\n",
            "Iteration 6697 Loss = 0.041652852370724205\n",
            "Iteration 6698 Loss = 0.041649972975506086\n",
            "Iteration 6699 Loss = 0.04164709519477731\n",
            "Iteration 6700 Loss = 0.04164421902763256\n",
            "Iteration 6701 Loss = 0.041641344473167224\n",
            "Iteration 6702 Loss = 0.04163847153047683\n",
            "Iteration 6703 Loss = 0.04163560019865792\n",
            "Iteration 6704 Loss = 0.04163273047680714\n",
            "Iteration 6705 Loss = 0.041629862364021754\n",
            "Iteration 6706 Loss = 0.04162699585939956\n",
            "Iteration 6707 Loss = 0.04162413096203899\n",
            "Iteration 6708 Loss = 0.041621267671038575\n",
            "Iteration 6709 Loss = 0.041618405985497875\n",
            "Iteration 6710 Loss = 0.04161554590451656\n",
            "Iteration 6711 Loss = 0.04161268742719506\n",
            "Iteration 6712 Loss = 0.04160983055263402\n",
            "Iteration 6713 Loss = 0.041606975279934975\n",
            "Iteration 6714 Loss = 0.04160412160819954\n",
            "Iteration 6715 Loss = 0.041601269536530254\n",
            "Iteration 6716 Loss = 0.041598419064029835\n",
            "Iteration 6717 Loss = 0.04159557018980167\n",
            "Iteration 6718 Loss = 0.041592722912949506\n",
            "Iteration 6719 Loss = 0.04158987723257787\n",
            "Iteration 6720 Loss = 0.041587033147791365\n",
            "Iteration 6721 Loss = 0.04158419065769556\n",
            "Iteration 6722 Loss = 0.04158134976139616\n",
            "Iteration 6723 Loss = 0.04157851045799964\n",
            "Iteration 6724 Loss = 0.04157567274661268\n",
            "Iteration 6725 Loss = 0.04157283662634275\n",
            "Iteration 6726 Loss = 0.04157000209629781\n",
            "Iteration 6727 Loss = 0.04156716915558601\n",
            "Iteration 6728 Loss = 0.04156433780331632\n",
            "Iteration 6729 Loss = 0.041561508038597976\n",
            "Iteration 6730 Loss = 0.041558679860541146\n",
            "Iteration 6731 Loss = 0.04155585326825573\n",
            "Iteration 6732 Loss = 0.041553028260852866\n",
            "Iteration 6733 Loss = 0.04155020483744379\n",
            "Iteration 6734 Loss = 0.04154738299714048\n",
            "Iteration 6735 Loss = 0.04154456273905514\n",
            "Iteration 6736 Loss = 0.04154174406230082\n",
            "Iteration 6737 Loss = 0.04153892696599053\n",
            "Iteration 6738 Loss = 0.0415361114492385\n",
            "Iteration 6739 Loss = 0.04153329751115873\n",
            "Iteration 6740 Loss = 0.04153048515086611\n",
            "Iteration 6741 Loss = 0.04152767436747618\n",
            "Iteration 6742 Loss = 0.04152486516010459\n",
            "Iteration 6743 Loss = 0.04152205752786764\n",
            "Iteration 6744 Loss = 0.0415192514698822\n",
            "Iteration 6745 Loss = 0.041516446985265625\n",
            "Iteration 6746 Loss = 0.041513644073135694\n",
            "Iteration 6747 Loss = 0.041510842732610596\n",
            "Iteration 6748 Loss = 0.04150804296280937\n",
            "Iteration 6749 Loss = 0.04150524476285112\n",
            "Iteration 6750 Loss = 0.04150244813185556\n",
            "Iteration 6751 Loss = 0.04149965306894309\n",
            "Iteration 6752 Loss = 0.041496859573234506\n",
            "Iteration 6753 Loss = 0.04149406764385093\n",
            "Iteration 6754 Loss = 0.04149127727991423\n",
            "Iteration 6755 Loss = 0.04148848848054663\n",
            "Iteration 6756 Loss = 0.04148570124487092\n",
            "Iteration 6757 Loss = 0.04148291557201026\n",
            "Iteration 6758 Loss = 0.04148013146108843\n",
            "Iteration 6759 Loss = 0.041477348911229485\n",
            "Iteration 6760 Loss = 0.04147456792155836\n",
            "Iteration 6761 Loss = 0.041471788491200076\n",
            "Iteration 6762 Loss = 0.041469010619280405\n",
            "Iteration 6763 Loss = 0.04146623430492563\n",
            "Iteration 6764 Loss = 0.041463459547262126\n",
            "Iteration 6765 Loss = 0.0414606863454172\n",
            "Iteration 6766 Loss = 0.041457914698518575\n",
            "Iteration 6767 Loss = 0.04145514460569429\n",
            "Iteration 6768 Loss = 0.04145237606607318\n",
            "Iteration 6769 Loss = 0.041449609078783964\n",
            "Iteration 6770 Loss = 0.041446843642956636\n",
            "Iteration 6771 Loss = 0.041444079757720916\n",
            "Iteration 6772 Loss = 0.041441317422207684\n",
            "Iteration 6773 Loss = 0.04143855663554787\n",
            "Iteration 6774 Loss = 0.041435797396873086\n",
            "Iteration 6775 Loss = 0.04143303970531532\n",
            "Iteration 6776 Loss = 0.0414302835600071\n",
            "Iteration 6777 Loss = 0.04142752896008136\n",
            "Iteration 6778 Loss = 0.04142477590467177\n",
            "Iteration 6779 Loss = 0.04142202439291223\n",
            "Iteration 6780 Loss = 0.041419274423937204\n",
            "Iteration 6781 Loss = 0.04141652599688163\n",
            "Iteration 6782 Loss = 0.041413779110880934\n",
            "Iteration 6783 Loss = 0.04141103376507114\n",
            "Iteration 6784 Loss = 0.04140828995858857\n",
            "Iteration 6785 Loss = 0.04140554769057009\n",
            "Iteration 6786 Loss = 0.0414028069601532\n",
            "Iteration 6787 Loss = 0.041400067766475604\n",
            "Iteration 6788 Loss = 0.0413973301086757\n",
            "Iteration 6789 Loss = 0.04139459398589234\n",
            "Iteration 6790 Loss = 0.041391859397264905\n",
            "Iteration 6791 Loss = 0.04138912634193313\n",
            "Iteration 6792 Loss = 0.041386394819037206\n",
            "Iteration 6793 Loss = 0.04138366482771793\n",
            "Iteration 6794 Loss = 0.04138093636711658\n",
            "Iteration 6795 Loss = 0.0413782094363749\n",
            "Iteration 6796 Loss = 0.04137548403463497\n",
            "Iteration 6797 Loss = 0.041372760161039614\n",
            "Iteration 6798 Loss = 0.04137003781473194\n",
            "Iteration 6799 Loss = 0.0413673169948556\n",
            "Iteration 6800 Loss = 0.0413645977005547\n",
            "Iteration 6801 Loss = 0.041361879930973865\n",
            "Iteration 6802 Loss = 0.04135916368525807\n",
            "Iteration 6803 Loss = 0.0413564489625531\n",
            "Iteration 6804 Loss = 0.04135373576200479\n",
            "Iteration 6805 Loss = 0.04135102408275966\n",
            "Iteration 6806 Loss = 0.041348313923964874\n",
            "Iteration 6807 Loss = 0.04134560528476779\n",
            "Iteration 6808 Loss = 0.04134289816431641\n",
            "Iteration 6809 Loss = 0.04134019256175916\n",
            "Iteration 6810 Loss = 0.04133748847624495\n",
            "Iteration 6811 Loss = 0.04133478590692309\n",
            "Iteration 6812 Loss = 0.041332084852943596\n",
            "Iteration 6813 Loss = 0.04132938531345659\n",
            "Iteration 6814 Loss = 0.04132668728761307\n",
            "Iteration 6815 Loss = 0.04132399077456439\n",
            "Iteration 6816 Loss = 0.0413212957734621\n",
            "Iteration 6817 Loss = 0.041318602283458634\n",
            "Iteration 6818 Loss = 0.04131591030370648\n",
            "Iteration 6819 Loss = 0.04131321983335916\n",
            "Iteration 6820 Loss = 0.04131053087157013\n",
            "Iteration 6821 Loss = 0.04130784341749355\n",
            "Iteration 6822 Loss = 0.04130515747028404\n",
            "Iteration 6823 Loss = 0.04130247302909679\n",
            "Iteration 6824 Loss = 0.0412997900930872\n",
            "Iteration 6825 Loss = 0.04129710866141147\n",
            "Iteration 6826 Loss = 0.04129442873322606\n",
            "Iteration 6827 Loss = 0.04129175030768786\n",
            "Iteration 6828 Loss = 0.04128907338395446\n",
            "Iteration 6829 Loss = 0.041286397961183816\n",
            "Iteration 6830 Loss = 0.04128372403853427\n",
            "Iteration 6831 Loss = 0.04128105161516461\n",
            "Iteration 6832 Loss = 0.041278380690234234\n",
            "Iteration 6833 Loss = 0.0412757112629031\n",
            "Iteration 6834 Loss = 0.04127304333233137\n",
            "Iteration 6835 Loss = 0.04127037689767974\n",
            "Iteration 6836 Loss = 0.04126771195810954\n",
            "Iteration 6837 Loss = 0.0412650485127825\n",
            "Iteration 6838 Loss = 0.04126238656086069\n",
            "Iteration 6839 Loss = 0.04125972610150687\n",
            "Iteration 6840 Loss = 0.04125706713388405\n",
            "Iteration 6841 Loss = 0.0412544096571559\n",
            "Iteration 6842 Loss = 0.04125175367048638\n",
            "Iteration 6843 Loss = 0.04124909917304009\n",
            "Iteration 6844 Loss = 0.04124644616398187\n",
            "Iteration 6845 Loss = 0.041243794642477405\n",
            "Iteration 6846 Loss = 0.04124114460769235\n",
            "Iteration 6847 Loss = 0.04123849605879332\n",
            "Iteration 6848 Loss = 0.04123584899494707\n",
            "Iteration 6849 Loss = 0.04123320341532097\n",
            "Iteration 6850 Loss = 0.04123055931908269\n",
            "Iteration 6851 Loss = 0.041227916705400686\n",
            "Iteration 6852 Loss = 0.04122527557344347\n",
            "Iteration 6853 Loss = 0.041222635922380325\n",
            "Iteration 6854 Loss = 0.041219997751381\n",
            "Iteration 6855 Loss = 0.041217361059615515\n",
            "Iteration 6856 Loss = 0.041214725846254484\n",
            "Iteration 6857 Loss = 0.04121209211046896\n",
            "Iteration 6858 Loss = 0.04120945985143036\n",
            "Iteration 6859 Loss = 0.04120682906831097\n",
            "Iteration 6860 Loss = 0.041204199760282854\n",
            "Iteration 6861 Loss = 0.041201571926519104\n",
            "Iteration 6862 Loss = 0.041198945566193204\n",
            "Iteration 6863 Loss = 0.04119632067847869\n",
            "Iteration 6864 Loss = 0.04119369726255018\n",
            "Iteration 6865 Loss = 0.04119107531758227\n",
            "Iteration 6866 Loss = 0.04118845484275025\n",
            "Iteration 6867 Loss = 0.04118583583722968\n",
            "Iteration 6868 Loss = 0.0411832183001969\n",
            "Iteration 6869 Loss = 0.041180602230828395\n",
            "Iteration 6870 Loss = 0.041177987628301244\n",
            "Iteration 6871 Loss = 0.04117537449179304\n",
            "Iteration 6872 Loss = 0.04117276282048167\n",
            "Iteration 6873 Loss = 0.041170152613545785\n",
            "Iteration 6874 Loss = 0.041167543870164115\n",
            "Iteration 6875 Loss = 0.04116493658951621\n",
            "Iteration 6876 Loss = 0.04116233077078173\n",
            "Iteration 6877 Loss = 0.041159726413141036\n",
            "Iteration 6878 Loss = 0.04115712351577494\n",
            "Iteration 6879 Loss = 0.041154522077864616\n",
            "Iteration 6880 Loss = 0.04115192209859181\n",
            "Iteration 6881 Loss = 0.0411493235771385\n",
            "Iteration 6882 Loss = 0.04114672651268751\n",
            "Iteration 6883 Loss = 0.04114413090442167\n",
            "Iteration 6884 Loss = 0.041141536751524634\n",
            "Iteration 6885 Loss = 0.04113894405318027\n",
            "Iteration 6886 Loss = 0.04113635280857308\n",
            "Iteration 6887 Loss = 0.04113376301688785\n",
            "Iteration 6888 Loss = 0.04113117467731011\n",
            "Iteration 6889 Loss = 0.04112858778902549\n",
            "Iteration 6890 Loss = 0.04112600235122035\n",
            "Iteration 6891 Loss = 0.041123418363081206\n",
            "Iteration 6892 Loss = 0.04112083582379544\n",
            "Iteration 6893 Loss = 0.04111825473255061\n",
            "Iteration 6894 Loss = 0.04111567508853473\n",
            "Iteration 6895 Loss = 0.041113096890936444\n",
            "Iteration 6896 Loss = 0.04111052013894461\n",
            "Iteration 6897 Loss = 0.04110794483174879\n",
            "Iteration 6898 Loss = 0.04110537096853875\n",
            "Iteration 6899 Loss = 0.04110279854850503\n",
            "Iteration 6900 Loss = 0.04110022757083823\n",
            "Iteration 6901 Loss = 0.0410976580347297\n",
            "Iteration 6902 Loss = 0.04109508993937124\n",
            "Iteration 6903 Loss = 0.041092523283954796\n",
            "Iteration 6904 Loss = 0.04108995806767323\n",
            "Iteration 6905 Loss = 0.041087394289719435\n",
            "Iteration 6906 Loss = 0.041084831949287075\n",
            "Iteration 6907 Loss = 0.04108227104556995\n",
            "Iteration 6908 Loss = 0.041079711577762724\n",
            "Iteration 6909 Loss = 0.04107715354505995\n",
            "Iteration 6910 Loss = 0.041074596946657355\n",
            "Iteration 6911 Loss = 0.0410720417817504\n",
            "Iteration 6912 Loss = 0.04106948804953541\n",
            "Iteration 6913 Loss = 0.04106693574920919\n",
            "Iteration 6914 Loss = 0.04106438487996868\n",
            "Iteration 6915 Loss = 0.041061835441011614\n",
            "Iteration 6916 Loss = 0.0410592874315359\n",
            "Iteration 6917 Loss = 0.041056740850740216\n",
            "Iteration 6918 Loss = 0.04105419569782323\n",
            "Iteration 6919 Loss = 0.041051651971984585\n",
            "Iteration 6920 Loss = 0.04104910967242386\n",
            "Iteration 6921 Loss = 0.041046568798341565\n",
            "Iteration 6922 Loss = 0.04104402934893834\n",
            "Iteration 6923 Loss = 0.04104149132341539\n",
            "Iteration 6924 Loss = 0.041038954720974186\n",
            "Iteration 6925 Loss = 0.04103641954081697\n",
            "Iteration 6926 Loss = 0.041033885782146264\n",
            "Iteration 6927 Loss = 0.04103135344416482\n",
            "Iteration 6928 Loss = 0.041028822526076446\n",
            "Iteration 6929 Loss = 0.0410262930270845\n",
            "Iteration 6930 Loss = 0.04102376494639374\n",
            "Iteration 6931 Loss = 0.04102123828320864\n",
            "Iteration 6932 Loss = 0.041018713036734517\n",
            "Iteration 6933 Loss = 0.04101618920617701\n",
            "Iteration 6934 Loss = 0.041013666790742156\n",
            "Iteration 6935 Loss = 0.0410111457896365\n",
            "Iteration 6936 Loss = 0.04100862620206698\n",
            "Iteration 6937 Loss = 0.04100610802724128\n",
            "Iteration 6938 Loss = 0.04100359126436692\n",
            "Iteration 6939 Loss = 0.04100107591265236\n",
            "Iteration 6940 Loss = 0.04099856197130649\n",
            "Iteration 6941 Loss = 0.04099604943953821\n",
            "Iteration 6942 Loss = 0.04099353831655746\n",
            "Iteration 6943 Loss = 0.040991028601574137\n",
            "Iteration 6944 Loss = 0.04098852029379881\n",
            "Iteration 6945 Loss = 0.04098601339244258\n",
            "Iteration 6946 Loss = 0.040983507896716696\n",
            "Iteration 6947 Loss = 0.04098100380583303\n",
            "Iteration 6948 Loss = 0.04097850111900391\n",
            "Iteration 6949 Loss = 0.04097599983544218\n",
            "Iteration 6950 Loss = 0.04097349995436094\n",
            "Iteration 6951 Loss = 0.040971001474973706\n",
            "Iteration 6952 Loss = 0.04096850439649478\n",
            "Iteration 6953 Loss = 0.040966008718138415\n",
            "Iteration 6954 Loss = 0.04096351443911975\n",
            "Iteration 6955 Loss = 0.04096102155865404\n",
            "Iteration 6956 Loss = 0.040958530075957164\n",
            "Iteration 6957 Loss = 0.04095603999024542\n",
            "Iteration 6958 Loss = 0.04095355130073543\n",
            "Iteration 6959 Loss = 0.04095106400664446\n",
            "Iteration 6960 Loss = 0.04094857810718998\n",
            "Iteration 6961 Loss = 0.04094609360158997\n",
            "Iteration 6962 Loss = 0.040943610489063115\n",
            "Iteration 6963 Loss = 0.04094112876882806\n",
            "Iteration 6964 Loss = 0.04093864844010433\n",
            "Iteration 6965 Loss = 0.040936169502111514\n",
            "Iteration 6966 Loss = 0.040933691954069984\n",
            "Iteration 6967 Loss = 0.04093121579520035\n",
            "Iteration 6968 Loss = 0.0409287410247236\n",
            "Iteration 6969 Loss = 0.04092626764186146\n",
            "Iteration 6970 Loss = 0.04092379564583566\n",
            "Iteration 6971 Loss = 0.040921325035868775\n",
            "Iteration 6972 Loss = 0.04091885581118352\n",
            "Iteration 6973 Loss = 0.04091638797100317\n",
            "Iteration 6974 Loss = 0.04091392151455146\n",
            "Iteration 6975 Loss = 0.04091145644105258\n",
            "Iteration 6976 Loss = 0.040908992749731005\n",
            "Iteration 6977 Loss = 0.040906530439811774\n",
            "Iteration 6978 Loss = 0.040904069510520426\n",
            "Iteration 6979 Loss = 0.040901609961082545\n",
            "Iteration 6980 Loss = 0.04089915179072479\n",
            "Iteration 6981 Loss = 0.04089669499867377\n",
            "Iteration 6982 Loss = 0.04089423958415664\n",
            "Iteration 6983 Loss = 0.040891785546401105\n",
            "Iteration 6984 Loss = 0.04088933288463499\n",
            "Iteration 6985 Loss = 0.04088688159808694\n",
            "Iteration 6986 Loss = 0.040884431685985885\n",
            "Iteration 6987 Loss = 0.04088198314756106\n",
            "Iteration 6988 Loss = 0.04087953598204227\n",
            "Iteration 6989 Loss = 0.04087709018865982\n",
            "Iteration 6990 Loss = 0.040874645766644235\n",
            "Iteration 6991 Loss = 0.04087220271522671\n",
            "Iteration 6992 Loss = 0.04086976103363853\n",
            "Iteration 6993 Loss = 0.040867320721111815\n",
            "Iteration 6994 Loss = 0.04086488177687883\n",
            "Iteration 6995 Loss = 0.04086244420017234\n",
            "Iteration 6996 Loss = 0.040860007990225704\n",
            "Iteration 6997 Loss = 0.04085757314627253\n",
            "Iteration 6998 Loss = 0.040855139667546836\n",
            "Iteration 6999 Loss = 0.040852707553283144\n",
            "Iteration 7000 Loss = 0.040850276802716426\n",
            "Iteration 7001 Loss = 0.04084784741508203\n",
            "Iteration 7002 Loss = 0.04084541938961577\n",
            "Iteration 7003 Loss = 0.040842992725553875\n",
            "Iteration 7004 Loss = 0.04084056742213291\n",
            "Iteration 7005 Loss = 0.04083814347859013\n",
            "Iteration 7006 Loss = 0.040835720894162944\n",
            "Iteration 7007 Loss = 0.04083329966808922\n",
            "Iteration 7008 Loss = 0.040830879799607506\n",
            "Iteration 7009 Loss = 0.04082846128795641\n",
            "Iteration 7010 Loss = 0.0408260441323753\n",
            "Iteration 7011 Loss = 0.040823628332103665\n",
            "Iteration 7012 Loss = 0.04082121388638178\n",
            "Iteration 7013 Loss = 0.04081880079445\n",
            "Iteration 7014 Loss = 0.040816389055549206\n",
            "Iteration 7015 Loss = 0.04081397866892091\n",
            "Iteration 7016 Loss = 0.040811569633806735\n",
            "Iteration 7017 Loss = 0.04080916194944893\n",
            "Iteration 7018 Loss = 0.0408067556150901\n",
            "Iteration 7019 Loss = 0.0408043506299735\n",
            "Iteration 7020 Loss = 0.04080194699334224\n",
            "Iteration 7021 Loss = 0.04079954470444053\n",
            "Iteration 7022 Loss = 0.04079714376251239\n",
            "Iteration 7023 Loss = 0.040794744166802954\n",
            "Iteration 7024 Loss = 0.040792345916557095\n",
            "Iteration 7025 Loss = 0.04078994901102058\n",
            "Iteration 7026 Loss = 0.04078755344943928\n",
            "Iteration 7027 Loss = 0.04078515923105962\n",
            "Iteration 7028 Loss = 0.0407827663551286\n",
            "Iteration 7029 Loss = 0.04078037482089342\n",
            "Iteration 7030 Loss = 0.04077798462760185\n",
            "Iteration 7031 Loss = 0.04077559577450201\n",
            "Iteration 7032 Loss = 0.04077320826084226\n",
            "Iteration 7033 Loss = 0.040770822085871926\n",
            "Iteration 7034 Loss = 0.040768437248840106\n",
            "Iteration 7035 Loss = 0.04076605374899655\n",
            "Iteration 7036 Loss = 0.040763671585591826\n",
            "Iteration 7037 Loss = 0.040761290757876274\n",
            "Iteration 7038 Loss = 0.040758911265101186\n",
            "Iteration 7039 Loss = 0.040756533106517875\n",
            "Iteration 7040 Loss = 0.040754156281378386\n",
            "Iteration 7041 Loss = 0.04075178078893506\n",
            "Iteration 7042 Loss = 0.040749406628440565\n",
            "Iteration 7043 Loss = 0.04074703379914806\n",
            "Iteration 7044 Loss = 0.04074466230031126\n",
            "Iteration 7045 Loss = 0.040742292131184035\n",
            "Iteration 7046 Loss = 0.04073992329102087\n",
            "Iteration 7047 Loss = 0.04073755577907659\n",
            "Iteration 7048 Loss = 0.04073518959460654\n",
            "Iteration 7049 Loss = 0.04073282473686625\n",
            "Iteration 7050 Loss = 0.040730461205111854\n",
            "Iteration 7051 Loss = 0.04072809899860002\n",
            "Iteration 7052 Loss = 0.04072573811658753\n",
            "Iteration 7053 Loss = 0.040723378558331864\n",
            "Iteration 7054 Loss = 0.04072102032309046\n",
            "Iteration 7055 Loss = 0.04071866341012196\n",
            "Iteration 7056 Loss = 0.040716307818684586\n",
            "Iteration 7057 Loss = 0.04071395354803757\n",
            "Iteration 7058 Loss = 0.04071160059744033\n",
            "Iteration 7059 Loss = 0.04070924896615249\n",
            "Iteration 7060 Loss = 0.04070689865343455\n",
            "Iteration 7061 Loss = 0.0407045496585471\n",
            "Iteration 7062 Loss = 0.040702201980751126\n",
            "Iteration 7063 Loss = 0.04069985561930834\n",
            "Iteration 7064 Loss = 0.04069751057348048\n",
            "Iteration 7065 Loss = 0.04069516684252991\n",
            "Iteration 7066 Loss = 0.04069282442571938\n",
            "Iteration 7067 Loss = 0.04069048332231216\n",
            "Iteration 7068 Loss = 0.04068814353157168\n",
            "Iteration 7069 Loss = 0.04068580505276191\n",
            "Iteration 7070 Loss = 0.04068346788514733\n",
            "Iteration 7071 Loss = 0.04068113202799271\n",
            "Iteration 7072 Loss = 0.04067879748056328\n",
            "Iteration 7073 Loss = 0.04067646424212477\n",
            "Iteration 7074 Loss = 0.04067413231194299\n",
            "Iteration 7075 Loss = 0.04067180168928458\n",
            "Iteration 7076 Loss = 0.04066947237341623\n",
            "Iteration 7077 Loss = 0.040667144363605405\n",
            "Iteration 7078 Loss = 0.04066481765911986\n",
            "Iteration 7079 Loss = 0.04066249225922732\n",
            "Iteration 7080 Loss = 0.04066016816319673\n",
            "Iteration 7081 Loss = 0.040657845370296715\n",
            "Iteration 7082 Loss = 0.04065552387979678\n",
            "Iteration 7083 Loss = 0.040653203690966554\n",
            "Iteration 7084 Loss = 0.04065088480307612\n",
            "Iteration 7085 Loss = 0.04064856721539622\n",
            "Iteration 7086 Loss = 0.040646250927197765\n",
            "Iteration 7087 Loss = 0.04064393593775198\n",
            "Iteration 7088 Loss = 0.04064162224633092\n",
            "Iteration 7089 Loss = 0.04063930985220645\n",
            "Iteration 7090 Loss = 0.0406369987546515\n",
            "Iteration 7091 Loss = 0.04063468895293891\n",
            "Iteration 7092 Loss = 0.040632380446341904\n",
            "Iteration 7093 Loss = 0.04063007323413472\n",
            "Iteration 7094 Loss = 0.04062776731559129\n",
            "Iteration 7095 Loss = 0.04062546268998634\n",
            "Iteration 7096 Loss = 0.04062315935659496\n",
            "Iteration 7097 Loss = 0.04062085731469242\n",
            "Iteration 7098 Loss = 0.0406185565635549\n",
            "Iteration 7099 Loss = 0.04061625710245828\n",
            "Iteration 7100 Loss = 0.04061395893067955\n",
            "Iteration 7101 Loss = 0.04061166204749559\n",
            "Iteration 7102 Loss = 0.04060936645218395\n",
            "Iteration 7103 Loss = 0.040607072144022614\n",
            "Iteration 7104 Loss = 0.040604779122289644\n",
            "Iteration 7105 Loss = 0.0406024873862638\n",
            "Iteration 7106 Loss = 0.040600196935224354\n",
            "Iteration 7107 Loss = 0.04059790776845064\n",
            "Iteration 7108 Loss = 0.040595619885222575\n",
            "Iteration 7109 Loss = 0.040593333284820586\n",
            "Iteration 7110 Loss = 0.04059104796652524\n",
            "Iteration 7111 Loss = 0.040588763929617765\n",
            "Iteration 7112 Loss = 0.04058648117337967\n",
            "Iteration 7113 Loss = 0.04058419969709283\n",
            "Iteration 7114 Loss = 0.04058191950003947\n",
            "Iteration 7115 Loss = 0.040579640581502575\n",
            "Iteration 7116 Loss = 0.04057736294076508\n",
            "Iteration 7117 Loss = 0.040575086577110564\n",
            "Iteration 7118 Loss = 0.04057281148982293\n",
            "Iteration 7119 Loss = 0.0405705376781866\n",
            "Iteration 7120 Loss = 0.04056826514148633\n",
            "Iteration 7121 Loss = 0.0405659938790071\n",
            "Iteration 7122 Loss = 0.040563723890034604\n",
            "Iteration 7123 Loss = 0.04056145517385468\n",
            "Iteration 7124 Loss = 0.040559187729753775\n",
            "Iteration 7125 Loss = 0.040556921557018406\n",
            "Iteration 7126 Loss = 0.04055465665493603\n",
            "Iteration 7127 Loss = 0.040552393022793974\n",
            "Iteration 7128 Loss = 0.040550130659880224\n",
            "Iteration 7129 Loss = 0.04054786956548318\n",
            "Iteration 7130 Loss = 0.04054560973889137\n",
            "Iteration 7131 Loss = 0.0405433511793942\n",
            "Iteration 7132 Loss = 0.040541093886280986\n",
            "Iteration 7133 Loss = 0.0405388378588418\n",
            "Iteration 7134 Loss = 0.04053658309636699\n",
            "Iteration 7135 Loss = 0.04053432959814725\n",
            "Iteration 7136 Loss = 0.0405320773634736\n",
            "Iteration 7137 Loss = 0.04052982639163762\n",
            "Iteration 7138 Loss = 0.040527576681931375\n",
            "Iteration 7139 Loss = 0.04052532823364695\n",
            "Iteration 7140 Loss = 0.0405230810460772\n",
            "Iteration 7141 Loss = 0.04052083511851524\n",
            "Iteration 7142 Loss = 0.04051859045025463\n",
            "Iteration 7143 Loss = 0.04051634704058914\n",
            "Iteration 7144 Loss = 0.0405141048888131\n",
            "Iteration 7145 Loss = 0.040511863994221256\n",
            "Iteration 7146 Loss = 0.040509624356108644\n",
            "Iteration 7147 Loss = 0.04050738597377084\n",
            "Iteration 7148 Loss = 0.04050514884650367\n",
            "Iteration 7149 Loss = 0.0405029129736034\n",
            "Iteration 7150 Loss = 0.0405006783543667\n",
            "Iteration 7151 Loss = 0.04049844498809064\n",
            "Iteration 7152 Loss = 0.040496212874072644\n",
            "Iteration 7153 Loss = 0.040493982011610676\n",
            "Iteration 7154 Loss = 0.040491752400002905\n",
            "Iteration 7155 Loss = 0.04048952403854803\n",
            "Iteration 7156 Loss = 0.040487296926544965\n",
            "Iteration 7157 Loss = 0.04048507106329325\n",
            "Iteration 7158 Loss = 0.04048284644809265\n",
            "Iteration 7159 Loss = 0.040480623080243354\n",
            "Iteration 7160 Loss = 0.04047840095904596\n",
            "Iteration 7161 Loss = 0.040476180083801695\n",
            "Iteration 7162 Loss = 0.04047396045381157\n",
            "Iteration 7163 Loss = 0.04047174206837763\n",
            "Iteration 7164 Loss = 0.04046952492680198\n",
            "Iteration 7165 Loss = 0.040467309028387216\n",
            "Iteration 7166 Loss = 0.04046509437243623\n",
            "Iteration 7167 Loss = 0.04046288095825237\n",
            "Iteration 7168 Loss = 0.040460668785139375\n",
            "Iteration 7169 Loss = 0.04045845785240139\n",
            "Iteration 7170 Loss = 0.040456248159342886\n",
            "Iteration 7171 Loss = 0.04045403970526896\n",
            "Iteration 7172 Loss = 0.040451832489484627\n",
            "Iteration 7173 Loss = 0.04044962651129582\n",
            "Iteration 7174 Loss = 0.04044742177000844\n",
            "Iteration 7175 Loss = 0.040445218264928966\n",
            "Iteration 7176 Loss = 0.04044301599536445\n",
            "Iteration 7177 Loss = 0.04044081496062184\n",
            "Iteration 7178 Loss = 0.04043861516000894\n",
            "Iteration 7179 Loss = 0.04043641659283377\n",
            "Iteration 7180 Loss = 0.040434219258404666\n",
            "Iteration 7181 Loss = 0.04043202315603046\n",
            "Iteration 7182 Loss = 0.040429828285020286\n",
            "Iteration 7183 Loss = 0.04042763464468384\n",
            "Iteration 7184 Loss = 0.04042544223433092\n",
            "Iteration 7185 Loss = 0.040423251053272026\n",
            "Iteration 7186 Loss = 0.040421061100817744\n",
            "Iteration 7187 Loss = 0.040418872376279315\n",
            "Iteration 7188 Loss = 0.040416684878968066\n",
            "Iteration 7189 Loss = 0.040414498608196105\n",
            "Iteration 7190 Loss = 0.04041231356327556\n",
            "Iteration 7191 Loss = 0.04041012974351912\n",
            "Iteration 7192 Loss = 0.040407947148239795\n",
            "Iteration 7193 Loss = 0.04040576577675114\n",
            "Iteration 7194 Loss = 0.040403585628366816\n",
            "Iteration 7195 Loss = 0.04040140670240128\n",
            "Iteration 7196 Loss = 0.04039922899816879\n",
            "Iteration 7197 Loss = 0.04039705251498441\n",
            "Iteration 7198 Loss = 0.04039487725216355\n",
            "Iteration 7199 Loss = 0.04039270320902204\n",
            "Iteration 7200 Loss = 0.04039053038487577\n",
            "Iteration 7201 Loss = 0.04038835877904153\n",
            "Iteration 7202 Loss = 0.04038618839083595\n",
            "Iteration 7203 Loss = 0.04038401921957645\n",
            "Iteration 7204 Loss = 0.04038185126458052\n",
            "Iteration 7205 Loss = 0.040379684525166416\n",
            "Iteration 7206 Loss = 0.040377519000652416\n",
            "Iteration 7207 Loss = 0.04037535469035736\n",
            "Iteration 7208 Loss = 0.040373191593600526\n",
            "Iteration 7209 Loss = 0.0403710297097012\n",
            "Iteration 7210 Loss = 0.04036886903797956\n",
            "Iteration 7211 Loss = 0.04036670957775593\n",
            "Iteration 7212 Loss = 0.040364551328350935\n",
            "Iteration 7213 Loss = 0.04036239428908571\n",
            "Iteration 7214 Loss = 0.04036023845928171\n",
            "Iteration 7215 Loss = 0.040358083838260714\n",
            "Iteration 7216 Loss = 0.04035593042534512\n",
            "Iteration 7217 Loss = 0.04035377821985739\n",
            "Iteration 7218 Loss = 0.04035162722112058\n",
            "Iteration 7219 Loss = 0.04034947742845794\n",
            "Iteration 7220 Loss = 0.04034732884119339\n",
            "Iteration 7221 Loss = 0.040345181458651104\n",
            "Iteration 7222 Loss = 0.040343035280155336\n",
            "Iteration 7223 Loss = 0.040340890305031164\n",
            "Iteration 7224 Loss = 0.04033874653260376\n",
            "Iteration 7225 Loss = 0.04033660396219894\n",
            "Iteration 7226 Loss = 0.04033446259314246\n",
            "Iteration 7227 Loss = 0.04033232242476077\n",
            "Iteration 7228 Loss = 0.04033018345638085\n",
            "Iteration 7229 Loss = 0.04032804568732975\n",
            "Iteration 7230 Loss = 0.04032590911693491\n",
            "Iteration 7231 Loss = 0.040323773744524304\n",
            "Iteration 7232 Loss = 0.04032163956942629\n",
            "Iteration 7233 Loss = 0.04031950659096942\n",
            "Iteration 7234 Loss = 0.0403173748084828\n",
            "Iteration 7235 Loss = 0.04031524422129572\n",
            "Iteration 7236 Loss = 0.04031311482873823\n",
            "Iteration 7237 Loss = 0.040310986630140176\n",
            "Iteration 7238 Loss = 0.040308859624832286\n",
            "Iteration 7239 Loss = 0.04030673381214558\n",
            "Iteration 7240 Loss = 0.04030460919141119\n",
            "Iteration 7241 Loss = 0.04030248576196065\n",
            "Iteration 7242 Loss = 0.040300363523126326\n",
            "Iteration 7243 Loss = 0.040298242474240395\n",
            "Iteration 7244 Loss = 0.040296122614635714\n",
            "Iteration 7245 Loss = 0.040294003943645455\n",
            "Iteration 7246 Loss = 0.04029188646060337\n",
            "Iteration 7247 Loss = 0.04028977016484285\n",
            "Iteration 7248 Loss = 0.0402876550556987\n",
            "Iteration 7249 Loss = 0.04028554113250532\n",
            "Iteration 7250 Loss = 0.04028342839459772\n",
            "Iteration 7251 Loss = 0.0402813168413115\n",
            "Iteration 7252 Loss = 0.04027920647198232\n",
            "Iteration 7253 Loss = 0.04027709728594617\n",
            "Iteration 7254 Loss = 0.04027498928253985\n",
            "Iteration 7255 Loss = 0.04027288246110006\n",
            "Iteration 7256 Loss = 0.04027077682096419\n",
            "Iteration 7257 Loss = 0.040268672361469915\n",
            "Iteration 7258 Loss = 0.04026656908195508\n",
            "Iteration 7259 Loss = 0.040264466981758125\n",
            "Iteration 7260 Loss = 0.040262366060217825\n",
            "Iteration 7261 Loss = 0.04026026631667326\n",
            "Iteration 7262 Loss = 0.04025816775046402\n",
            "Iteration 7263 Loss = 0.04025607036092994\n",
            "Iteration 7264 Loss = 0.04025397414741118\n",
            "Iteration 7265 Loss = 0.04025187910924836\n",
            "Iteration 7266 Loss = 0.040249785245782516\n",
            "Iteration 7267 Loss = 0.04024769255635496\n",
            "Iteration 7268 Loss = 0.04024560104030733\n",
            "Iteration 7269 Loss = 0.040243510696981816\n",
            "Iteration 7270 Loss = 0.040241421525720834\n",
            "Iteration 7271 Loss = 0.04023933352586723\n",
            "Iteration 7272 Loss = 0.04023724669676401\n",
            "Iteration 7273 Loss = 0.04023516103775502\n",
            "Iteration 7274 Loss = 0.040233076548183984\n",
            "Iteration 7275 Loss = 0.04023099322739515\n",
            "Iteration 7276 Loss = 0.04022891107473341\n",
            "Iteration 7277 Loss = 0.04022683008954366\n",
            "Iteration 7278 Loss = 0.040224750271171214\n",
            "Iteration 7279 Loss = 0.04022267161896188\n",
            "Iteration 7280 Loss = 0.0402205941322619\n",
            "Iteration 7281 Loss = 0.04021851781041757\n",
            "Iteration 7282 Loss = 0.04021644265277593\n",
            "Iteration 7283 Loss = 0.040214368658684066\n",
            "Iteration 7284 Loss = 0.040212295827489676\n",
            "Iteration 7285 Loss = 0.040210224158540676\n",
            "Iteration 7286 Loss = 0.04020815365118538\n",
            "Iteration 7287 Loss = 0.04020608430477258\n",
            "Iteration 7288 Loss = 0.04020401611865117\n",
            "Iteration 7289 Loss = 0.04020194909217058\n",
            "Iteration 7290 Loss = 0.04019988322468075\n",
            "Iteration 7291 Loss = 0.04019781851553169\n",
            "Iteration 7292 Loss = 0.04019575496407406\n",
            "Iteration 7293 Loss = 0.040193692569658435\n",
            "Iteration 7294 Loss = 0.0401916313316364\n",
            "Iteration 7295 Loss = 0.04018957124935933\n",
            "Iteration 7296 Loss = 0.04018751232217942\n",
            "Iteration 7297 Loss = 0.040185454549448746\n",
            "Iteration 7298 Loss = 0.04018339793052013\n",
            "Iteration 7299 Loss = 0.040181342464746674\n",
            "Iteration 7300 Loss = 0.04017928815148172\n",
            "Iteration 7301 Loss = 0.040177234990079136\n",
            "Iteration 7302 Loss = 0.04017518297989296\n",
            "Iteration 7303 Loss = 0.04017313212027766\n",
            "Iteration 7304 Loss = 0.04017108241058833\n",
            "Iteration 7305 Loss = 0.04016903385018002\n",
            "Iteration 7306 Loss = 0.04016698643840827\n",
            "Iteration 7307 Loss = 0.04016494017462929\n",
            "Iteration 7308 Loss = 0.040162895058199116\n",
            "Iteration 7309 Loss = 0.0401608510884746\n",
            "Iteration 7310 Loss = 0.04015880826481273\n",
            "Iteration 7311 Loss = 0.04015676658657091\n",
            "Iteration 7312 Loss = 0.0401547260531069\n",
            "Iteration 7313 Loss = 0.04015268666377874\n",
            "Iteration 7314 Loss = 0.04015064841794509\n",
            "Iteration 7315 Loss = 0.04014861131496459\n",
            "Iteration 7316 Loss = 0.04014657535419653\n",
            "Iteration 7317 Loss = 0.040144540535000356\n",
            "Iteration 7318 Loss = 0.040142506856736204\n",
            "Iteration 7319 Loss = 0.04014047431876427\n",
            "Iteration 7320 Loss = 0.0401384429204451\n",
            "Iteration 7321 Loss = 0.04013641266113967\n",
            "Iteration 7322 Loss = 0.04013438354020946\n",
            "Iteration 7323 Loss = 0.04013235555701615\n",
            "Iteration 7324 Loss = 0.04013032871092175\n",
            "Iteration 7325 Loss = 0.04012830300128871\n",
            "Iteration 7326 Loss = 0.04012627842747969\n",
            "Iteration 7327 Loss = 0.04012425498885797\n",
            "Iteration 7328 Loss = 0.040122232684787075\n",
            "Iteration 7329 Loss = 0.040120211514630744\n",
            "Iteration 7330 Loss = 0.040118191477753265\n",
            "Iteration 7331 Loss = 0.04011617257351931\n",
            "Iteration 7332 Loss = 0.040114154801293433\n",
            "Iteration 7333 Loss = 0.04011213816044128\n",
            "Iteration 7334 Loss = 0.040110122650328345\n",
            "Iteration 7335 Loss = 0.040108108270320575\n",
            "Iteration 7336 Loss = 0.04010609501978449\n",
            "Iteration 7337 Loss = 0.04010408289808652\n",
            "Iteration 7338 Loss = 0.04010207190459396\n",
            "Iteration 7339 Loss = 0.04010006203867407\n",
            "Iteration 7340 Loss = 0.04009805329969465\n",
            "Iteration 7341 Loss = 0.04009604568702391\n",
            "Iteration 7342 Loss = 0.04009403920003023\n",
            "Iteration 7343 Loss = 0.040092033838082484\n",
            "Iteration 7344 Loss = 0.04009002960054985\n",
            "Iteration 7345 Loss = 0.04008802648680184\n",
            "Iteration 7346 Loss = 0.04008602449620825\n",
            "Iteration 7347 Loss = 0.04008402362813955\n",
            "Iteration 7348 Loss = 0.04008202388196619\n",
            "Iteration 7349 Loss = 0.04008002525705911\n",
            "Iteration 7350 Loss = 0.04007802775278966\n",
            "Iteration 7351 Loss = 0.04007603136852945\n",
            "Iteration 7352 Loss = 0.040074036103650415\n",
            "Iteration 7353 Loss = 0.04007204195752519\n",
            "Iteration 7354 Loss = 0.040070048929526245\n",
            "Iteration 7355 Loss = 0.040068057019026604\n",
            "Iteration 7356 Loss = 0.04006606622539985\n",
            "Iteration 7357 Loss = 0.040064076548019634\n",
            "Iteration 7358 Loss = 0.040062087986260216\n",
            "Iteration 7359 Loss = 0.0400601005394959\n",
            "Iteration 7360 Loss = 0.04005811420710147\n",
            "Iteration 7361 Loss = 0.04005612898845226\n",
            "Iteration 7362 Loss = 0.040054144882923776\n",
            "Iteration 7363 Loss = 0.040052161889891656\n",
            "Iteration 7364 Loss = 0.04005018000873231\n",
            "Iteration 7365 Loss = 0.04004819923882236\n",
            "Iteration 7366 Loss = 0.04004621957953868\n",
            "Iteration 7367 Loss = 0.0400442410302584\n",
            "Iteration 7368 Loss = 0.04004226359035931\n",
            "Iteration 7369 Loss = 0.04004028725921927\n",
            "Iteration 7370 Loss = 0.040038312036216696\n",
            "Iteration 7371 Loss = 0.04003633792073008\n",
            "Iteration 7372 Loss = 0.04003436491213862\n",
            "Iteration 7373 Loss = 0.040032393009821504\n",
            "Iteration 7374 Loss = 0.04003042221315857\n",
            "Iteration 7375 Loss = 0.040028452521529874\n",
            "Iteration 7376 Loss = 0.040026483934315786\n",
            "Iteration 7377 Loss = 0.04002451645089687\n",
            "Iteration 7378 Loss = 0.04002255007065457\n",
            "Iteration 7379 Loss = 0.040020584792970104\n",
            "Iteration 7380 Loss = 0.040018620617225234\n",
            "Iteration 7381 Loss = 0.0400166575428023\n",
            "Iteration 7382 Loss = 0.04001469556908362\n",
            "Iteration 7383 Loss = 0.04001273469545211\n",
            "Iteration 7384 Loss = 0.04001077492129083\n",
            "Iteration 7385 Loss = 0.040008816245983396\n",
            "Iteration 7386 Loss = 0.04000685866891362\n",
            "Iteration 7387 Loss = 0.0400049021894658\n",
            "Iteration 7388 Loss = 0.04000294680702444\n",
            "Iteration 7389 Loss = 0.040000992520974554\n",
            "Iteration 7390 Loss = 0.03999903933070123\n",
            "Iteration 7391 Loss = 0.03999708723559001\n",
            "Iteration 7392 Loss = 0.039995136235027054\n",
            "Iteration 7393 Loss = 0.03999318632839848\n",
            "Iteration 7394 Loss = 0.039991237515091\n",
            "Iteration 7395 Loss = 0.0399892897944915\n",
            "Iteration 7396 Loss = 0.03998734316598739\n",
            "Iteration 7397 Loss = 0.03998539762896626\n",
            "Iteration 7398 Loss = 0.0399834531828161\n",
            "Iteration 7399 Loss = 0.0399815098269253\n",
            "Iteration 7400 Loss = 0.03997956756068266\n",
            "Iteration 7401 Loss = 0.039977626383476954\n",
            "Iteration 7402 Loss = 0.03997568629469772\n",
            "Iteration 7403 Loss = 0.039973747293734645\n",
            "Iteration 7404 Loss = 0.039971809379977664\n",
            "Iteration 7405 Loss = 0.0399698725528174\n",
            "Iteration 7406 Loss = 0.039967936811644525\n",
            "Iteration 7407 Loss = 0.039966002155849945\n",
            "Iteration 7408 Loss = 0.039964068584825256\n",
            "Iteration 7409 Loss = 0.03996213609796223\n",
            "Iteration 7410 Loss = 0.03996020469465286\n",
            "Iteration 7411 Loss = 0.03995827437428974\n",
            "Iteration 7412 Loss = 0.03995634513626554\n",
            "Iteration 7413 Loss = 0.03995441697997344\n",
            "Iteration 7414 Loss = 0.03995248990480689\n",
            "Iteration 7415 Loss = 0.039950563910159595\n",
            "Iteration 7416 Loss = 0.03994863899542593\n",
            "Iteration 7417 Loss = 0.03994671516000025\n",
            "Iteration 7418 Loss = 0.03994479240327739\n",
            "Iteration 7419 Loss = 0.03994287072465254\n",
            "Iteration 7420 Loss = 0.039940950123521085\n",
            "Iteration 7421 Loss = 0.039939030599279085\n",
            "Iteration 7422 Loss = 0.03993711215132247\n",
            "Iteration 7423 Loss = 0.03993519477904797\n",
            "Iteration 7424 Loss = 0.039933278481852436\n",
            "Iteration 7425 Loss = 0.03993136325913295\n",
            "Iteration 7426 Loss = 0.03992944911028722\n",
            "Iteration 7427 Loss = 0.039927536034712795\n",
            "Iteration 7428 Loss = 0.03992562403180822\n",
            "Iteration 7429 Loss = 0.039923713100971904\n",
            "Iteration 7430 Loss = 0.03992180324160279\n",
            "Iteration 7431 Loss = 0.03991989445310003\n",
            "Iteration 7432 Loss = 0.03991798673486332\n",
            "Iteration 7433 Loss = 0.03991608008629233\n",
            "Iteration 7434 Loss = 0.03991417450678745\n",
            "Iteration 7435 Loss = 0.03991226999574924\n",
            "Iteration 7436 Loss = 0.03991036655257855\n",
            "Iteration 7437 Loss = 0.0399084641766767\n",
            "Iteration 7438 Loss = 0.03990656286744514\n",
            "Iteration 7439 Loss = 0.03990466262428599\n",
            "Iteration 7440 Loss = 0.03990276344660131\n",
            "Iteration 7441 Loss = 0.03990086533379377\n",
            "Iteration 7442 Loss = 0.039898968285266276\n",
            "Iteration 7443 Loss = 0.03989707230042205\n",
            "Iteration 7444 Loss = 0.03989517737866479\n",
            "Iteration 7445 Loss = 0.03989328351939821\n",
            "Iteration 7446 Loss = 0.03989139072202678\n",
            "Iteration 7447 Loss = 0.03988949898595499\n",
            "Iteration 7448 Loss = 0.03988760831058773\n",
            "Iteration 7449 Loss = 0.03988571869533031\n",
            "Iteration 7450 Loss = 0.03988383013958839\n",
            "Iteration 7451 Loss = 0.039881942642767794\n",
            "Iteration 7452 Loss = 0.03988005620427473\n",
            "Iteration 7453 Loss = 0.039878170823515906\n",
            "Iteration 7454 Loss = 0.03987628649989827\n",
            "Iteration 7455 Loss = 0.03987440323282895\n",
            "Iteration 7456 Loss = 0.03987252102171566\n",
            "Iteration 7457 Loss = 0.039870639865966194\n",
            "Iteration 7458 Loss = 0.03986875976498891\n",
            "Iteration 7459 Loss = 0.03986688071819241\n",
            "Iteration 7460 Loss = 0.03986500272498541\n",
            "Iteration 7461 Loss = 0.03986312578477747\n",
            "Iteration 7462 Loss = 0.03986124989697792\n",
            "Iteration 7463 Loss = 0.03985937506099677\n",
            "Iteration 7464 Loss = 0.039857501276244205\n",
            "Iteration 7465 Loss = 0.03985562854213088\n",
            "Iteration 7466 Loss = 0.03985375685806759\n",
            "Iteration 7467 Loss = 0.03985188622346575\n",
            "Iteration 7468 Loss = 0.03985001663773671\n",
            "Iteration 7469 Loss = 0.03984814810029248\n",
            "Iteration 7470 Loss = 0.03984628061054521\n",
            "Iteration 7471 Loss = 0.039844414167907544\n",
            "Iteration 7472 Loss = 0.03984254877179237\n",
            "Iteration 7473 Loss = 0.039840684421612746\n",
            "Iteration 7474 Loss = 0.039838821116782365\n",
            "Iteration 7475 Loss = 0.03983695885671504\n",
            "Iteration 7476 Loss = 0.03983509764082496\n",
            "Iteration 7477 Loss = 0.039833237468526614\n",
            "Iteration 7478 Loss = 0.039831378339234974\n",
            "Iteration 7479 Loss = 0.039829520252365115\n",
            "Iteration 7480 Loss = 0.03982766320733261\n",
            "Iteration 7481 Loss = 0.03982580720355324\n",
            "Iteration 7482 Loss = 0.03982395224044318\n",
            "Iteration 7483 Loss = 0.03982209831741893\n",
            "Iteration 7484 Loss = 0.03982024543389741\n",
            "Iteration 7485 Loss = 0.03981839358929564\n",
            "Iteration 7486 Loss = 0.03981654278303108\n",
            "Iteration 7487 Loss = 0.03981469301452154\n",
            "Iteration 7488 Loss = 0.03981284428318528\n",
            "Iteration 7489 Loss = 0.03981099658844058\n",
            "Iteration 7490 Loss = 0.03980914992970635\n",
            "Iteration 7491 Loss = 0.03980730430640164\n",
            "Iteration 7492 Loss = 0.03980545971794592\n",
            "Iteration 7493 Loss = 0.03980361616375884\n",
            "Iteration 7494 Loss = 0.03980177364326059\n",
            "Iteration 7495 Loss = 0.03979993215587152\n",
            "Iteration 7496 Loss = 0.03979809170101226\n",
            "Iteration 7497 Loss = 0.039796252278104106\n",
            "Iteration 7498 Loss = 0.03979441388656832\n",
            "Iteration 7499 Loss = 0.039792576525826545\n",
            "Iteration 7500 Loss = 0.03979074019530088\n",
            "Iteration 7501 Loss = 0.03978890489441369\n",
            "Iteration 7502 Loss = 0.0397870706225876\n",
            "Iteration 7503 Loss = 0.039785237379245574\n",
            "Iteration 7504 Loss = 0.03978340516381108\n",
            "Iteration 7505 Loss = 0.039781573975707696\n",
            "Iteration 7506 Loss = 0.03977974381435933\n",
            "Iteration 7507 Loss = 0.039777914679190375\n",
            "Iteration 7508 Loss = 0.03977608656962538\n",
            "Iteration 7509 Loss = 0.039774259485089294\n",
            "Iteration 7510 Loss = 0.039772433425007366\n",
            "Iteration 7511 Loss = 0.0397706083888053\n",
            "Iteration 7512 Loss = 0.03976878437590877\n",
            "Iteration 7513 Loss = 0.03976696138574432\n",
            "Iteration 7514 Loss = 0.03976513941773819\n",
            "Iteration 7515 Loss = 0.03976331847131735\n",
            "Iteration 7516 Loss = 0.03976149854590916\n",
            "Iteration 7517 Loss = 0.0397596796409409\n",
            "Iteration 7518 Loss = 0.03975786175584053\n",
            "Iteration 7519 Loss = 0.03975604489003618\n",
            "Iteration 7520 Loss = 0.03975422904295628\n",
            "Iteration 7521 Loss = 0.03975241421402971\n",
            "Iteration 7522 Loss = 0.039750600402685625\n",
            "Iteration 7523 Loss = 0.03974878760835333\n",
            "Iteration 7524 Loss = 0.0397469758304626\n",
            "Iteration 7525 Loss = 0.03974516506844359\n",
            "Iteration 7526 Loss = 0.03974335532172665\n",
            "Iteration 7527 Loss = 0.03974154658974247\n",
            "Iteration 7528 Loss = 0.039739738871922094\n",
            "Iteration 7529 Loss = 0.039737932167696964\n",
            "Iteration 7530 Loss = 0.03973612647649871\n",
            "Iteration 7531 Loss = 0.03973432179775925\n",
            "Iteration 7532 Loss = 0.03973251813091095\n",
            "Iteration 7533 Loss = 0.03973071547538647\n",
            "Iteration 7534 Loss = 0.03972891383061865\n",
            "Iteration 7535 Loss = 0.03972711319604088\n",
            "Iteration 7536 Loss = 0.03972531357108672\n",
            "Iteration 7537 Loss = 0.039723514955189956\n",
            "Iteration 7538 Loss = 0.03972171734778494\n",
            "Iteration 7539 Loss = 0.03971992074830609\n",
            "Iteration 7540 Loss = 0.03971812515618836\n",
            "Iteration 7541 Loss = 0.03971633057086691\n",
            "Iteration 7542 Loss = 0.03971453699177713\n",
            "Iteration 7543 Loss = 0.039712744418354894\n",
            "Iteration 7544 Loss = 0.039710952850036396\n",
            "Iteration 7545 Loss = 0.03970916228625787\n",
            "Iteration 7546 Loss = 0.03970737272645625\n",
            "Iteration 7547 Loss = 0.03970558417006855\n",
            "Iteration 7548 Loss = 0.03970379661653202\n",
            "Iteration 7549 Loss = 0.03970201006528459\n",
            "Iteration 7550 Loss = 0.03970022451576408\n",
            "Iteration 7551 Loss = 0.039698439967408974\n",
            "Iteration 7552 Loss = 0.03969665641965769\n",
            "Iteration 7553 Loss = 0.03969487387194947\n",
            "Iteration 7554 Loss = 0.039693092323723395\n",
            "Iteration 7555 Loss = 0.03969131177441906\n",
            "Iteration 7556 Loss = 0.03968953222347637\n",
            "Iteration 7557 Loss = 0.03968775367033566\n",
            "Iteration 7558 Loss = 0.03968597611443735\n",
            "Iteration 7559 Loss = 0.039684199555222247\n",
            "Iteration 7560 Loss = 0.03968242399213165\n",
            "Iteration 7561 Loss = 0.03968064942460696\n",
            "Iteration 7562 Loss = 0.03967887585208977\n",
            "Iteration 7563 Loss = 0.039677103274022456\n",
            "Iteration 7564 Loss = 0.039675331689847354\n",
            "Iteration 7565 Loss = 0.03967356109900708\n",
            "Iteration 7566 Loss = 0.03967179150094469\n",
            "Iteration 7567 Loss = 0.03967002289510358\n",
            "Iteration 7568 Loss = 0.039668255280927475\n",
            "Iteration 7569 Loss = 0.039666488657860204\n",
            "Iteration 7570 Loss = 0.03966472302534615\n",
            "Iteration 7571 Loss = 0.039662958382829835\n",
            "Iteration 7572 Loss = 0.039661194729756176\n",
            "Iteration 7573 Loss = 0.039659432065570446\n",
            "Iteration 7574 Loss = 0.03965767038971808\n",
            "Iteration 7575 Loss = 0.03965590970164501\n",
            "Iteration 7576 Loss = 0.03965415000079729\n",
            "Iteration 7577 Loss = 0.03965239128662146\n",
            "Iteration 7578 Loss = 0.039650633558564234\n",
            "Iteration 7579 Loss = 0.03964887681607275\n",
            "Iteration 7580 Loss = 0.03964712105859434\n",
            "Iteration 7581 Loss = 0.03964536628557682\n",
            "Iteration 7582 Loss = 0.039643612496467945\n",
            "Iteration 7583 Loss = 0.03964185969071626\n",
            "Iteration 7584 Loss = 0.03964010786777036\n",
            "Iteration 7585 Loss = 0.03963835702707908\n",
            "Iteration 7586 Loss = 0.03963660716809171\n",
            "Iteration 7587 Loss = 0.039634858290257854\n",
            "Iteration 7588 Loss = 0.03963311039302736\n",
            "Iteration 7589 Loss = 0.03963136347585038\n",
            "Iteration 7590 Loss = 0.03962961753817751\n",
            "Iteration 7591 Loss = 0.0396278725794592\n",
            "Iteration 7592 Loss = 0.03962612859914696\n",
            "Iteration 7593 Loss = 0.039624385596691826\n",
            "Iteration 7594 Loss = 0.0396226435715458\n",
            "Iteration 7595 Loss = 0.039620902523160854\n",
            "Iteration 7596 Loss = 0.03961916245098915\n",
            "Iteration 7597 Loss = 0.03961742335448343\n",
            "Iteration 7598 Loss = 0.03961568523309668\n",
            "Iteration 7599 Loss = 0.03961394808628213\n",
            "Iteration 7600 Loss = 0.039612211913493164\n",
            "Iteration 7601 Loss = 0.0396104767141838\n",
            "Iteration 7602 Loss = 0.0396087424878082\n",
            "Iteration 7603 Loss = 0.03960700923382082\n",
            "Iteration 7604 Loss = 0.039605276951676406\n",
            "Iteration 7605 Loss = 0.039603545640830067\n",
            "Iteration 7606 Loss = 0.03960181530073723\n",
            "Iteration 7607 Loss = 0.03960008593085357\n",
            "Iteration 7608 Loss = 0.039598357530634884\n",
            "Iteration 7609 Loss = 0.039596630099537955\n",
            "Iteration 7610 Loss = 0.03959490363701883\n",
            "Iteration 7611 Loss = 0.03959317814253488\n",
            "Iteration 7612 Loss = 0.039591453615542976\n",
            "Iteration 7613 Loss = 0.0395897300555009\n",
            "Iteration 7614 Loss = 0.03958800746186626\n",
            "Iteration 7615 Loss = 0.03958628583409744\n",
            "Iteration 7616 Loss = 0.03958456517165271\n",
            "Iteration 7617 Loss = 0.03958284547399076\n",
            "Iteration 7618 Loss = 0.03958112674057079\n",
            "Iteration 7619 Loss = 0.03957940897085194\n",
            "Iteration 7620 Loss = 0.03957769216429405\n",
            "Iteration 7621 Loss = 0.03957597632035716\n",
            "Iteration 7622 Loss = 0.03957426143850114\n",
            "Iteration 7623 Loss = 0.03957254751818702\n",
            "Iteration 7624 Loss = 0.03957083455887521\n",
            "Iteration 7625 Loss = 0.0395691225600271\n",
            "Iteration 7626 Loss = 0.03956741152110418\n",
            "Iteration 7627 Loss = 0.03956570144156808\n",
            "Iteration 7628 Loss = 0.03956399232088106\n",
            "Iteration 7629 Loss = 0.039562284158505295\n",
            "Iteration 7630 Loss = 0.039560576953903574\n",
            "Iteration 7631 Loss = 0.039558870706538976\n",
            "Iteration 7632 Loss = 0.039557165415874465\n",
            "Iteration 7633 Loss = 0.039555461081373824\n",
            "Iteration 7634 Loss = 0.03955375770250095\n",
            "Iteration 7635 Loss = 0.03955205527871987\n",
            "Iteration 7636 Loss = 0.03955035380949522\n",
            "Iteration 7637 Loss = 0.03954865329429172\n",
            "Iteration 7638 Loss = 0.039546953732574344\n",
            "Iteration 7639 Loss = 0.03954525512380861\n",
            "Iteration 7640 Loss = 0.03954355746746011\n",
            "Iteration 7641 Loss = 0.03954186076299492\n",
            "Iteration 7642 Loss = 0.03954016500987921\n",
            "Iteration 7643 Loss = 0.03953847020757948\n",
            "Iteration 7644 Loss = 0.03953677635556279\n",
            "Iteration 7645 Loss = 0.03953508345329633\n",
            "Iteration 7646 Loss = 0.03953339150024736\n",
            "Iteration 7647 Loss = 0.039531700495883844\n",
            "Iteration 7648 Loss = 0.039530010439673725\n",
            "Iteration 7649 Loss = 0.039528321331085414\n",
            "Iteration 7650 Loss = 0.039526633169587586\n",
            "Iteration 7651 Loss = 0.039524945954649206\n",
            "Iteration 7652 Loss = 0.03952325968573951\n",
            "Iteration 7653 Loss = 0.03952157436232813\n",
            "Iteration 7654 Loss = 0.03951988998388472\n",
            "Iteration 7655 Loss = 0.0395182065498796\n",
            "Iteration 7656 Loss = 0.03951652405978329\n",
            "Iteration 7657 Loss = 0.03951484251306639\n",
            "Iteration 7658 Loss = 0.03951316190919995\n",
            "Iteration 7659 Loss = 0.03951148224765533\n",
            "Iteration 7660 Loss = 0.039509803527904265\n",
            "Iteration 7661 Loss = 0.039508125749418574\n",
            "Iteration 7662 Loss = 0.039506448911670544\n",
            "Iteration 7663 Loss = 0.039504773014132674\n",
            "Iteration 7664 Loss = 0.039503098056277615\n",
            "Iteration 7665 Loss = 0.039501424037578906\n",
            "Iteration 7666 Loss = 0.03949975095750949\n",
            "Iteration 7667 Loss = 0.03949807881554347\n",
            "Iteration 7668 Loss = 0.039496407611154596\n",
            "Iteration 7669 Loss = 0.03949473734381717\n",
            "Iteration 7670 Loss = 0.039493068013005936\n",
            "Iteration 7671 Loss = 0.03949139961819569\n",
            "Iteration 7672 Loss = 0.03948973215886147\n",
            "Iteration 7673 Loss = 0.039488065634479125\n",
            "Iteration 7674 Loss = 0.03948640004452397\n",
            "Iteration 7675 Loss = 0.0394847353884725\n",
            "Iteration 7676 Loss = 0.03948307166580067\n",
            "Iteration 7677 Loss = 0.039481408875985384\n",
            "Iteration 7678 Loss = 0.03947974701850354\n",
            "Iteration 7679 Loss = 0.039478086092832355\n",
            "Iteration 7680 Loss = 0.03947642609844939\n",
            "Iteration 7681 Loss = 0.03947476703483233\n",
            "Iteration 7682 Loss = 0.039473108901459414\n",
            "Iteration 7683 Loss = 0.0394714516978092\n",
            "Iteration 7684 Loss = 0.03946979542336007\n",
            "Iteration 7685 Loss = 0.03946814007759124\n",
            "Iteration 7686 Loss = 0.03946648565998189\n",
            "Iteration 7687 Loss = 0.03946483217001164\n",
            "Iteration 7688 Loss = 0.03946317960716036\n",
            "Iteration 7689 Loss = 0.03946152797090815\n",
            "Iteration 7690 Loss = 0.03945987726073563\n",
            "Iteration 7691 Loss = 0.03945822747612332\n",
            "Iteration 7692 Loss = 0.039456578616552315\n",
            "Iteration 7693 Loss = 0.039454930681504165\n",
            "Iteration 7694 Loss = 0.03945328367046017\n",
            "Iteration 7695 Loss = 0.03945163758290239\n",
            "Iteration 7696 Loss = 0.039449992418313005\n",
            "Iteration 7697 Loss = 0.039448348176174486\n",
            "Iteration 7698 Loss = 0.03944670485596972\n",
            "Iteration 7699 Loss = 0.039445062457181594\n",
            "Iteration 7700 Loss = 0.039443420979293556\n",
            "Iteration 7701 Loss = 0.03944178042178929\n",
            "Iteration 7702 Loss = 0.039440140784152686\n",
            "Iteration 7703 Loss = 0.039438502065868006\n",
            "Iteration 7704 Loss = 0.039436864266419666\n",
            "Iteration 7705 Loss = 0.039435227385292565\n",
            "Iteration 7706 Loss = 0.03943359142197183\n",
            "Iteration 7707 Loss = 0.039431956375942705\n",
            "Iteration 7708 Loss = 0.039430322246691005\n",
            "Iteration 7709 Loss = 0.039428689033702585\n",
            "Iteration 7710 Loss = 0.03942705673646382\n",
            "Iteration 7711 Loss = 0.03942542535446105\n",
            "Iteration 7712 Loss = 0.039423794887181354\n",
            "Iteration 7713 Loss = 0.039422165334111615\n",
            "Iteration 7714 Loss = 0.039420536694739196\n",
            "Iteration 7715 Loss = 0.039418908968552054\n",
            "Iteration 7716 Loss = 0.03941728215503798\n",
            "Iteration 7717 Loss = 0.039415656253685226\n",
            "Iteration 7718 Loss = 0.03941403126398236\n",
            "Iteration 7719 Loss = 0.03941240718541821\n",
            "Iteration 7720 Loss = 0.039410784017482034\n",
            "Iteration 7721 Loss = 0.03940916175966304\n",
            "Iteration 7722 Loss = 0.03940754041145098\n",
            "Iteration 7723 Loss = 0.03940591997233596\n",
            "Iteration 7724 Loss = 0.03940430044180811\n",
            "Iteration 7725 Loss = 0.039402681819357945\n",
            "Iteration 7726 Loss = 0.03940106410447641\n",
            "Iteration 7727 Loss = 0.0393994472966546\n",
            "Iteration 7728 Loss = 0.03939783139538389\n",
            "Iteration 7729 Loss = 0.03939621640015604\n",
            "Iteration 7730 Loss = 0.039394602310463\n",
            "Iteration 7731 Loss = 0.0393929891257969\n",
            "Iteration 7732 Loss = 0.0393913768456505\n",
            "Iteration 7733 Loss = 0.03938976546951653\n",
            "Iteration 7734 Loss = 0.03938815499688806\n",
            "Iteration 7735 Loss = 0.039386545427258515\n",
            "Iteration 7736 Loss = 0.039384936760121676\n",
            "Iteration 7737 Loss = 0.03938332899497144\n",
            "Iteration 7738 Loss = 0.0393817221313021\n",
            "Iteration 7739 Loss = 0.03938011616860804\n",
            "Iteration 7740 Loss = 0.03937851110638425\n",
            "Iteration 7741 Loss = 0.0393769069441257\n",
            "Iteration 7742 Loss = 0.039375303681327925\n",
            "Iteration 7743 Loss = 0.03937370131748654\n",
            "Iteration 7744 Loss = 0.03937209985209749\n",
            "Iteration 7745 Loss = 0.03937049928465691\n",
            "Iteration 7746 Loss = 0.039368899614661404\n",
            "Iteration 7747 Loss = 0.03936730084160778\n",
            "Iteration 7748 Loss = 0.039365702964993185\n",
            "Iteration 7749 Loss = 0.039364105984314814\n",
            "Iteration 7750 Loss = 0.03936250989907047\n",
            "Iteration 7751 Loss = 0.039360914708758014\n",
            "Iteration 7752 Loss = 0.03935932041287554\n",
            "Iteration 7753 Loss = 0.039357727010921696\n",
            "Iteration 7754 Loss = 0.03935613450239524\n",
            "Iteration 7755 Loss = 0.039354542886795144\n",
            "Iteration 7756 Loss = 0.039352952163620766\n",
            "Iteration 7757 Loss = 0.03935136233237182\n",
            "Iteration 7758 Loss = 0.03934977339254802\n",
            "Iteration 7759 Loss = 0.03934818534364971\n",
            "Iteration 7760 Loss = 0.03934659818517719\n",
            "Iteration 7761 Loss = 0.039345011916631284\n",
            "Iteration 7762 Loss = 0.03934342653751309\n",
            "Iteration 7763 Loss = 0.03934184204732365\n",
            "Iteration 7764 Loss = 0.039340258445564866\n",
            "Iteration 7765 Loss = 0.03933867573173835\n",
            "Iteration 7766 Loss = 0.03933709390534629\n",
            "Iteration 7767 Loss = 0.039335512965891134\n",
            "Iteration 7768 Loss = 0.03933393291287554\n",
            "Iteration 7769 Loss = 0.0393323537458025\n",
            "Iteration 7770 Loss = 0.03933077546417524\n",
            "Iteration 7771 Loss = 0.039329198067497344\n",
            "Iteration 7772 Loss = 0.039327621555272474\n",
            "Iteration 7773 Loss = 0.03932604592700492\n",
            "Iteration 7774 Loss = 0.03932447118219884\n",
            "Iteration 7775 Loss = 0.039322897320358985\n",
            "Iteration 7776 Loss = 0.039321324340990234\n",
            "Iteration 7777 Loss = 0.0393197522435979\n",
            "Iteration 7778 Loss = 0.03931818102768725\n",
            "Iteration 7779 Loss = 0.03931661069276422\n",
            "Iteration 7780 Loss = 0.0393150412383347\n",
            "Iteration 7781 Loss = 0.03931347266390519\n",
            "Iteration 7782 Loss = 0.03931190496898196\n",
            "Iteration 7783 Loss = 0.03931033815307216\n",
            "Iteration 7784 Loss = 0.03930877221568264\n",
            "Iteration 7785 Loss = 0.03930720715632108\n",
            "Iteration 7786 Loss = 0.03930564297449505\n",
            "Iteration 7787 Loss = 0.039304079669712506\n",
            "Iteration 7788 Loss = 0.039302517241481774\n",
            "Iteration 7789 Loss = 0.03930095568931116\n",
            "Iteration 7790 Loss = 0.03929939501270957\n",
            "Iteration 7791 Loss = 0.03929783521118624\n",
            "Iteration 7792 Loss = 0.039296276284250295\n",
            "Iteration 7793 Loss = 0.039294718231411375\n",
            "Iteration 7794 Loss = 0.03929316105217945\n",
            "Iteration 7795 Loss = 0.03929160474606462\n",
            "Iteration 7796 Loss = 0.03929004931257726\n",
            "Iteration 7797 Loss = 0.03928849475122821\n",
            "Iteration 7798 Loss = 0.039286941061528496\n",
            "Iteration 7799 Loss = 0.03928538824298926\n",
            "Iteration 7800 Loss = 0.03928383629512204\n",
            "Iteration 7801 Loss = 0.0392822852174387\n",
            "Iteration 7802 Loss = 0.03928073500945133\n",
            "Iteration 7803 Loss = 0.03927918567067229\n",
            "Iteration 7804 Loss = 0.039277637200614175\n",
            "Iteration 7805 Loss = 0.03927608959878991\n",
            "Iteration 7806 Loss = 0.0392745428647126\n",
            "Iteration 7807 Loss = 0.03927299699789581\n",
            "Iteration 7808 Loss = 0.03927145199785319\n",
            "Iteration 7809 Loss = 0.03926990786409871\n",
            "Iteration 7810 Loss = 0.03926836459614677\n",
            "Iteration 7811 Loss = 0.039266822193511824\n",
            "Iteration 7812 Loss = 0.039265280655708695\n",
            "Iteration 7813 Loss = 0.039263739982252466\n",
            "Iteration 7814 Loss = 0.03926220017265843\n",
            "Iteration 7815 Loss = 0.03926066122644238\n",
            "Iteration 7816 Loss = 0.03925912314312004\n",
            "Iteration 7817 Loss = 0.03925758592220765\n",
            "Iteration 7818 Loss = 0.039256049563221665\n",
            "Iteration 7819 Loss = 0.039254514065678814\n",
            "Iteration 7820 Loss = 0.039252979429096085\n",
            "Iteration 7821 Loss = 0.039251445652990595\n",
            "Iteration 7822 Loss = 0.03924991273688011\n",
            "Iteration 7823 Loss = 0.03924838068028226\n",
            "Iteration 7824 Loss = 0.03924684948271514\n",
            "Iteration 7825 Loss = 0.03924531914369704\n",
            "Iteration 7826 Loss = 0.039243789662746754\n",
            "Iteration 7827 Loss = 0.039242261039383\n",
            "Iteration 7828 Loss = 0.039240733273124884\n",
            "Iteration 7829 Loss = 0.03923920636349193\n",
            "Iteration 7830 Loss = 0.03923768031000386\n",
            "Iteration 7831 Loss = 0.03923615511218056\n",
            "Iteration 7832 Loss = 0.03923463076954218\n",
            "Iteration 7833 Loss = 0.039233107281609295\n",
            "Iteration 7834 Loss = 0.03923158464790276\n",
            "Iteration 7835 Loss = 0.03923006286794343\n",
            "Iteration 7836 Loss = 0.039228541941252776\n",
            "Iteration 7837 Loss = 0.039227021867352205\n",
            "Iteration 7838 Loss = 0.039225502645763646\n",
            "Iteration 7839 Loss = 0.0392239842760092\n",
            "Iteration 7840 Loss = 0.03922246675761124\n",
            "Iteration 7841 Loss = 0.03922095009009234\n",
            "Iteration 7842 Loss = 0.03921943427297547\n",
            "Iteration 7843 Loss = 0.039217919305783916\n",
            "Iteration 7844 Loss = 0.039216405188040857\n",
            "Iteration 7845 Loss = 0.039214891919270224\n",
            "Iteration 7846 Loss = 0.039213379498995865\n",
            "Iteration 7847 Loss = 0.039211867926742076\n",
            "Iteration 7848 Loss = 0.03921035720203344\n",
            "Iteration 7849 Loss = 0.03920884732439469\n",
            "Iteration 7850 Loss = 0.03920733829335078\n",
            "Iteration 7851 Loss = 0.03920583010842712\n",
            "Iteration 7852 Loss = 0.03920432276914916\n",
            "Iteration 7853 Loss = 0.03920281627504297\n",
            "Iteration 7854 Loss = 0.03920131062563448\n",
            "Iteration 7855 Loss = 0.03919980582045005\n",
            "Iteration 7856 Loss = 0.039198301859016425\n",
            "Iteration 7857 Loss = 0.03919679874086043\n",
            "Iteration 7858 Loss = 0.039195296465509344\n",
            "Iteration 7859 Loss = 0.039193795032490424\n",
            "Iteration 7860 Loss = 0.0391922944413315\n",
            "Iteration 7861 Loss = 0.0391907946915606\n",
            "Iteration 7862 Loss = 0.039189295782705795\n",
            "Iteration 7863 Loss = 0.039187797714295675\n",
            "Iteration 7864 Loss = 0.039186300485859055\n",
            "Iteration 7865 Loss = 0.03918480409692478\n",
            "Iteration 7866 Loss = 0.03918330854702217\n",
            "Iteration 7867 Loss = 0.039181813835680986\n",
            "Iteration 7868 Loss = 0.039180319962430855\n",
            "Iteration 7869 Loss = 0.03917882692680195\n",
            "Iteration 7870 Loss = 0.039177334728324416\n",
            "Iteration 7871 Loss = 0.039175843366529135\n",
            "Iteration 7872 Loss = 0.03917435284094678\n",
            "Iteration 7873 Loss = 0.039172863151108586\n",
            "Iteration 7874 Loss = 0.03917137429654593\n",
            "Iteration 7875 Loss = 0.03916988627679045\n",
            "Iteration 7876 Loss = 0.039168399091373964\n",
            "Iteration 7877 Loss = 0.03916691273982883\n",
            "Iteration 7878 Loss = 0.039165427221687456\n",
            "Iteration 7879 Loss = 0.03916394253648251\n",
            "Iteration 7880 Loss = 0.039162458683746834\n",
            "Iteration 7881 Loss = 0.03916097566301388\n",
            "Iteration 7882 Loss = 0.039159493473817066\n",
            "Iteration 7883 Loss = 0.03915801211569023\n",
            "Iteration 7884 Loss = 0.03915653158816708\n",
            "Iteration 7885 Loss = 0.03915505189078224\n",
            "Iteration 7886 Loss = 0.03915357302307003\n",
            "Iteration 7887 Loss = 0.0391520949845655\n",
            "Iteration 7888 Loss = 0.039150617774803276\n",
            "Iteration 7889 Loss = 0.03914914139331905\n",
            "Iteration 7890 Loss = 0.03914766583964836\n",
            "Iteration 7891 Loss = 0.039146191113326956\n",
            "Iteration 7892 Loss = 0.039144717213890866\n",
            "Iteration 7893 Loss = 0.03914324414087658\n",
            "Iteration 7894 Loss = 0.03914177189382081\n",
            "Iteration 7895 Loss = 0.03914030047226018\n",
            "Iteration 7896 Loss = 0.03913882987573206\n",
            "Iteration 7897 Loss = 0.03913736010377372\n",
            "Iteration 7898 Loss = 0.03913589115592284\n",
            "Iteration 7899 Loss = 0.039134423031717346\n",
            "Iteration 7900 Loss = 0.03913295573069549\n",
            "Iteration 7901 Loss = 0.03913148925239556\n",
            "Iteration 7902 Loss = 0.039130023596356416\n",
            "Iteration 7903 Loss = 0.039128558762116857\n",
            "Iteration 7904 Loss = 0.039127094749216315\n",
            "Iteration 7905 Loss = 0.03912563155719404\n",
            "Iteration 7906 Loss = 0.03912416918558984\n",
            "Iteration 7907 Loss = 0.039122707633943654\n",
            "Iteration 7908 Loss = 0.03912124690179588\n",
            "Iteration 7909 Loss = 0.03911978698868691\n",
            "Iteration 7910 Loss = 0.03911832789415748\n",
            "Iteration 7911 Loss = 0.039116869617748684\n",
            "Iteration 7912 Loss = 0.03911541215900171\n",
            "Iteration 7913 Loss = 0.03911395551745819\n",
            "Iteration 7914 Loss = 0.03911249969265986\n",
            "Iteration 7915 Loss = 0.03911104468414882\n",
            "Iteration 7916 Loss = 0.03910959049146723\n",
            "Iteration 7917 Loss = 0.039108137114157754\n",
            "Iteration 7918 Loss = 0.03910668455176327\n",
            "Iteration 7919 Loss = 0.039105232803826814\n",
            "Iteration 7920 Loss = 0.039103781869891614\n",
            "Iteration 7921 Loss = 0.03910233174950138\n",
            "Iteration 7922 Loss = 0.039100882442199884\n",
            "Iteration 7923 Loss = 0.03909943394753121\n",
            "Iteration 7924 Loss = 0.03909798626503981\n",
            "Iteration 7925 Loss = 0.03909653939427023\n",
            "Iteration 7926 Loss = 0.03909509333476722\n",
            "Iteration 7927 Loss = 0.039093648086076106\n",
            "Iteration 7928 Loss = 0.039092203647742234\n",
            "Iteration 7929 Loss = 0.03909076001931109\n",
            "Iteration 7930 Loss = 0.0390893172003286\n",
            "Iteration 7931 Loss = 0.03908787519034098\n",
            "Iteration 7932 Loss = 0.03908643398889466\n",
            "Iteration 7933 Loss = 0.03908499359553604\n",
            "Iteration 7934 Loss = 0.03908355400981224\n",
            "Iteration 7935 Loss = 0.03908211523127049\n",
            "Iteration 7936 Loss = 0.03908067725945799\n",
            "Iteration 7937 Loss = 0.03907924009392249\n",
            "Iteration 7938 Loss = 0.039077803734211866\n",
            "Iteration 7939 Loss = 0.03907636817987444\n",
            "Iteration 7940 Loss = 0.039074933430458404\n",
            "Iteration 7941 Loss = 0.039073499485512614\n",
            "Iteration 7942 Loss = 0.03907206634458587\n",
            "Iteration 7943 Loss = 0.039070634007227587\n",
            "Iteration 7944 Loss = 0.03906920247298692\n",
            "Iteration 7945 Loss = 0.03906777174141372\n",
            "Iteration 7946 Loss = 0.039066341812057856\n",
            "Iteration 7947 Loss = 0.03906491268446953\n",
            "Iteration 7948 Loss = 0.039063484358199216\n",
            "Iteration 7949 Loss = 0.03906205683279772\n",
            "Iteration 7950 Loss = 0.0390606301078157\n",
            "Iteration 7951 Loss = 0.03905920418280467\n",
            "Iteration 7952 Loss = 0.0390577790573159\n",
            "Iteration 7953 Loss = 0.03905635473090115\n",
            "Iteration 7954 Loss = 0.039054931203112306\n",
            "Iteration 7955 Loss = 0.03905350847350173\n",
            "Iteration 7956 Loss = 0.03905208654162176\n",
            "Iteration 7957 Loss = 0.03905066540702506\n",
            "Iteration 7958 Loss = 0.0390492450692647\n",
            "Iteration 7959 Loss = 0.039047825527893903\n",
            "Iteration 7960 Loss = 0.03904640678246603\n",
            "Iteration 7961 Loss = 0.03904498883253488\n",
            "Iteration 7962 Loss = 0.0390435716776543\n",
            "Iteration 7963 Loss = 0.03904215531737862\n",
            "Iteration 7964 Loss = 0.03904073975126223\n",
            "Iteration 7965 Loss = 0.03903932497885991\n",
            "Iteration 7966 Loss = 0.0390379109997265\n",
            "Iteration 7967 Loss = 0.03903649781341745\n",
            "Iteration 7968 Loss = 0.039035085419487865\n",
            "Iteration 7969 Loss = 0.03903367381749377\n",
            "Iteration 7970 Loss = 0.039032263006990996\n",
            "Iteration 7971 Loss = 0.039030852987535766\n",
            "Iteration 7972 Loss = 0.039029443758684515\n",
            "Iteration 7973 Loss = 0.03902803531999397\n",
            "Iteration 7974 Loss = 0.03902662767102099\n",
            "Iteration 7975 Loss = 0.03902522081132294\n",
            "Iteration 7976 Loss = 0.03902381474045725\n",
            "Iteration 7977 Loss = 0.03902240945798148\n",
            "Iteration 7978 Loss = 0.039021004963453655\n",
            "Iteration 7979 Loss = 0.03901960125643198\n",
            "Iteration 7980 Loss = 0.03901819833647489\n",
            "Iteration 7981 Loss = 0.039016796203141\n",
            "Iteration 7982 Loss = 0.039015394855989496\n",
            "Iteration 7983 Loss = 0.039013994294579295\n",
            "Iteration 7984 Loss = 0.03901259451846988\n",
            "Iteration 7985 Loss = 0.039011195527220974\n",
            "Iteration 7986 Loss = 0.039009797320392614\n",
            "Iteration 7987 Loss = 0.0390083998975447\n",
            "Iteration 7988 Loss = 0.03900700325823781\n",
            "Iteration 7989 Loss = 0.039005607402032696\n",
            "Iteration 7990 Loss = 0.03900421232849012\n",
            "Iteration 7991 Loss = 0.039002818037171365\n",
            "Iteration 7992 Loss = 0.03900142452763771\n",
            "Iteration 7993 Loss = 0.03900003179945089\n",
            "Iteration 7994 Loss = 0.038998639852172765\n",
            "Iteration 7995 Loss = 0.0389972486853655\n",
            "Iteration 7996 Loss = 0.03899585829859145\n",
            "Iteration 7997 Loss = 0.038994468691413264\n",
            "Iteration 7998 Loss = 0.038993079863393815\n",
            "Iteration 7999 Loss = 0.038991691814096195\n",
            "Iteration 8000 Loss = 0.03899030454308385\n",
            "Iteration 8001 Loss = 0.03898891804992035\n",
            "Iteration 8002 Loss = 0.03898753233416964\n",
            "Iteration 8003 Loss = 0.03898614739539559\n",
            "Iteration 8004 Loss = 0.03898476323316268\n",
            "Iteration 8005 Loss = 0.03898337984703555\n",
            "Iteration 8006 Loss = 0.038981997236578955\n",
            "Iteration 8007 Loss = 0.03898061540135812\n",
            "Iteration 8008 Loss = 0.038979234340938196\n",
            "Iteration 8009 Loss = 0.03897785405488475\n",
            "Iteration 8010 Loss = 0.038976474542763685\n",
            "Iteration 8011 Loss = 0.03897509580414105\n",
            "Iteration 8012 Loss = 0.03897371783858306\n",
            "Iteration 8013 Loss = 0.03897234064565638\n",
            "Iteration 8014 Loss = 0.03897096422492762\n",
            "Iteration 8015 Loss = 0.038969588575963963\n",
            "Iteration 8016 Loss = 0.038968213698332574\n",
            "Iteration 8017 Loss = 0.03896683959160105\n",
            "Iteration 8018 Loss = 0.038965466255337054\n",
            "Iteration 8019 Loss = 0.03896409368910863\n",
            "Iteration 8020 Loss = 0.03896272189248408\n",
            "Iteration 8021 Loss = 0.038961350865031774\n",
            "Iteration 8022 Loss = 0.03895998060632045\n",
            "Iteration 8023 Loss = 0.038958611115919105\n",
            "Iteration 8024 Loss = 0.03895724239339696\n",
            "Iteration 8025 Loss = 0.03895587443832342\n",
            "Iteration 8026 Loss = 0.03895450725026819\n",
            "Iteration 8027 Loss = 0.03895314082880119\n",
            "Iteration 8028 Loss = 0.03895177517349262\n",
            "Iteration 8029 Loss = 0.0389504102839129\n",
            "Iteration 8030 Loss = 0.03894904615963264\n",
            "Iteration 8031 Loss = 0.03894768280022269\n",
            "Iteration 8032 Loss = 0.038946320205254355\n",
            "Iteration 8033 Loss = 0.0389449583742988\n",
            "Iteration 8034 Loss = 0.03894359730692779\n",
            "Iteration 8035 Loss = 0.038942237002713126\n",
            "Iteration 8036 Loss = 0.03894087746122683\n",
            "Iteration 8037 Loss = 0.03893951868204141\n",
            "Iteration 8038 Loss = 0.0389381606647293\n",
            "Iteration 8039 Loss = 0.03893680340886329\n",
            "Iteration 8040 Loss = 0.038935446914016646\n",
            "Iteration 8041 Loss = 0.03893409117976242\n",
            "Iteration 8042 Loss = 0.038932736205674334\n",
            "Iteration 8043 Loss = 0.03893138199132587\n",
            "Iteration 8044 Loss = 0.038930028536291406\n",
            "Iteration 8045 Loss = 0.03892867584014499\n",
            "Iteration 8046 Loss = 0.03892732390246103\n",
            "Iteration 8047 Loss = 0.03892597272281453\n",
            "Iteration 8048 Loss = 0.038924622300780164\n",
            "Iteration 8049 Loss = 0.03892327263593331\n",
            "Iteration 8050 Loss = 0.03892192372784939\n",
            "Iteration 8051 Loss = 0.03892057557610397\n",
            "Iteration 8052 Loss = 0.03891922818027303\n",
            "Iteration 8053 Loss = 0.03891788153993285\n",
            "Iteration 8054 Loss = 0.038916535654659704\n",
            "Iteration 8055 Loss = 0.03891519052403018\n",
            "Iteration 8056 Loss = 0.038913846147621306\n",
            "Iteration 8057 Loss = 0.03891250252501001\n",
            "Iteration 8058 Loss = 0.03891115965577371\n",
            "Iteration 8059 Loss = 0.038909817539490014\n",
            "Iteration 8060 Loss = 0.038908476175736714\n",
            "Iteration 8061 Loss = 0.03890713556409176\n",
            "Iteration 8062 Loss = 0.03890579570413359\n",
            "Iteration 8063 Loss = 0.0389044565954407\n",
            "Iteration 8064 Loss = 0.03890311823759182\n",
            "Iteration 8065 Loss = 0.038901780630165986\n",
            "Iteration 8066 Loss = 0.03890044377274242\n",
            "Iteration 8067 Loss = 0.03889910766490055\n",
            "Iteration 8068 Loss = 0.03889777230622005\n",
            "Iteration 8069 Loss = 0.03889643769628103\n",
            "Iteration 8070 Loss = 0.03889510383466344\n",
            "Iteration 8071 Loss = 0.03889377072094791\n",
            "Iteration 8072 Loss = 0.038892438354714995\n",
            "Iteration 8073 Loss = 0.03889110673554552\n",
            "Iteration 8074 Loss = 0.03888977586302064\n",
            "Iteration 8075 Loss = 0.03888844573672181\n",
            "Iteration 8076 Loss = 0.038887116356230524\n",
            "Iteration 8077 Loss = 0.03888578772112865\n",
            "Iteration 8078 Loss = 0.03888445983099813\n",
            "Iteration 8079 Loss = 0.038883132685421394\n",
            "Iteration 8080 Loss = 0.03888180628398079\n",
            "Iteration 8081 Loss = 0.038880480626259296\n",
            "Iteration 8082 Loss = 0.03887915571183976\n",
            "Iteration 8083 Loss = 0.038877831540305455\n",
            "Iteration 8084 Loss = 0.03887650811123989\n",
            "Iteration 8085 Loss = 0.038875185424226716\n",
            "Iteration 8086 Loss = 0.03887386347884978\n",
            "Iteration 8087 Loss = 0.03887254227469342\n",
            "Iteration 8088 Loss = 0.03887122181134189\n",
            "Iteration 8089 Loss = 0.038869902088379815\n",
            "Iteration 8090 Loss = 0.038868583105392165\n",
            "Iteration 8091 Loss = 0.03886726486196398\n",
            "Iteration 8092 Loss = 0.0388659473576805\n",
            "Iteration 8093 Loss = 0.03886463059212738\n",
            "Iteration 8094 Loss = 0.03886331456489032\n",
            "Iteration 8095 Loss = 0.038861999275555496\n",
            "Iteration 8096 Loss = 0.03886068472370914\n",
            "Iteration 8097 Loss = 0.038859370908937624\n",
            "Iteration 8098 Loss = 0.03885805783082763\n",
            "Iteration 8099 Loss = 0.03885674548896636\n",
            "Iteration 8100 Loss = 0.03885543388294068\n",
            "Iteration 8101 Loss = 0.038854123012338204\n",
            "Iteration 8102 Loss = 0.03885281287674657\n",
            "Iteration 8103 Loss = 0.03885150347575358\n",
            "Iteration 8104 Loss = 0.038850194808947386\n",
            "Iteration 8105 Loss = 0.038848886875916366\n",
            "Iteration 8106 Loss = 0.03884757967624893\n",
            "Iteration 8107 Loss = 0.03884627320953408\n",
            "Iteration 8108 Loss = 0.03884496747536073\n",
            "Iteration 8109 Loss = 0.038843662473318046\n",
            "Iteration 8110 Loss = 0.03884235820299579\n",
            "Iteration 8111 Loss = 0.03884105466398347\n",
            "Iteration 8112 Loss = 0.03883975185587117\n",
            "Iteration 8113 Loss = 0.0388384497782489\n",
            "Iteration 8114 Loss = 0.03883714843070725\n",
            "Iteration 8115 Loss = 0.038835847812836756\n",
            "Iteration 8116 Loss = 0.03883454792422828\n",
            "Iteration 8117 Loss = 0.03883324876447297\n",
            "Iteration 8118 Loss = 0.03883195033316217\n",
            "Iteration 8119 Loss = 0.038830652629887344\n",
            "Iteration 8120 Loss = 0.0388293556542404\n",
            "Iteration 8121 Loss = 0.03882805940581333\n",
            "Iteration 8122 Loss = 0.03882676388419833\n",
            "Iteration 8123 Loss = 0.038825469088987756\n",
            "Iteration 8124 Loss = 0.03882417501977454\n",
            "Iteration 8125 Loss = 0.03882288167615165\n",
            "Iteration 8126 Loss = 0.0388215890577119\n",
            "Iteration 8127 Loss = 0.03882029716404896\n",
            "Iteration 8128 Loss = 0.03881900599475635\n",
            "Iteration 8129 Loss = 0.03881771554942799\n",
            "Iteration 8130 Loss = 0.03881642582765787\n",
            "Iteration 8131 Loss = 0.03881513682904024\n",
            "Iteration 8132 Loss = 0.03881384855316976\n",
            "Iteration 8133 Loss = 0.03881256099964118\n",
            "Iteration 8134 Loss = 0.03881127416804941\n",
            "Iteration 8135 Loss = 0.038809988057989554\n",
            "Iteration 8136 Loss = 0.03880870266905717\n",
            "Iteration 8137 Loss = 0.038807418000848054\n",
            "Iteration 8138 Loss = 0.03880613405295784\n",
            "Iteration 8139 Loss = 0.038804850824982765\n",
            "Iteration 8140 Loss = 0.03880356831651907\n",
            "Iteration 8141 Loss = 0.03880228652716351\n",
            "Iteration 8142 Loss = 0.038801005456512765\n",
            "Iteration 8143 Loss = 0.03879972510416378\n",
            "Iteration 8144 Loss = 0.03879844546971393\n",
            "Iteration 8145 Loss = 0.03879716655276064\n",
            "Iteration 8146 Loss = 0.03879588835290161\n",
            "Iteration 8147 Loss = 0.03879461086973476\n",
            "Iteration 8148 Loss = 0.038793334102858235\n",
            "Iteration 8149 Loss = 0.03879205805187037\n",
            "Iteration 8150 Loss = 0.038790782716369814\n",
            "Iteration 8151 Loss = 0.038789508095955424\n",
            "Iteration 8152 Loss = 0.03878823419022618\n",
            "Iteration 8153 Loss = 0.038786960998781324\n",
            "Iteration 8154 Loss = 0.03878568852122044\n",
            "Iteration 8155 Loss = 0.038784416757143225\n",
            "Iteration 8156 Loss = 0.03878314570614956\n",
            "Iteration 8157 Loss = 0.038781875367839655\n",
            "Iteration 8158 Loss = 0.03878060574181405\n",
            "Iteration 8159 Loss = 0.038779336827673115\n",
            "Iteration 8160 Loss = 0.03877806862501787\n",
            "Iteration 8161 Loss = 0.038776801133449376\n",
            "Iteration 8162 Loss = 0.0387755343525688\n",
            "Iteration 8163 Loss = 0.038774268281977727\n",
            "Iteration 8164 Loss = 0.038773002921278\n",
            "Iteration 8165 Loss = 0.03877173827007124\n",
            "Iteration 8166 Loss = 0.03877047432796015\n",
            "Iteration 8167 Loss = 0.03876921109454666\n",
            "Iteration 8168 Loss = 0.038767948569433656\n",
            "Iteration 8169 Loss = 0.03876668675222402\n",
            "Iteration 8170 Loss = 0.03876542564252062\n",
            "Iteration 8171 Loss = 0.03876416523992681\n",
            "Iteration 8172 Loss = 0.03876290554404634\n",
            "Iteration 8173 Loss = 0.0387616465544827\n",
            "Iteration 8174 Loss = 0.03876038827083994\n",
            "Iteration 8175 Loss = 0.03875913069272227\n",
            "Iteration 8176 Loss = 0.0387578738197341\n",
            "Iteration 8177 Loss = 0.038756617651479995\n",
            "Iteration 8178 Loss = 0.038755362187564805\n",
            "Iteration 8179 Loss = 0.03875410742759375\n",
            "Iteration 8180 Loss = 0.03875285337117188\n",
            "Iteration 8181 Loss = 0.038751600017904994\n",
            "Iteration 8182 Loss = 0.03875034736739869\n",
            "Iteration 8183 Loss = 0.03874909541925884\n",
            "Iteration 8184 Loss = 0.03874784417309176\n",
            "Iteration 8185 Loss = 0.03874659362850386\n",
            "Iteration 8186 Loss = 0.03874534378510169\n",
            "Iteration 8187 Loss = 0.03874409464249205\n",
            "Iteration 8188 Loss = 0.03874284620028202\n",
            "Iteration 8189 Loss = 0.038741598458079\n",
            "Iteration 8190 Loss = 0.038740351415490405\n",
            "Iteration 8191 Loss = 0.03873910507212394\n",
            "Iteration 8192 Loss = 0.03873785942758764\n",
            "Iteration 8193 Loss = 0.03873661448148959\n",
            "Iteration 8194 Loss = 0.0387353702334381\n",
            "Iteration 8195 Loss = 0.038734126683041985\n",
            "Iteration 8196 Loss = 0.038732883829909893\n",
            "Iteration 8197 Loss = 0.038731641673650945\n",
            "Iteration 8198 Loss = 0.03873040021387428\n",
            "Iteration 8199 Loss = 0.03872915945018947\n",
            "Iteration 8200 Loss = 0.038727919382206294\n",
            "Iteration 8201 Loss = 0.0387266800095346\n",
            "Iteration 8202 Loss = 0.03872544133178448\n",
            "Iteration 8203 Loss = 0.03872420334856623\n",
            "Iteration 8204 Loss = 0.03872296605949066\n",
            "Iteration 8205 Loss = 0.038721729464168374\n",
            "Iteration 8206 Loss = 0.03872049356221032\n",
            "Iteration 8207 Loss = 0.03871925835322789\n",
            "Iteration 8208 Loss = 0.038718023836832496\n",
            "Iteration 8209 Loss = 0.03871679001263569\n",
            "Iteration 8210 Loss = 0.03871555688024956\n",
            "Iteration 8211 Loss = 0.03871432443928593\n",
            "Iteration 8212 Loss = 0.03871309268935737\n",
            "Iteration 8213 Loss = 0.038711861630076234\n",
            "Iteration 8214 Loss = 0.038710631261055437\n",
            "Iteration 8215 Loss = 0.03870940158190789\n",
            "Iteration 8216 Loss = 0.03870817259224662\n",
            "Iteration 8217 Loss = 0.0387069442916852\n",
            "Iteration 8218 Loss = 0.03870571667983724\n",
            "Iteration 8219 Loss = 0.03870448975631652\n",
            "Iteration 8220 Loss = 0.03870326352073712\n",
            "Iteration 8221 Loss = 0.03870203797271326\n",
            "Iteration 8222 Loss = 0.038700813111859494\n",
            "Iteration 8223 Loss = 0.03869958893779044\n",
            "Iteration 8224 Loss = 0.03869836545012111\n",
            "Iteration 8225 Loss = 0.03869714264846669\n",
            "Iteration 8226 Loss = 0.038695920532442286\n",
            "Iteration 8227 Loss = 0.03869469910166367\n",
            "Iteration 8228 Loss = 0.03869347835574654\n",
            "Iteration 8229 Loss = 0.03869225829430701\n",
            "Iteration 8230 Loss = 0.03869103891696112\n",
            "Iteration 8231 Loss = 0.038689820223325344\n",
            "Iteration 8232 Loss = 0.038688602213016406\n",
            "Iteration 8233 Loss = 0.038687384885651016\n",
            "Iteration 8234 Loss = 0.03868616824084641\n",
            "Iteration 8235 Loss = 0.03868495227821976\n",
            "Iteration 8236 Loss = 0.03868373699738866\n",
            "Iteration 8237 Loss = 0.03868252239797069\n",
            "Iteration 8238 Loss = 0.038681308479583884\n",
            "Iteration 8239 Loss = 0.03868009524184627\n",
            "Iteration 8240 Loss = 0.038678882684376366\n",
            "Iteration 8241 Loss = 0.03867767080679272\n",
            "Iteration 8242 Loss = 0.03867645960871396\n",
            "Iteration 8243 Loss = 0.038675249089759264\n",
            "Iteration 8244 Loss = 0.03867403924954773\n",
            "Iteration 8245 Loss = 0.03867283008769889\n",
            "Iteration 8246 Loss = 0.03867162160383227\n",
            "Iteration 8247 Loss = 0.03867041379756778\n",
            "Iteration 8248 Loss = 0.03866920666852544\n",
            "Iteration 8249 Loss = 0.038668000216325646\n",
            "Iteration 8250 Loss = 0.0386667944405887\n",
            "Iteration 8251 Loss = 0.03866558934093538\n",
            "Iteration 8252 Loss = 0.03866438491698679\n",
            "Iteration 8253 Loss = 0.038663181168363765\n",
            "Iteration 8254 Loss = 0.038661978094687785\n",
            "Iteration 8255 Loss = 0.03866077569558056\n",
            "Iteration 8256 Loss = 0.038659573970663594\n",
            "Iteration 8257 Loss = 0.038658372919558974\n",
            "Iteration 8258 Loss = 0.038657172541888905\n",
            "Iteration 8259 Loss = 0.03865597283727582\n",
            "Iteration 8260 Loss = 0.03865477380534227\n",
            "Iteration 8261 Loss = 0.03865357544571105\n",
            "Iteration 8262 Loss = 0.03865237775800521\n",
            "Iteration 8263 Loss = 0.03865118074184812\n",
            "Iteration 8264 Loss = 0.038649984396863075\n",
            "Iteration 8265 Loss = 0.038648788722673924\n",
            "Iteration 8266 Loss = 0.0386475937189044\n",
            "Iteration 8267 Loss = 0.03864639938517868\n",
            "Iteration 8268 Loss = 0.03864520572112095\n",
            "Iteration 8269 Loss = 0.03864401272635585\n",
            "Iteration 8270 Loss = 0.03864282040050808\n",
            "Iteration 8271 Loss = 0.038641628743202545\n",
            "Iteration 8272 Loss = 0.038640437754064345\n",
            "Iteration 8273 Loss = 0.03863924743271893\n",
            "Iteration 8274 Loss = 0.03863805777879191\n",
            "Iteration 8275 Loss = 0.03863686879190889\n",
            "Iteration 8276 Loss = 0.038635680471695966\n",
            "Iteration 8277 Loss = 0.0386344928177793\n",
            "Iteration 8278 Loss = 0.03863330582978534\n",
            "Iteration 8279 Loss = 0.0386321195073407\n",
            "Iteration 8280 Loss = 0.03863093385007209\n",
            "Iteration 8281 Loss = 0.038629748857606644\n",
            "Iteration 8282 Loss = 0.03862856452957167\n",
            "Iteration 8283 Loss = 0.03862738086559446\n",
            "Iteration 8284 Loss = 0.038626197865302775\n",
            "Iteration 8285 Loss = 0.03862501552832443\n",
            "Iteration 8286 Loss = 0.0386238338542875\n",
            "Iteration 8287 Loss = 0.03862265284282041\n",
            "Iteration 8288 Loss = 0.03862147249355149\n",
            "Iteration 8289 Loss = 0.03862029280610942\n",
            "Iteration 8290 Loss = 0.03861911378012329\n",
            "Iteration 8291 Loss = 0.038617935415222106\n",
            "Iteration 8292 Loss = 0.03861675771103523\n",
            "Iteration 8293 Loss = 0.03861558066719202\n",
            "Iteration 8294 Loss = 0.03861440428332245\n",
            "Iteration 8295 Loss = 0.03861322855905648\n",
            "Iteration 8296 Loss = 0.038612053494024155\n",
            "Iteration 8297 Loss = 0.03861087908785582\n",
            "Iteration 8298 Loss = 0.0386097053401821\n",
            "Iteration 8299 Loss = 0.038608532250633744\n",
            "Iteration 8300 Loss = 0.038607359818841794\n",
            "Iteration 8301 Loss = 0.03860618804443739\n",
            "Iteration 8302 Loss = 0.03860501692705193\n",
            "Iteration 8303 Loss = 0.03860384646631708\n",
            "Iteration 8304 Loss = 0.03860267666186459\n",
            "Iteration 8305 Loss = 0.03860150751332656\n",
            "Iteration 8306 Loss = 0.038600339020335\n",
            "Iteration 8307 Loss = 0.038599171182522686\n",
            "Iteration 8308 Loss = 0.03859800399952204\n",
            "Iteration 8309 Loss = 0.03859683747096587\n",
            "Iteration 8310 Loss = 0.03859567159648732\n",
            "Iteration 8311 Loss = 0.038594506375719644\n",
            "Iteration 8312 Loss = 0.03859334180829619\n",
            "Iteration 8313 Loss = 0.03859217789385081\n",
            "Iteration 8314 Loss = 0.03859101463201723\n",
            "Iteration 8315 Loss = 0.03858985202242966\n",
            "Iteration 8316 Loss = 0.03858869006472231\n",
            "Iteration 8317 Loss = 0.0385875287585295\n",
            "Iteration 8318 Loss = 0.038586368103486195\n",
            "Iteration 8319 Loss = 0.038585208099227185\n",
            "Iteration 8320 Loss = 0.03858404874538753\n",
            "Iteration 8321 Loss = 0.038582890041602536\n",
            "Iteration 8322 Loss = 0.03858173198750786\n",
            "Iteration 8323 Loss = 0.03858057458273913\n",
            "Iteration 8324 Loss = 0.038579417826932125\n",
            "Iteration 8325 Loss = 0.0385782617197231\n",
            "Iteration 8326 Loss = 0.038577106260748485\n",
            "Iteration 8327 Loss = 0.03857595144964461\n",
            "Iteration 8328 Loss = 0.03857479728604836\n",
            "Iteration 8329 Loss = 0.03857364376959652\n",
            "Iteration 8330 Loss = 0.03857249089992646\n",
            "Iteration 8331 Loss = 0.03857133867667528\n",
            "Iteration 8332 Loss = 0.03857018709948072\n",
            "Iteration 8333 Loss = 0.03856903616798044\n",
            "Iteration 8334 Loss = 0.038567885881812446\n",
            "Iteration 8335 Loss = 0.0385667362406148\n",
            "Iteration 8336 Loss = 0.03856558724402601\n",
            "Iteration 8337 Loss = 0.038564438891684566\n",
            "Iteration 8338 Loss = 0.038563291183229216\n",
            "Iteration 8339 Loss = 0.03856214411829892\n",
            "Iteration 8340 Loss = 0.03856099769653297\n",
            "Iteration 8341 Loss = 0.03855985191757048\n",
            "Iteration 8342 Loss = 0.03855870678105142\n",
            "Iteration 8343 Loss = 0.038557562286615193\n",
            "Iteration 8344 Loss = 0.03855641843390194\n",
            "Iteration 8345 Loss = 0.03855527522255183\n",
            "Iteration 8346 Loss = 0.0385541326522053\n",
            "Iteration 8347 Loss = 0.03855299072250288\n",
            "Iteration 8348 Loss = 0.03855184943308543\n",
            "Iteration 8349 Loss = 0.03855070878359376\n",
            "Iteration 8350 Loss = 0.03854956877366925\n",
            "Iteration 8351 Loss = 0.03854842940295326\n",
            "Iteration 8352 Loss = 0.03854729067108719\n",
            "Iteration 8353 Loss = 0.038546152577713176\n",
            "Iteration 8354 Loss = 0.03854501512247298\n",
            "Iteration 8355 Loss = 0.03854387830500888\n",
            "Iteration 8356 Loss = 0.03854274212496309\n",
            "Iteration 8357 Loss = 0.03854160658197846\n",
            "Iteration 8358 Loss = 0.03854047167569765\n",
            "Iteration 8359 Loss = 0.03853933740576369\n",
            "Iteration 8360 Loss = 0.0385382037718198\n",
            "Iteration 8361 Loss = 0.03853707077350929\n",
            "Iteration 8362 Loss = 0.03853593841047591\n",
            "Iteration 8363 Loss = 0.0385348066823634\n",
            "Iteration 8364 Loss = 0.038533675588815534\n",
            "Iteration 8365 Loss = 0.03853254512947686\n",
            "Iteration 8366 Loss = 0.038531415303991505\n",
            "Iteration 8367 Loss = 0.03853028611200439\n",
            "Iteration 8368 Loss = 0.03852915755315991\n",
            "Iteration 8369 Loss = 0.0385280296271033\n",
            "Iteration 8370 Loss = 0.03852690233347967\n",
            "Iteration 8371 Loss = 0.03852577567193444\n",
            "Iteration 8372 Loss = 0.0385246496421132\n",
            "Iteration 8373 Loss = 0.03852352424366175\n",
            "Iteration 8374 Loss = 0.038522399476226044\n",
            "Iteration 8375 Loss = 0.038521275339452335\n",
            "Iteration 8376 Loss = 0.03852015183298697\n",
            "Iteration 8377 Loss = 0.038519028956476443\n",
            "Iteration 8378 Loss = 0.03851790670956769\n",
            "Iteration 8379 Loss = 0.03851678509190761\n",
            "Iteration 8380 Loss = 0.0385156641031434\n",
            "Iteration 8381 Loss = 0.03851454374292241\n",
            "Iteration 8382 Loss = 0.038513424010892236\n",
            "Iteration 8383 Loss = 0.03851230490670073\n",
            "Iteration 8384 Loss = 0.03851118642999563\n",
            "Iteration 8385 Loss = 0.038510068580425286\n",
            "Iteration 8386 Loss = 0.03850895135763803\n",
            "Iteration 8387 Loss = 0.038507834761282396\n",
            "Iteration 8388 Loss = 0.03850671879100722\n",
            "Iteration 8389 Loss = 0.038505603446461346\n",
            "Iteration 8390 Loss = 0.03850448872729388\n",
            "Iteration 8391 Loss = 0.03850337463315438\n",
            "Iteration 8392 Loss = 0.038502261163692264\n",
            "Iteration 8393 Loss = 0.03850114831855724\n",
            "Iteration 8394 Loss = 0.0385000360973993\n",
            "Iteration 8395 Loss = 0.038498924499868546\n",
            "Iteration 8396 Loss = 0.03849781352561536\n",
            "Iteration 8397 Loss = 0.03849670317429023\n",
            "Iteration 8398 Loss = 0.03849559344554377\n",
            "Iteration 8399 Loss = 0.03849448433902718\n",
            "Iteration 8400 Loss = 0.0384933758543913\n",
            "Iteration 8401 Loss = 0.03849226799128755\n",
            "Iteration 8402 Loss = 0.03849116074936745\n",
            "Iteration 8403 Loss = 0.0384900541282826\n",
            "Iteration 8404 Loss = 0.038488948127685114\n",
            "Iteration 8405 Loss = 0.03848784274722681\n",
            "Iteration 8406 Loss = 0.03848673798656013\n",
            "Iteration 8407 Loss = 0.038485633845337514\n",
            "Iteration 8408 Loss = 0.03848453032321161\n",
            "Iteration 8409 Loss = 0.03848342741983535\n",
            "Iteration 8410 Loss = 0.038482325134861835\n",
            "Iteration 8411 Loss = 0.03848122346794416\n",
            "Iteration 8412 Loss = 0.038480122418735954\n",
            "Iteration 8413 Loss = 0.038479021986890724\n",
            "Iteration 8414 Loss = 0.03847792217206241\n",
            "Iteration 8415 Loss = 0.03847682297390502\n",
            "Iteration 8416 Loss = 0.03847572439207274\n",
            "Iteration 8417 Loss = 0.03847462642622011\n",
            "Iteration 8418 Loss = 0.03847352907600169\n",
            "Iteration 8419 Loss = 0.03847243234107215\n",
            "Iteration 8420 Loss = 0.03847133622108677\n",
            "Iteration 8421 Loss = 0.03847024071570052\n",
            "Iteration 8422 Loss = 0.038469145824568785\n",
            "Iteration 8423 Loss = 0.03846805154734733\n",
            "Iteration 8424 Loss = 0.03846695788369181\n",
            "Iteration 8425 Loss = 0.038465864833258284\n",
            "Iteration 8426 Loss = 0.0384647723957027\n",
            "Iteration 8427 Loss = 0.03846368057068167\n",
            "Iteration 8428 Loss = 0.03846258935785156\n",
            "Iteration 8429 Loss = 0.03846149875686931\n",
            "Iteration 8430 Loss = 0.038460408767391686\n",
            "Iteration 8431 Loss = 0.03845931938907581\n",
            "Iteration 8432 Loss = 0.0384582306215791\n",
            "Iteration 8433 Loss = 0.03845714246455907\n",
            "Iteration 8434 Loss = 0.03845605491767333\n",
            "Iteration 8435 Loss = 0.03845496798057984\n",
            "Iteration 8436 Loss = 0.03845388165293662\n",
            "Iteration 8437 Loss = 0.03845279593440207\n",
            "Iteration 8438 Loss = 0.038451710824634534\n",
            "Iteration 8439 Loss = 0.03845062632329275\n",
            "Iteration 8440 Loss = 0.03844954243003551\n",
            "Iteration 8441 Loss = 0.038448459144521975\n",
            "Iteration 8442 Loss = 0.03844737646641117\n",
            "Iteration 8443 Loss = 0.038446294395362846\n",
            "Iteration 8444 Loss = 0.038445212931036164\n",
            "Iteration 8445 Loss = 0.038444132073091414\n",
            "Iteration 8446 Loss = 0.03844305182118826\n",
            "Iteration 8447 Loss = 0.03844197217498707\n",
            "Iteration 8448 Loss = 0.038440893134148155\n",
            "Iteration 8449 Loss = 0.03843981469833212\n",
            "Iteration 8450 Loss = 0.038438736867199653\n",
            "Iteration 8451 Loss = 0.03843765964041177\n",
            "Iteration 8452 Loss = 0.03843658301762958\n",
            "Iteration 8453 Loss = 0.038435506998514425\n",
            "Iteration 8454 Loss = 0.03843443158272789\n",
            "Iteration 8455 Loss = 0.038433356769931544\n",
            "Iteration 8456 Loss = 0.038432282559787416\n",
            "Iteration 8457 Loss = 0.03843120895195754\n",
            "Iteration 8458 Loss = 0.03843013594610427\n",
            "Iteration 8459 Loss = 0.03842906354188984\n",
            "Iteration 8460 Loss = 0.03842799173897724\n",
            "Iteration 8461 Loss = 0.03842692053702916\n",
            "Iteration 8462 Loss = 0.03842584993570857\n",
            "Iteration 8463 Loss = 0.03842477993467873\n",
            "Iteration 8464 Loss = 0.03842371053360315\n",
            "Iteration 8465 Loss = 0.03842264173214529\n",
            "Iteration 8466 Loss = 0.03842157352996909\n",
            "Iteration 8467 Loss = 0.03842050592673846\n",
            "Iteration 8468 Loss = 0.038419438922117626\n",
            "Iteration 8469 Loss = 0.03841837251577076\n",
            "Iteration 8470 Loss = 0.03841730670736257\n",
            "Iteration 8471 Loss = 0.03841624149655775\n",
            "Iteration 8472 Loss = 0.038415176883021214\n",
            "Iteration 8473 Loss = 0.03841411286641814\n",
            "Iteration 8474 Loss = 0.038413049446413744\n",
            "Iteration 8475 Loss = 0.038411986622673505\n",
            "Iteration 8476 Loss = 0.03841092439486313\n",
            "Iteration 8477 Loss = 0.038409862762648496\n",
            "Iteration 8478 Loss = 0.03840880172569562\n",
            "Iteration 8479 Loss = 0.03840774128367071\n",
            "Iteration 8480 Loss = 0.038406681436240236\n",
            "Iteration 8481 Loss = 0.03840562218307077\n",
            "Iteration 8482 Loss = 0.038404563523829184\n",
            "Iteration 8483 Loss = 0.03840350545818233\n",
            "Iteration 8484 Loss = 0.038402447985797616\n",
            "Iteration 8485 Loss = 0.038401391106342025\n",
            "Iteration 8486 Loss = 0.03840033481948339\n",
            "Iteration 8487 Loss = 0.03839927912488933\n",
            "Iteration 8488 Loss = 0.03839822402222777\n",
            "Iteration 8489 Loss = 0.03839716951116688\n",
            "Iteration 8490 Loss = 0.03839611559137487\n",
            "Iteration 8491 Loss = 0.03839506226252018\n",
            "Iteration 8492 Loss = 0.03839400952427161\n",
            "Iteration 8493 Loss = 0.03839295737629786\n",
            "Iteration 8494 Loss = 0.03839190581826807\n",
            "Iteration 8495 Loss = 0.03839085484985146\n",
            "Iteration 8496 Loss = 0.038389804470717256\n",
            "Iteration 8497 Loss = 0.03838875468053515\n",
            "Iteration 8498 Loss = 0.03838770547897498\n",
            "Iteration 8499 Loss = 0.038386656865706636\n",
            "Iteration 8500 Loss = 0.03838560884040033\n",
            "Iteration 8501 Loss = 0.0383845614027263\n",
            "Iteration 8502 Loss = 0.03838351455235506\n",
            "Iteration 8503 Loss = 0.038382468288957444\n",
            "Iteration 8504 Loss = 0.03838142261220412\n",
            "Iteration 8505 Loss = 0.03838037752176626\n",
            "Iteration 8506 Loss = 0.03837933301731518\n",
            "Iteration 8507 Loss = 0.038378289098522145\n",
            "Iteration 8508 Loss = 0.03837724576505901\n",
            "Iteration 8509 Loss = 0.03837620301659738\n",
            "Iteration 8510 Loss = 0.038375160852809356\n",
            "Iteration 8511 Loss = 0.03837411927336699\n",
            "Iteration 8512 Loss = 0.03837307827794279\n",
            "Iteration 8513 Loss = 0.03837203786620924\n",
            "Iteration 8514 Loss = 0.038370998037838944\n",
            "Iteration 8515 Loss = 0.03836995879250493\n",
            "Iteration 8516 Loss = 0.038368920129880295\n",
            "Iteration 8517 Loss = 0.038367882049638286\n",
            "Iteration 8518 Loss = 0.038366844551452166\n",
            "Iteration 8519 Loss = 0.03836580763499598\n",
            "Iteration 8520 Loss = 0.03836477129994322\n",
            "Iteration 8521 Loss = 0.03836373554596807\n",
            "Iteration 8522 Loss = 0.03836270037274457\n",
            "Iteration 8523 Loss = 0.03836166577994718\n",
            "Iteration 8524 Loss = 0.03836063176725045\n",
            "Iteration 8525 Loss = 0.03835959833432911\n",
            "Iteration 8526 Loss = 0.03835856548085807\n",
            "Iteration 8527 Loss = 0.03835753320651237\n",
            "Iteration 8528 Loss = 0.038356501510967395\n",
            "Iteration 8529 Loss = 0.03835547039389851\n",
            "Iteration 8530 Loss = 0.038354439854981416\n",
            "Iteration 8531 Loss = 0.03835340989389205\n",
            "Iteration 8532 Loss = 0.03835238051030629\n",
            "Iteration 8533 Loss = 0.038351351703900236\n",
            "Iteration 8534 Loss = 0.038350323474350446\n",
            "Iteration 8535 Loss = 0.03834929582133349\n",
            "Iteration 8536 Loss = 0.03834826874452582\n",
            "Iteration 8537 Loss = 0.03834724224360467\n",
            "Iteration 8538 Loss = 0.038346216318247\n",
            "Iteration 8539 Loss = 0.03834519096813012\n",
            "Iteration 8540 Loss = 0.0383441661929314\n",
            "Iteration 8541 Loss = 0.03834314199232862\n",
            "Iteration 8542 Loss = 0.038342118365999524\n",
            "Iteration 8543 Loss = 0.03834109531362211\n",
            "Iteration 8544 Loss = 0.03834007283487461\n",
            "Iteration 8545 Loss = 0.038339050929435306\n",
            "Iteration 8546 Loss = 0.03833802959698288\n",
            "Iteration 8547 Loss = 0.03833700883719583\n",
            "Iteration 8548 Loss = 0.03833598864975338\n",
            "Iteration 8549 Loss = 0.03833496903433428\n",
            "Iteration 8550 Loss = 0.03833394999061793\n",
            "Iteration 8551 Loss = 0.03833293151828389\n",
            "Iteration 8552 Loss = 0.03833191361701168\n",
            "Iteration 8553 Loss = 0.03833089628648104\n",
            "Iteration 8554 Loss = 0.03832987952637202\n",
            "Iteration 8555 Loss = 0.038328863336364774\n",
            "Iteration 8556 Loss = 0.03832784771613973\n",
            "Iteration 8557 Loss = 0.03832683266537736\n",
            "Iteration 8558 Loss = 0.038325818183758166\n",
            "Iteration 8559 Loss = 0.03832480427096334\n",
            "Iteration 8560 Loss = 0.038323790926673616\n",
            "Iteration 8561 Loss = 0.038322778150570544\n",
            "Iteration 8562 Loss = 0.03832176594233544\n",
            "Iteration 8563 Loss = 0.03832075430164979\n",
            "Iteration 8564 Loss = 0.038319743228195466\n",
            "Iteration 8565 Loss = 0.03831873272165434\n",
            "Iteration 8566 Loss = 0.03831772278170865\n",
            "Iteration 8567 Loss = 0.03831671340804057\n",
            "Iteration 8568 Loss = 0.03831570460033271\n",
            "Iteration 8569 Loss = 0.03831469635826768\n",
            "Iteration 8570 Loss = 0.0383136886815283\n",
            "Iteration 8571 Loss = 0.038312681569797655\n",
            "Iteration 8572 Loss = 0.03831167502275884\n",
            "Iteration 8573 Loss = 0.038310669040095384\n",
            "Iteration 8574 Loss = 0.03830966362149071\n",
            "Iteration 8575 Loss = 0.038308658766628666\n",
            "Iteration 8576 Loss = 0.03830765447519284\n",
            "Iteration 8577 Loss = 0.03830665074686778\n",
            "Iteration 8578 Loss = 0.038305647581337504\n",
            "Iteration 8579 Loss = 0.038304644978286354\n",
            "Iteration 8580 Loss = 0.038303642937399164\n",
            "Iteration 8581 Loss = 0.03830264145836057\n",
            "Iteration 8582 Loss = 0.03830164054085563\n",
            "Iteration 8583 Loss = 0.038300640184569405\n",
            "Iteration 8584 Loss = 0.03829964038918736\n"
          ]
        }
      ],
      "source": [
        "beta, final_loss = gradient_descent(x_train, y_train, beta, alpha, delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Printing the final weights and loss function value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Weights: [19.8554217   2.31965826]\n",
            "Final Loss: 0.03829964038918736\n"
          ]
        }
      ],
      "source": [
        "print('Final Weights:', beta.ravel())\n",
        "print('Final Loss:', final_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding a column of ones to the test data matrix for the intercept term (beta0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_test = np.hstack((np.ones((len(test_data), 1)), test_data[:, :-1]))\n",
        "y_test = test_data[:, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computing and printing the loss function on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.04514858744558952\n"
          ]
        }
      ],
      "source": [
        "test_loss = compute_cost(x_test, y_test.reshape((-1, 1)), beta)\n",
        "print('Test Loss:', test_loss)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the training and testing data points, the line of best fit (our predicted values) and legend with labes. Stretching our line on the test vals area to see result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(train_data[:, 0], train_data[:, 1], marker='x', color='r', label='Training Data')\n",
        "plt.scatter(test_data[:, 0], test_data[:, 1], marker='o', color='b', label='Testing Data')\n",
        "x_min, x_max = min(np.min(train[:, 1]), np.min(test[:, 1])), max(np.max(train[:, 1]), np.max(test[:, 1]))\n",
        "x_vals = np.linspace(x_min, x_max, 100)\n",
        "y_vals = beta[0] + beta[1]*x_vals\n",
        "plt.plot(x_vals, y_vals, color='g', label='Linear Regression')\n",
        "plt.legend()\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10 (tags/v3.10.10:aad5f6a, Feb  7 2023, 17:20:36) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "18f24e5fb3a20ee37addaed3f8875bcbf0b538970bf0650eb09be53afed25558"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
